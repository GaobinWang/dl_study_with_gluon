{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **Matrix Factorization using Deep Network (Recommendation)**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[Ref.]\n",
    "  - https://hackernoon.com/autoencoders-deep-learning-bits-1-11731e200694\n",
    "  - http://katbailey.github.io/post/matrix-factorization-with-tensorflow/\n",
    "  - https://www.oreilly.com/ideas/deep-matrix-factorization-using-apache-mxnet\n",
    "  - http://nicolas-hug.com/blog/matrix_facto_3\n",
    "\n",
    "<br/>\n",
    "\n",
    "### **1) Recommendation 방식**\n",
    "\n",
    "\n",
    "\n",
    "<br/>\n",
    "- 기본적인 Recommendation의 구조는 아래와 같이 1) 사용자(User) x 2) 아이템(Item) 간의 스코어 matrix 형태로 표현할 수 있음\n",
    "<br/>\n",
    "![recomm._mat](recomm._mat.png)\n",
    "<br/>\n",
    "- Recommendation을 하기 위해 사용자그룹의 기존(과거) 구매/평점기록 데이터를 활용함\n",
    "- 그러나 모든 사용자가 모든 아이템에 대한 구매/평점 기록이 없기에 전체 사용자 x 아이템으로 표현되는 Recommendation Matrix는 Sparse & Incomplete한 형태가 됨\n",
    "- 이 Incomplete Matrix의 Missing Values를 찾는 것이 Recommendation Algorithm의 목적\n",
    "- 크게 2가지 형태의 Recommendation Algorithm이 있음\n",
    "\n",
    "  **1) Collaborative Filtering (User-based or Item-based)**\n",
    "    - 협업필터링 방식으로 추천하고자 하는 대상과 가장 유사한 사용자그룹 혹은 아이템그룹의 유사도와 과거 구매/평점기록을 이용하여 추천하는 방식\n",
    "    - 그룹 간 유사도를 계산하는 방식으로 Cosine Similarity을 일반적으로 많이 이용함 \n",
    "    - Missing value에 대한 예측 시 (User or Item) Similairty와 기 존재하는 구매/평점기록값(구매 or 별점)과의 가중평균으로 계산 함\n",
    "    \n",
    "    (참조 : https://buildingrecommenders.wordpress.com/2015/11/18/overview-of-recommender-algorithms-part-2/)\n",
    "    - 1) User-Based CF\n",
    "        - 추천대상과 유사한 사용자 그룹을 파악하고, 해당 그룹이 많이 사용/구매한 아이템을 추천하는 방식\n",
    "        - 알고리즘 구현이 간단한 편이지만, 유저가 많아질 수록 연산이 복잡함 (nC2 경우의 수가 많아짐)\n",
    "    - 2) Item-Based CF\n",
    "        - 아이템간의 유사도 matrix를 기반으로, 특정 아이템을 추천하는 방식 (가중평균)\n",
    "        - User-Based CF에 비해 연산이 가벼운 편이지만, 개인 성향을 반영하기 어려움\n",
    "<br/>\n",
    "![CF_ex](CF_ex.png)\n",
    "<br/>\n",
    "\n",
    "  **2) Matrix Factorization**\n",
    "    - Sparse & Incomplete Recommendation을 2개 이상의 Low Rank Matrices로 Decomposition하는 방식\n",
    "    - Decomposition하는 2개의 Matrices는 추천대상(User) / 아이템(Item) Matrix로 나눠 짐\n",
    "    - 협업 필터링과 달리 사용자(User) / 아이템(Item)의 특성을 표현하는 Latent Factor (:= Embedding values)로 표현함\n",
    "    [Ex) 2 Dimension Latent Factor으로 표현한 예시]\n",
    "<br/>\n",
    "![mat_fact](mat_fact.png)\n",
    "<br/>\n",
    "    - Latent Factor로 표현된 2개의 Matrix의 Dot Product가 Recommendation Matrix의 추천 값이 되며, 이 값 중 과거 기록과의 차이를 최소화 하는 Latent Matrix를 찾는 것이 목적\n",
    "    - Performance Metric으로 RMSE를 주로 사용함\n",
    "    - 일반적으로 Latent Facotr의 Dimension은 User / Item의 Dimension보다 작은 값을 이용\n",
    "    - Matrix Factorization 방법으로 PCA / SVD 방법등이 있지만, Sparse & Incomplete Matrix이기에 Latent Matrices에 초기값을 주고 Iteration 마다 최적값을 찾아가는 Optimization 방법을 주로 이용\n",
    "    - 본 스터디에서는 Optimization 방법 중 Deep Learning Framework를 이용한 Vanila Matrix Factorization 방법과 Deep Matrix Facotrization 방법을 소개 함\n",
    "\n",
    " <br/>\n",
    "- **Deep Matrix Factorization의 장점**\n",
    "    - Flatten Layer를 사용하기 때문에 Embedding Layer의 Latent Dimension의 크기를 다르게 가져갈 수 있음 (Ex. User는 10차원, Item은 5차원)\n",
    "    - Vanilla Matrix Factorization은 User간 or Item간의 Inter-connection의 영향을 반영하기 어렵지만, Flatten으로 펼쳐서 학습하기 떄문에 User간, Item간, User-Item간의 Inter-connection을 학습할 수 있음\n",
    "    - Vanilla Matrix Factorization에서 불가능한 non-linear connections을 표현 가능\n",
    "    - Latent Factor를 표현하는 Embedding Layer를 2개 이상 할 수 있음 (Flatten 함수 써서)\n",
    "    - Dot Product가 없기에 학습속도가 훨씬 빠름\n",
    "\n",
    " <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T08:42:28.497286Z",
     "start_time": "2018-02-08T08:42:26.945985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import mxnet as mx\n",
    "import pandas\n",
    "import seaborn; seaborn.set_style('whitegrid')\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T08:42:28.503741Z",
     "start_time": "2018-02-08T08:42:28.499066Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gluon module load\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from mxnet import nd, autograd, gluon\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **1) Vanila Matrix Facotrization using synthetic data**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2개의 low rank latent matrix의 dot product로 예측하는 일반적인 Matrix Facotrization 방법\n",
    "- 2개의 latent matrix rank는 같음\n",
    "- rank <= input dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T08:42:28.972715Z",
     "start_time": "2018-02-08T08:42:28.505316Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic data set is generated. [row num : 35000]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfwAAAFtCAYAAADiaNj1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XtUVPX+//HXAIIKg0rnW5YdSlIwRfIW6lEx7KuYZVoZ\nKsVXK0tJTdQUM0XLW6aiR1rkLS1BQ0qrY3lKpdKOF7SptGNifbFakeYxNQU0rvP7w6/zE0EcuQ0z\n+/lYy7WcvWf2fn+Ggdd8PnvvzzZZrVarAACAS3NzdAEAAKDmEfgAABgAgQ8AgAEQ+AAAGACBDwCA\nARD4gB0ceTELF9KUZYT3xAhtRO0i8OESoqOjFRQUZPvXunVrde7cWSNGjNCePXtKPXfTpk0KCgrS\n6dOn7dr2l19+qeeee+6azwsKCtIbb7whSUpMTFT79u2vvyFX+OGHHzRs2DDb44yMDAUFBenbb7+t\n8rarw7x589SpUyd16NBBFoulzPpL7/Wlf61atVK7du304IMPasWKFSosLLzufW7fvl0zZsyocu2X\n/7yqso2goCAlJyeXu37//v0KCgq67s9CZT5zwLV4OLoAoLp06NBBcXFxkqSioiL95z//UWpqqp54\n4gktXLhQDzzwgCTpnnvu0YYNG+Tr62vXdt999139+OOP13zehg0bdMstt1S+AeX4+OOPS4V7mzZt\ntGHDBt1xxx3Vup/KOHLkiN58800NGzZMvXv31p133nnV565atUpms1lWq1U5OTnau3evli5dKovF\noqSkJLm7u9u937feeksNGzascv3V9fMymUzaunWroqOjy6z7+OOPK7VNR37m4LoIfLgMX19ftWvX\nrtSyvn37atiwYZo5c6Z69OihRo0ayc/PT35+ftW+/yv3XRN8fHxqZT/2OHv2rCTpgQceUEhISIXP\nbdOmTan3PCwsTAEBAXrxxRf13nvvadCgQTVaa3mq631s3769LBaLTp8+XaqNJSUl+uSTTxQUFKRf\nfvmlWvZ1pbryWYBzYEgfLs3NzU3PPvuscnJybL2tK4f0jx49qhEjRtiGpp966illZmZKkqZMmaL3\n3ntPP/zwg4KCgpSRkaFNmzapc+fOWrVqlTp37qyePXvq/Pnz5Q6vvv/++woPD9ddd92lkSNH6uef\nf7atmzJlim3U4ZLt27crKChI2dnZSkxM1GuvvWbb9qZNm8od0t+2bZseeeQRtWvXTj179tSSJUtU\nVFRkW9+rVy+tXLlSM2bMUGhoqG0kJDc3t8L3LjMzUyNGjFBoaKhCQ0M1adIk/f7775IuHrK41KN9\n9NFHy+3dXssjjzyiZs2a6d1337Uty83N1ezZsxUeHq7g4GB16dJFcXFxOnfunKSLh2727dunzz//\n3PY+SdIXX3yhxx9/XO3bt1fbtm01YMAAbd26tcL9X3kI5uGHH9aHH36oiIgItW3bVo888oi++uqr\na7aje/fu8vLyUnp6eqnlX331lfLy8hQWFlZqeWFhoZYuXaqIiAgFBwfr7rvv1pgxY3T8+HFJlfvM\nFRUVacCAAerVq5f+/PNP23769++vgQMHVurQCVwPgQ+XFxoaKnd3d3399ddl1pWUlCgmJkbFxcVa\nvHixFi9erDNnzmjkyJEqLi7Ws88+q549e+qvf/2rNmzYoDZt2kiScnJytHnzZi1cuFAvvPBCuUPM\nFy5c0MKFC/Xcc8/p1Vdf1U8//aThw4fr/PnzdtX96KOPatCgQapfv742bNige+65p8xzNmzYoDFj\nxigkJESvvfaaHn/8ca1evVpTpkwp9bzly5fr3LlzSkhIUGxsrD766CO9/vrrV9334cOHNXjwYBUW\nFuqVV17R1KlT9eWXX+rxxx/X+fPn9eijjyo+Pl7SxeP4lTmmbjKZ1LlzZ3377be2QJo4caI+/fRT\nTZw4UW+88YaefPJJffjhh0pKSpIkzZgxQ61bt1aHDh20YcMG3XjjjTp48KCeeeYZtWzZUklJSVq8\neLEaNGigiRMn2n2ehiT99NNPWrp0qcaMGaPExETl5+dr3Lhxpb48lad+/foKCwsr8wXj448/Vq9e\nveTl5VVq+bx585SSkqKnn35aq1evVmxsrPbs2aO5c+dKUqU+cx4eHpozZ45+++03LV++XJK0bNky\n/fjjj5o/f77q1atn9/sA18WQPlyeu7u7GjdubOudXu7UqVP66aefNHbsWPXo0UOSdPPNN+vDDz/U\n+fPn5e/vLz8/Px07dqzU8GlxcbHGjBlje015rFarFixYoK5du0qSAgIC1L9/f3300Ud69NFHr1l3\n06ZN1bRpU7m5uZU7dFtcXKwlS5bo/vvvtwVu9+7dZTabNWPGDI0YMUKtWrWybSshIUEmk0ndu3fX\nvn37tHPnTk2aNKncfSclJcnPz08rV66Up6enJCk4OFj9+/fXxo0bFR0drRYtWkiSWrZsafv/9fLz\n81NRUZHOnj0rs9mswsJCzZw509Yr7ty5s77++mvt27dPktSiRQv5+PioYcOGtvfkhx9+UO/evUt9\n6bjlllv00EMP6cCBAwoPD7erlry8PL355pu2wxOXvvBlZmYqODi4wtdGRERo8uTJysnJsZ2r8Mkn\nnyg+Pt42WnTJ6dOnNXnyZNthjNDQUP3444/avHmzJFX6MxccHKwnn3xSq1at0l133aXly5dr7Nix\nCgoKsqv9cH308GFoN9xwg26//XZNnz5dU6dO1SeffKJmzZppwoQJMpvNFb62efPmFa43m822sJcu\nBuNf//rXcs9mr4yjR4/q9OnT6tu3b6nl999/v6SLZ3pf0rZtW5lMJtvjpk2bVjjSsH//ft177722\nsJcuhm1QUJD2799fLfVfycvLS6tXr1ZYWJiys7P1r3/9S2vWrFFWVlaFQ9KPPPKIli5dqvPnz+vb\nb7/V5s2btW7dOklSQUGB3fv38PAoFexNmzaVdHGk5lp69uwpNzc3ffbZZ5Iki8Wi3NzcMsP5krRk\nyRINGjRIJ06c0J49e7Ru3Tp99dVXdtV6rc/c2LFjdcsttygmJkZt2rTRiBEjrrlNGAc9fLi8/Px8\nnT17VjfddFOZdW5ubnrzzTeVmJio9PR0bdy4UfXr19eQIUMUFxcnN7erfye+1ol/N9xwQ7mvycnJ\nuf5GlOPSSXNX7sdsNsvT07PUMfoGDRqUeo7JZKrwOu9z586VW/8NN9xwzWP/1+PEiRPy9PRU48aN\nJUnp6emaN2+efvnlFzVp0kTBwcGqX7++SkpKrrqN8+fPKz4+Xv/85z8lXQzFSyMb13Mtu6enZ6mf\n96X/V7TvS7y9vdWjRw9t27ZNDz74oD755BOFh4eXGc6XLh7bnzlzpo4cOSKz2aw777yz3OeV51qf\nOS8vL0VERGj58uXq1q3bdV39ANdHDx8u78svv1RRUZE6duxY7vqbb75Zc+fO1Z49e/T222+rX79+\nevPNNyt9SdUll040u9zvv/9u+6NtMpnKhEleXp7d278UkqdOnSqz34KCAtv6ymjUqFGZ7UoX66/K\ndi9XUlKi/fv3q127dvLw8NBPP/2kcePGqWvXrtqxY4f27t2rVatWXbNXO2vWLO3atUsrVqzQ119/\nrQ8//FCjRo2qlhqvR58+ffTFF1/o/Pnz2rp1q+67774yz8nJydGoUaN0yy23aOvWrfryyy+VnJxc\nLXM2SNLPP/+st956S0FBQVq1apVdl/bBOAh8uDSr1aoVK1aocePG6tOnT5n1mZmZ6t69uw4dOiQ3\nNzd16NBBs2fPloeHh44dOyZJFfbyK3L69GkdOnTI9vjQoUPKzs5WaGiopIu9wlOnTpUK/SuH+yva\nd/PmzdWkSZMyX0y2bNki6eK8BJXVsWNHpaenlxpmzsrK0vfff1+l7V7ugw8+0G+//WY7n+G7775T\nYWGhnnnmGdtw+vnz52WxWEr11K98T7755hv16NFD3bp1sx2C+OKLLyTV7mx1vXr1UlFRkZYtW6Zz\n586VO5x/9OhRnT17VsOGDdNtt90m6eIXn927d1fYRntYrVZNmzZNt956q1JTU9WsWTNNmzaNGftg\nw5A+XMa5c+f0zTffSLo48c6JEyf0zjvvaP/+/Vq4cKF8fHzKvKZFixby9vZWXFycxowZo0aNGun9\n99+XyWSynRXv6+ur3377Tbt27brmyVuX8/T01IQJE/T888+rsLBQCxcuVKtWrRQRESHp4rXoycnJ\neumll9SvXz/t3btX27dvL7UNX19fXbhwQdu3by9zrbu7u7vGjBmjWbNmqVGjRrr33nt15MgRJSYm\nqm/fvgoMDLyet6+UUaNGaciQIXr66ac1fPhw5eTkaMmSJWrWrJkGDhx43ds7dOiQ7WS2c+fOKSMj\nQ2vXrlWvXr3Uv39/SdKdd94pd3d3LViwQEOHDtWZM2e0evVq/f7776XOJfD19dXhw4eVkZGhu+66\nS23bttWnn36q9957TzfffLP27t1ru9zu0iVqtcFsNutvf/ubVq9erT59+pQ7TB8QECBvb28lJSWp\npKREf/75p9avX6/MzEzbYRaTyVSpz9yGDRu0b98+JScnq2HDhoqPj9fw4cO1fv16PfbYY9XdXDgh\nevhwGV999ZUGDx6swYMHKzo6WrNmzVL9+vW1du1a9evXr9zXeHh4aOXKlbrttts0c+ZMjRw5UkeP\nHtXy5cttZ54PHjxYN9xwg0aOHKldu3bZXU+zZs30xBNP6KWXXtKLL76okJAQrV692hZeYWFhGj9+\nvNLT0/XMM8/o8OHDeuWVV0pt4/7771ebNm0UGxurDz74oMw+Hn/8cc2ZM0cZGRkaNWqU1q1bZ5tZ\nsCqCg4P11ltvqaioSOPGjdOcOXPUqVMnvf322+V+cbqWESNGaPDgwRoyZIjGjRunvXv36vnnn1di\nYqLtZMLmzZtr/vz5OnLkiJ555hktXLhQbdu21YwZM3T8+HGdOHFCkjR8+HAVFBRoxIgR+u677zRl\nyhT97W9/09y5czV27Fjt3btXr732mm6//fZyL8WsSX369FFhYWGZEykvMZvNSkxM1Llz5xQTE6OX\nX35ZjRs31t///neVlJTowIEDkq7/M3fixAktWLBAAwYMsI0gde3aVQ888IAWLVpku8YfxmayMt4D\nAIDLo4cPAIABEPgAABhArQb+gQMHbHNuHz58WFFRUYqOjtZTTz1lmwUtLS1NDz/8sCIjI22TWPz5\n558aO3asoqKi9PTTT1/XdJkAAKAWA3/lypWaNm2a8vPzJUlz5szR9OnTlZycrN69e2vlypU6efKk\nkpOTlZqaqjfeeEMJCQkqKCjQ22+/rcDAQK1fv14DBw60zasNAADsU2uB7+/vr8TERNvjhIQE2/2z\ni4uL5eXlpYMHD6p9+/by9PSU2WyWv7+/MjMzZbFYbPNHh4WFac+ePbVVNgAALqHWrsOPiIiw3cpS\nkm688UZJFy+lSklJ0bp16/TFF1+Umr/c29tbubm5ys3NtS339va2e2rS6pqzHAAAZ3G1WUUdOvHO\nli1b9Prrr2vFihXy8/OTj49PqalF8/LyZDabSy3Py8uTr6+v3fu4suEWi+Wqb4Yzoj11G+2p21yt\nPZLrtYn2XP/2r8ZhZ+l/8MEHSklJUXJysv76179KkkJCQmSxWJSfn6+cnBxlZWUpMDBQHTp00I4d\nOyRJO3fudKkfPgAAtcEhPfzi4mLNmTNHN998s8aOHStJuvvuu/Xcc88pOjpaUVFRslqtGj9+vLy8\nvDR06FDFxcVp6NChqlevnhYtWuSIsgEAcFq1Gvi33nqr0tLSJEn79u0r9zmRkZGKjIwstaxBgwZa\nunRpjdcHAICrYuIdAAAMgMAHAMAACHwAAAyAwAcAwAAIfAAADIDABwDAAAh8AAAMgMAHAMAACHwA\nAAzAoTfPAVA39Z/4QanHmxcNcFAlAKoLPXwAAAyAwAcAwAAIfAAADIDABwDAAAh8AAAMgMAHAMAA\nCHwAAAyAwAcAwAAIfAAADIDABwDAAAh8AAAMgMAHAMAACHwAAAyAwAcAwAAIfAAADIDABwDAAAh8\nAAAMwMPRBQCoXv0nflDq8eZFA677NQBcDz18AAAMgMAHAMAAGNIHcN2uddigMocVANQsevgAABgA\ngQ8AgAEwpA+gyjjLH6j76OEDAGAABD4AAAbAkD7g4sobbuesecB46OEDAGAABD4AAAbAkD6Aayp1\nWGB9tuMKAVBp9PABADAAAh8AAAOo1SH9AwcOaOHChUpOTtbPP/+sKVOmyGQyqWXLlpoxY4bc3NyU\nlpam1NRUeXh4KCYmRuHh4frzzz81adIknTp1St7e3po/f778/Pxqs3TApTBRDmA8tdbDX7lypaZN\nm6b8/HxJ0rx58xQbG6v169fLarUqPT1dJ0+eVHJyslJTU/XGG28oISFBBQUFevvttxUYGKj169dr\n4MCBSkpKqq2yAQBwCbXWw/f391diYqImT54sSTp06JBCQ0MlSWFhYdq1a5fc3NzUvn17eXp6ytPT\nU/7+/srMzJTFYtGIESNszyXwAefC3fMAx6u1wI+IiFB29v8/u9dqtcpkMkmSvL29lZOTo9zcXJnN\nZttzvL29lZubW2r5pefay2Kx2LXMmdGeus3V2lMd6tJ7UpdqqS6u1ibaUz0cdlmem9v/P5qQl5cn\nX19f+fj4KC8vr9Rys9lcavml59qrY8eOpR5bLJYyy5wZ7anbHNIeJ7hsrq78jF3t8ya5Xptoz/Vv\n/2ocdpZ+69atlZGRIUnauXOnOnXqpJCQEFksFuXn5ysnJ0dZWVkKDAxUhw4dtGPHDttzXemHDwBA\nbXBYDz8uLk7Tp09XQkKCAgICFBERIXd3d0VHRysqKkpWq1Xjx4+Xl5eXhg4dqri4OA0dOlT16tXT\nokWLHFU2AABOqVYD/9Zbb1VaWpokqXnz5kpJSSnznMjISEVGRpZa1qBBAy1durRWagQAwBUxtS6A\nWsdZ+0DtY6Y9AAAMgMAHAMAACHwAAAyAwAcAwAAIfAAADIDABwDAAAh8AAAMgMAHAMAACHwAAAyA\nwAcAwAAIfAAADIDABwDAALh5DuBkuPEMgMqghw8AgAHQwwdQ5zCKAVQ/Ah+o464MPwCoDAIfQJ1H\njx+oOo7hAwBgAPTwASfnCkP+rtAGoK6jhw8AgAEQ+AAAGACBDwCAARD4AAAYAIEPAIABEPgAABgA\ngQ8AgAEQ+AAAGACBDwCAARD4AAAYAIEPAIABMJc+4GDcCQ5AbaCHDwCAARD4AAAYAEP6QB3DrWIB\n1AR6+AAAGACBDwCAARD4AAAYAIEPAIABEPgAABgAgQ8AgAFwWR4Ap8dshcC1OTTwCwsLNWXKFP36\n669yc3PTrFmz5OHhoSlTpshkMqlly5aaMWOG3NzclJaWptTUVHl4eCgmJkbh4eGOLB0AAKfi0MDf\nsWOHioqKlJqaql27dmnJkiUqLCxUbGysOnfurPj4eKWnp6tdu3ZKTk7Wxo0blZ+fr6ioKHXr1k2e\nnp6OLB8AAKfh0GP4zZs3V3FxsUpKSpSbmysPDw8dOnRIoaGhkqSwsDDt3r1bBw8eVPv27eXp6Smz\n2Sx/f39lZmY6snQAAJyKQ3v4DRs21K+//qr77rtPZ86c0bJly7R//36ZTCZJkre3t3JycpSbmyuz\n2Wx7nbe3t3Jzc+3ah8VisWuZM6M9dZurtacuuNZ7ej3vuSv+fFytTbSnejg08N988011795dEydO\n1PHjxzVs2DAVFhba1ufl5cnX11c+Pj7Ky8srtfzyLwAV6dixY6nHFoulzDJnRnvqNrvasz67dopx\nITOv8Z7Z+xlytc+b5Hptoj3Xv/2rceiQvq+vry24GzVqpKKiIrVu3VoZGRmSpJ07d6pTp04KCQmR\nxWJRfn6+cnJylJWVpcDAQEeWDgCAU3FoD3/48OGaOnWqoqKiVFhYqPHjxys4OFjTp09XQkKCAgIC\nFBERIXd3d0VHRysqKkpWq1Xjx4+Xl5eXI0sHAMCpODTwvb299fe//73M8pSUlDLLIiMjFRkZWRtl\nAQDgcphpDwAAAyDwAQAwAAIfAAADIPABADAAAh8AAAMg8AEAMAACHwAAA3DodfgAUBP6T/yg1OPN\niwY4qBKg7qCHDwCAARD4AAAYAIEPAIABEPgAABgAgQ8AgAFwlj5QwzhjHEBdQA8fAAADIPABADAA\nAh8AAAMg8AEAMAACHwAAA+AsfQAujyslAHr4AAAYAoEPAIABEPgAABgAgQ8AgAFw0h5Qy648gQwA\nagOBD1QzAh1AXcSQPgAABlAtgX/69Onq2AwAAKghdgf+nXfeWW6wZ2dn6957763WogAAQPWq8Bj+\ne++9p3fffVeSZLVaFRMTIw+P0i85efKkbrzxxpqrEAAAVFmFgR8REaFff/1VkmSxWNShQwd5e3uX\neo63t7f69OlTcxUCAIAqqzDwGzZsqDFjxkiSmjVrpn79+snLy6tWCgMAANXH7svyHnroIWVlZenf\n//63ioqKZLVaS60fNGhQtRcHAACqh92Bv2LFCiUkJKhRo0ZlhvVNJhOBDwBAHWZ34K9Zs0aTJk3S\nU089VZP1AACAGmB34BcWFnJyHgCXUGo2xPXZkqTNiwY4qBqgdth9Hf6AAQO0bt26MsfuAQBA3Wd3\nD//MmTPaunWrNm/erGbNmqlevXql1q9bt67aiwMAANXD7sAPCAjQqFGjarIWAABQQ+wO/EvX4wMA\nAOdjd+BPnjy5wvWvvvpqlYsBAAA1w+7Ad3d3L/W4qKhIv/zyiw4fPqxhw4ZVuoDly5fr008/VWFh\noYYOHarQ0FBNmTJFJpNJLVu21IwZM+Tm5qa0tDSlpqbKw8NDMTExCg8Pr/Q+AeBKpc7cF2ftw/XY\nHfjz5s0rd/maNWv03XffVWrnGRkZ+vrrr/X222/rwoULWr16tebNm6fY2Fh17txZ8fHxSk9PV7t2\n7ZScnKyNGzcqPz9fUVFR6tatmzw9PSu1XwAAjMbuy/Kupnfv3tq+fXulXvuvf/1LgYGBGj16tEaN\nGqV77rlHhw4dUmhoqCQpLCxMu3fv1sGDB9W+fXt5enrKbDbL399fmZmZVS0dAADDsLuHX1JSUmZZ\nXl6eUlNT1aRJk0rt/MyZMzp27JiWLVum7OxsxcTEyGq1ymQySbp4J76cnBzl5ubKbDbbXuft7a3c\n3NxK7ROoblcOBQNAXWR34Ldu3doWxJfz8vLS7NmzK7Xzxo0bKyAgQJ6engoICJCXl5d+++032/q8\nvDz5+vrKx8dHeXl5pZZf/gWgIhaLxa5lzoz2ANXPmT+Hzlx7eWhP9bA78NeuXVvqsclkUr169dSi\nRQv5+PhUaucdO3bU2rVr9cQTT+g///mPLly4oK5duyojI0OdO3fWzp071aVLF4WEhGjJkiXKz89X\nQUGBsrKyFBgYaPc+LmexWMosc2a0pw74v6lZ4Vqc7nP4f5zyd6gCtOf6t381dgf+pePqWVlZysrK\nUnFxsZo3b17psJek8PBw7d+/X4MGDZLValV8fLxuvfVWTZ8+XQkJCQoICFBERITc3d0VHR2tqKgo\nWa1WjR8/Xl5eXpXeLwAARmN34J89e1ZxcXH6/PPP1ahRIxUXFysvL0+dOnVSUlKS3UPsVyrv+v6U\nlJQyyyIjIxUZGVmpfQAAYHR2n6U/a9YsnTx5Ulu2bFFGRoa+/PJLbd68WRcuXLjqJXsAAKBusDvw\nP/vsM7300ksKCAiwLWvRooXtWnkAAFB32T2kX79+/XKXm0wmFRcXV1tBQF3HZXgAnJHdPfxevXrp\n5Zdf1o8//mhbdvToUc2aNYtpbgEAqOPs7uFPmjRJo0eP1n333Wc7Mz8vL089e/bU9OnTa6xAAABQ\ndXYF/sGDBxUUFKTk5GQdOXJEWVlZKigo0K233qpOnTrVdI0AAKCKKhzSLyoq0qRJkzR48GAdOHBA\nkhQUFKR+/fppx44dio6O1rRp0ziGDwBAHVdh4K9evVoZGRlau3atbeKdSxYvXqw1a9YoPT1dycnJ\nNVokAAComgoD/7333tP06dN19913l7u+S5cumjx5st59990aKQ4AAFSPCo/hHz9+XK1bt65wA506\nddJLL71UrUUBgKNdefnl5kUDHFQJUD0q7OH/5S9/UXZ2xTcGOXbsWKVvjwsAAGpHhYHfu3dvJSYm\nqrCwsNz1hYWFeu211xQWFlYjxQEAgOpR4ZD+s88+q0GDBunhhx9WdHS0goODZTabdfbsWR08eFDr\n1q1Tfn6+EhISaqteAABQCRUGvtlsVlpamhYsWKBXXnlFFy5ckCRZrVY1atRIDzzwgEaPHi0/P79a\nKRYA6iqO+aOuu+bEO40aNdLs2bMVHx+vX375RefOnVOTJk3k7+8vNze7Z+YFAAAOZPfUup6enrrj\njjtqshYAAFBD6KIDAGAABD4AAAZA4AMAYAAEPgAABmD3SXuAUV15uRUAOCN6+AAAGACBDwCAARD4\nAAAYAIEPAIABEPgAABgAgQ8AgAEQ+AAAGACBDwCAATDxDgDYgQmY4Ozo4QMAYAAEPgAABkDgAwBg\nAAQ+AAAGQOADAGAABD4AAAbAZXnAFbj8CoAroocPAIABEPgAABgAgQ8AgAEQ+AAAGACBDwCAAdSJ\ns/RPnTqlhx9+WKtXr5aHh4emTJkik8mkli1basaMGXJzc1NaWppSU1Pl4eGhmJgYhYeHO7psuAjO\nygdgBA7v4RcWFio+Pl7169eXJM2bN0+xsbFav369rFar0tPTdfLkSSUnJys1NVVvvPGGEhISVFBQ\n4ODKAQBwHg4P/Pnz52vIkCG68cYbJUmHDh1SaGioJCksLEy7d+/WwYMH1b59e3l6espsNsvf31+Z\nmZmOLBsAAKfi0CH9TZs2yc/PTz169NCKFSskSVarVSaTSZLk7e2tnJwc5ebmymw2217n7e2t3Nxc\nu/ZhsVjsWubMaA9Q9zjyc+xqv0O0p3o4NPA3btwok8mkPXv26PDhw4qLi9Pp06dt6/Py8uTr6ysf\nHx/l5eWVWn75F4CKdOzYsdRji8VSZpkzoz3Xj2P2qA2O+r3kb0LdVtPtqejLhEOH9NetW6eUlBQl\nJyfrzjvNMJfGAAANw0lEQVTv1Pz58xUWFqaMjAxJ0s6dO9WpUyeFhITIYrEoPz9fOTk5ysrKUmBg\noCNLBwDAqdSJs/QvFxcXp+nTpyshIUEBAQGKiIiQu7u7oqOjFRUVJavVqvHjx8vLy8vRpQIA4DTq\nTOAnJyfb/p+SklJmfWRkpCIjI2uzJAAAXIbDz9IHAAA1r8708IGawkl6AEAPHwAAQyDwAQAwAAIf\nAAADIPABADAAAh8AAAMg8AEAMAACHwAAAyDwAQAwAAIfAAADIPABADAAAh8AAAMg8AEAMABungMA\nNeDKmzZtXjTAQZUAF9HDBwDAAAh8AAAMgMAHAMAAOIYPALWAY/pwNAIfLufKP6wAAIb0AQAwBHr4\nAOAADPGjttHDBwDAAAh8AAAMgMAHAMAACHwAAAyAwAcAwAAIfAAADIDABwDAALgOHwDqAK7LR02j\nhw8AgAEQ+AAAGACBDwCAAXAMH06NO+MBgH3o4QMAYAAEPgAABkDgAwBgAAQ+AAAGQOADAGAABD4A\nAAZA4AMAYABchw8AToC59lFVDg38wsJCTZ06Vb/++qsKCgoUExOjFi1aaMqUKTKZTGrZsqVmzJgh\nNzc3paWlKTU1VR4eHoqJiVF4eLgjSwcAwKk4NPD/8Y9/qHHjxlqwYIH++OMPDRw4UK1atVJsbKw6\nd+6s+Ph4paenq127dkpOTtbGjRuVn5+vqKgodevWTZ6eno4sHwAAp+HQwO/bt68iIiIkSVarVe7u\n7jp06JBCQ0MlSWFhYdq1a5fc3NzUvn17eXp6ytPTU/7+/srMzFRISIgjy0ctYBgTAKqHQwPf29tb\nkpSbm6vnnntOsbGxmj9/vkwmk219Tk6OcnNzZTabS70uNzfXrn1YLBa7ljkzI7XH1doKXM217hNh\npN8T2lM9HH7S3vHjxzV69GhFRUWpf//+WrBggW1dXl6efH195ePjo7y8vFLLL/8CUJGOHTuWemyx\nWMosc2Yu35712aXWl2nrFesBo7ja773L/01wcjXdnoq+TDj0srzff/9dTz75pCZNmqRBgwZJklq3\nbq2MjAxJ0s6dO9WpUyeFhITIYrEoPz9fOTk5ysrKUmBgoCNLBwDAqTi0h79s2TKdO3dOSUlJSkpK\nkiS9+OKLmj17thISEhQQEKCIiAi5u7srOjpaUVFRslqtGj9+vLy8vBxZOhyE2+ECQOU4NPCnTZum\nadOmlVmekpJSZllkZKQiIyNroywAAFwOM+0BAGAABD4AAAZA4AMAYAAOvywPAHD9mJQK14sePgAA\nBkAPH3XKzPXZTKYDADWAHj4AAAZADx8AXECpY/rrszmmjzLo4QMAYAAEPgAABkDgAwBgABzDR63i\n2mHAMfjdA4EPh+LudwBQOxjSBwDAAAh8AAAMgMAHAMAACHwAAAyAk/YAwAVd64RYzto3Hnr4AAAY\nAIEPAIABEPgAABgAx/BRo5hYBwDqBnr4AAAYAD18AEC5o3Gcue9aCHxUK4bwAaBuYkgfAAADIPAB\nADAAAh8AAAPgGD6qhGP2AOAc6OEDAGAABD4AAAbAkD6uC0P4AOCc6OEDAGAA9PBRIXr0gHFd6/ef\nmficC4EPAKiUK78Q8AWgbiPwUQo9egBwTQQ+AKBGMAJQt3DSHgAABkAP32AYsgcAYyLwAQDVgg5F\n3UbguxiOmQFwFlz2V7sIfBfHN24AdQV/jxzLaQK/pKREM2fO1JEjR+Tp6anZs2frtttuc3RZNcqe\n3vrM9dnS+uzaKgkAao3tb+BV/sYxAnB9nCbwt2/froKCAm3YsEHffPONXnnlFb3++uuOLgsAYCDO\nfNjUaQLfYrGoR48ekqR27drp3//+d63XcK0f9PWur+r+AcDIqnoOgCPC+8pR2dr8wmCyWq3WWttb\nFbz44ovq06ePevbsKUm65557tH37dnl4XP07i8Viqa3yAACoEzp27Fjucqfp4fv4+CgvL8/2uKSk\npMKwl67eaAAAjMZpZtrr0KGDdu7cKUn65ptvFBgY6OCKAABwHk4zpH/pLP3vv/9eVqtVc+fO1R13\n3OHosgAAcApOE/gAAKDynGZIHwAAVB6BDwCAARgy8LOystSxY0fl5+c7upQqO3/+vGJiYvTYY49p\n+PDhOnHihKNLqpKcnByNGjVKjz/+uAYPHqyvv/7a0SVVi23btmnixImOLqPSSkpKFB8fr8GDBys6\nOlo///yzo0uqFgcOHFB0dLSjy6iywsJCTZo0SVFRURo0aJDS09MdXVKVFBcX64UXXtCQIUM0dOhQ\nff/9944uqVqcOnVKPXv2VFZWlkP2b7jAz83N1fz58+Xp6enoUqpFWlqa2rRpo3Xr1unBBx/UypUr\nHV1SlaxZs0ZdunRRSkqK5s2bp5dfftnRJVXZ7NmztWjRIpWUlDi6lEq7fKbLiRMn6pVXXnF0SVW2\ncuVKTZs2zSW++P/jH/9Q48aNtX79eq1atUqzZs1ydElV8tlnn0mSUlNTFRsbq8WLFzu4oqorLCxU\nfHy86tev77AaDBX4VqtV06dP14QJE9SgQQNHl1Mthg8frpiYGEnSsWPH5Ovr6+CKqmb48OEaMmSI\npIvf8r28vBxcUdV16NBBM2fOdHQZVVIXZrqsbv7+/kpMTHR0GdWib9++GjdunKSLf+fc3d0dXFHV\n/Pd//7ftS4sr/F2TpPnz52vIkCG68cYbHVaD00y8c73eeecdvfXWW6WW3XLLLerXr59atWrloKqq\nprw2zZ07VyEhIfqf//kfff/991qzZo2Dqrt+FbXn5MmTmjRpkqZOneqg6q7f1drTr18/ZWRkOKiq\n6pGbmysfHx/bY3d3dxUVFV1z8qu6LCIiQtnZrnHjKW9vb0kXf07PPfecYmNjHVxR1Xl4eCguLk7b\ntm3T0qVLHV1OlWzatEl+fn7q0aOHVqxY4bA6DHVZXu/evdW0aVNJFyfvCQkJ0bp16xxcVfXJysrS\nyJEjtX37dkeXUiVHjhzRhAkTNHnyZNtUys4uIyNDqampTjs0OW/ePN11113q16+fJCksLMw2EZYz\ny87O1oQJE5SWluboUqrs+PHjGj16tO04vqs4efKkIiMj9dFHH6lhw4aOLqdSHnvsMZlMJplMJh0+\nfFi33367Xn/9df3Xf/1XrdbhvF/PK2Hbtm22//fq1UurV692YDXVY/ny5brppps0cOBAeXt7O/1Q\n3v/+7/9q3LhxWrJkidOOxLiiDh066LPPPlO/fv2Y6bIO+v333/Xkk08qPj5eXbt2dXQ5Vfb+++/r\nxIkTGjlypBo0aCCTySQ3N+c9An15xzI6OlozZ86s9bCXDBb4ruiRRx5RXFycNm7cqOLiYs2dO9fR\nJVXJokWLVFBQoDlz5ki6eA8FboPseL1799auXbs0ZMgQ20yXqDuWLVumc+fOKSkpSUlJSZIunpTo\nyBPEqqJPnz564YUX9Nhjj6moqEhTp0512rbUJYYa0gcAwKicd4wEAADYjcAHAMAACHwAAAyAwAcA\nwAAIfAAADIDAB2ATFRV11VnaPv/8cwUHB+vMmTNXfX1RUZGCgoKcfmZBwBUR+ABs+vfvrx07dpR7\nQ5ktW7aoe/fuatKkiQMqA1BVBD4Am759+6qwsFBffPFFqeUFBQX69NNP9eCDDzqoMgBVReADsGnS\npIm6d++ujz/+uNTynTt3qqSkRL169VJubq5eeOEFde3aVcHBwerbt+9V798QFhamTZs22R7v3r1b\nQUFBtsfHjh3TqFGj1K5dO4WHh2vx4sUqLCysmcYBBkfgAyjlgQce0Oeff66CggLbsn/+85/q06eP\n6tevr9mzZ+vnn3/WmjVr9OGHH6p9+/Z68cUXSz3fHiUlJXr22Wd1ww03aOPGjXr11Ve1bds2LVmy\npLqbBEAEPoAr3HvvvSouLtbu3bslSfn5+fr000/Vv39/SVKnTp300ksvqVWrVrr99tv15JNP6o8/\n/tCpU6euaz+7du3SiRMnNGvWLN1xxx26++67NX36dKWkpKikpKTa2wUYHTfPAVBKgwYNdO+99+qT\nTz7RPffcox07dsjb21tdunSRJD388MPaunWrUlNTdfToUR06dEiSVFxcfF37ycrK0h9//KGOHTva\nllmtVv355586fvy4mjVrVn2NAkDgAyirf//+mjx5soqKirRlyxb169fPduvliRMn6uDBgxowYICi\noqLk5+enqKiocrdjMplKPS4qKrL9v7i4WAEBAba7u13OEbcOBVwdgQ+gjG7dusnNzU179uzRjh07\nlJycLEk6e/astmzZonfeeUchISGSpPT0dEkXe+dXqlevnvLy8myPf/nlF9v/mzdvrmPHjsnPz09m\ns1mStG/fPq1fv16vvvpqjbUNMCqO4QMow8PDQ/fdd58WLVqkm266ScHBwZKk+vXrq0GDBtq6dauy\ns7O1c+dOzZkzR5LKPWmvbdu22rhxo3744QdlZGRo7dq1tnVhYWFq2rSpnn/+eWVmZuqrr77StGnT\n5O7uLk9Pz9ppKGAgBD6AcvXv31+HDx+2nawnSV5eXnr11Vf18ccf6/7779f8+fM1evRo/eUvf9Hh\nw4fLbGPChAny9vbWQw89pDlz5mjcuHG2dR4eHlq+fLmsVquGDBmiZ599Vl26dNGsWbNqpX2A0Zis\n5Y3DAQAAl0IPHwAAAyDwAQAwAAIfAAADIPABADAAAh8AAAMg8AEAMAACHwAAAyDwAQAwAAIfAAAD\n+H9wBBDVslwDhwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117f64cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It appears to be a normal distribution that is centered around 0 with a variance of 1\n"
     ]
    }
   ],
   "source": [
    "## Generate random synthetic data set (250 users x 250 movies)\n",
    "X = numpy.random.randn(250, 250) # 250x250 = 62,500 complete data set (no missing values)\n",
    "\n",
    "# extract subset of data (Assume this is only given data)\n",
    "n = 35000\n",
    "i = numpy.random.randint(250, size=n) # Generate random row indexes\n",
    "j = numpy.random.randint(250, size=n) # Generate random column indexes\n",
    "X_values = X[i, j]\n",
    "print(\"Synthetic data set is generated. [row num : {}]\".format(X_values.shape[0]))\n",
    "\n",
    "# distribution of extracted values\n",
    "plt.title(\"Distribution of Data in Matrix\", fontsize=16)\n",
    "plt.ylabel(\"Count\", fontsize=14)\n",
    "plt.xlabel(\"Value\", fontsize=14)\n",
    "plt.hist(X_values, bins=100)\n",
    "plt.show()\n",
    "\n",
    "print('It appears to be a normal distribution that is centered around 0 with a variance of 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T08:42:28.985766Z",
     "start_time": "2018-02-08T08:42:28.974678Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Make Vanila Matrix Factorization(MF) using Gluon hybrid-block \n",
    "class Vanilla_MF(gluon.HybridBlock):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Vanilla_MF, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.user = nn.Embedding(input_dim=250, output_dim=25) # Embedding Layer for Users (25 latent factors)\n",
    "            self.movie = nn.Embedding(input_dim=250, output_dim=25) # Embedding Layer for Movies (25 latent factors)\n",
    "            self.flat = nn.Flatten()\n",
    "            \n",
    "    def hybrid_forward(self, F, user_i, movie_i):\n",
    "        # Here `F` can be either mx.nd or mx.sym, x is the input data,\n",
    "        user_i = self.user(user_i)\n",
    "        movie_i = self.movie(movie_i)\n",
    "        z = F.sum(F.dot(user_i, movie_i, transpose_b=True), axis = 1) # Dot product of users x movies and rowwise sum of result\n",
    "#         z = self.flat(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T08:42:28.999412Z",
     "start_time": "2018-02-08T08:42:28.989080Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model initialization\n",
    "model = Vanilla_MF()\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx=ctx)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "criterion = gluon.loss.L2Loss()\n",
    "optimizer = gluon.Trainer(model.collect_params(), 'adam',\n",
    "                          {'learning_rate': learning_rate,\n",
    "                           'wd': 1e-5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T08:42:29.011054Z",
     "start_time": "2018-02-08T08:42:29.002495Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User embedding layer shape : (250, 25)\n",
      "\n",
      "Item embedding layer shape : (250, 25)\n",
      "\n",
      "25 Embedding latents factors of the first user : \n",
      "[ 0.08690061  0.11293826 -0.10049446  0.12735233 -0.03226358  0.10438174\n",
      "  0.12894627 -0.00171298  0.04623531 -0.14136802  0.01747404  0.06442726\n",
      "  0.05782974 -0.0706878  -0.11482002 -0.03529139 -0.03970727 -0.08992843\n",
      "  0.09868617  0.060701   -0.09062057 -0.02898844  0.10589617  0.02242789\n",
      " -0.11005463]\n"
     ]
    }
   ],
   "source": [
    "# Embedding layer latent factors\n",
    "user_latent_v = model.user.weight.data().asnumpy()\n",
    "print(\"User embedding layer shape : {}\\n\".format(user_latent_v.shape))\n",
    "item_latent_v = model.movie.weight.data().asnumpy()\n",
    "print(\"Item embedding layer shape : {}\\n\".format(item_latent_v.shape))\n",
    "\n",
    "print(\"25 Embedding latents factors of the first user : \\n{}\".format(user_latent_v[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T08:42:29.023925Z",
     "start_time": "2018-02-08T08:42:29.013103Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product of 1th user and 2th movie : -0.0330\n",
      "Model result of 1th user and 2th movie : -0.0330\n"
     ]
    }
   ],
   "source": [
    "# User rating sample by dot product of user x item embedding matrix\n",
    "user_i = 1\n",
    "movie_i = 2\n",
    "\n",
    "# naive dot product example\n",
    "user_v = user_latent_v[user_i]\n",
    "movie_v = item_latent_v[movie_i]\n",
    "print(\"Dot product of {}th user and {}th movie : {:.4f}\".format(user_i, movie_i, np.dot(user_v,movie_v)))\n",
    "\n",
    "# dot product using gluon network \n",
    "user_n = mx.nd.array([user_i])\n",
    "item_n = mx.nd.array([movie_i])\n",
    "print(\"Model result of {}th user and {}th movie : {:.4f}\".format(user_i, movie_i, model(user_n, item_n).asnumpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T08:42:29.030193Z",
     "start_time": "2018-02-08T08:42:29.025319Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make batch input using gluon DataLoader\n",
    "batch_size = 10000\n",
    "X_train = gluon.data.DataLoader(gluon.data.ArrayDataset(i[:25000].astype('float32'), j[:25000].astype('float32'), X_values[:25000].astype('float32')),\n",
    "                                      batch_size=batch_size, shuffle=True)\n",
    "\n",
    "X_eval = gluon.data.DataLoader(gluon.data.ArrayDataset(i[25000:].astype('float32'), j[25000:].astype('float32'), X_values[25000:].astype('float32')),\n",
    "                                      batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T08:42:29.054318Z",
     "start_time": "2018-02-08T08:42:29.031650Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Epoch Running Function\n",
    "def run_epoch(model, X_train, X_eval, num_epochs = 20):\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        n_total = 0.0\n",
    "\n",
    "        # for training\n",
    "        for user, movie, rating in X_train:\n",
    "            user = user.as_in_context(ctx)\n",
    "            movie = movie.as_in_context(ctx)\n",
    "            rating = rating.as_in_context(ctx)\n",
    "\n",
    "            with mx.autograd.record():\n",
    "                output = model(user, movie)\n",
    "                loss = criterion(output, rating)\n",
    "            loss.backward()\n",
    "            optimizer.step(user.shape[0])\n",
    "            running_loss += mx.nd.sum(loss).asscalar()\n",
    "            n_total += user.shape[0]\n",
    "\n",
    "        for val_user, val_movie, val_rating in X_eval:\n",
    "            val_user = val_user.as_in_context(ctx)\n",
    "            val_movie = val_movie.as_in_context(ctx)\n",
    "\n",
    "            with mx.autograd.record():\n",
    "                val_output = model(val_user, val_movie)\n",
    "                val_loss_tmp = criterion(val_output, val_rating)\n",
    "            val_loss += mx.nd.sum(val_loss_tmp).asscalar()\n",
    "\n",
    "        # ===================log========================\n",
    "        print('epoch [{}/{}], (rmse) loss:{:.4f}, (rmse) val_loss:{:.4f}'\n",
    "              .format(epoch + 1, num_epochs, np.sqrt(running_loss / n_total), np.sqrt(val_loss / n_total)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T08:44:47.273326Z",
     "start_time": "2018-02-08T08:42:29.056161Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/20], (rmse) loss:15.2509, (rmse) val_loss:10.1072\n",
      "epoch [2/20], (rmse) loss:14.2606, (rmse) val_loss:9.5894\n",
      "epoch [3/20], (rmse) loss:13.2654, (rmse) val_loss:9.1010\n",
      "epoch [4/20], (rmse) loss:12.7117, (rmse) val_loss:8.6405\n",
      "epoch [5/20], (rmse) loss:11.9225, (rmse) val_loss:8.2088\n",
      "epoch [6/20], (rmse) loss:11.5516, (rmse) val_loss:7.8047\n",
      "epoch [7/20], (rmse) loss:10.5129, (rmse) val_loss:7.4306\n",
      "epoch [8/20], (rmse) loss:10.1461, (rmse) val_loss:7.0836\n",
      "epoch [9/20], (rmse) loss:9.6921, (rmse) val_loss:6.7610\n",
      "epoch [10/20], (rmse) loss:8.9398, (rmse) val_loss:6.4634\n",
      "epoch [11/20], (rmse) loss:8.6287, (rmse) val_loss:6.1872\n",
      "epoch [12/20], (rmse) loss:8.2749, (rmse) val_loss:5.9286\n",
      "epoch [13/20], (rmse) loss:7.7057, (rmse) val_loss:5.6873\n",
      "epoch [14/20], (rmse) loss:7.3769, (rmse) val_loss:5.4616\n",
      "epoch [15/20], (rmse) loss:7.2322, (rmse) val_loss:5.2511\n",
      "epoch [16/20], (rmse) loss:6.5863, (rmse) val_loss:5.0548\n",
      "epoch [17/20], (rmse) loss:6.4270, (rmse) val_loss:4.8688\n",
      "epoch [18/20], (rmse) loss:6.2095, (rmse) val_loss:4.6924\n",
      "epoch [19/20], (rmse) loss:6.0212, (rmse) val_loss:4.5270\n",
      "epoch [20/20], (rmse) loss:5.5854, (rmse) val_loss:4.3698\n"
     ]
    }
   ],
   "source": [
    "## Model Running (Training)\n",
    "model = run_epoch(model, X_train, X_eval, num_epochs= 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Problem of the totally and randomly generated data set : no strong connections(dependencies) between two latent matrices\n",
    "- If there does not exist connections(dependencies) between two latent matrices, the performance of Vanilla MF does not seem to get increased well. \n",
    "- And this is because we generated the data using total random normal. \n",
    "- It is like  he \"movie watchers\" don't have any preferences, they just randomly like or don't like movies! It would be impossible to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T08:44:47.672416Z",
     "start_time": "2018-02-08T08:44:47.274820Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic data set is generated. [row num : 35000]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfwAAAFtCAYAAADiaNj1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11852f198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It appears to be a normal distribution that is centered around 0 but lager variance than original synthetic data set\n"
     ]
    }
   ],
   "source": [
    "## Generate synthetic data set using two row rank latent matrix (randomly generated) \n",
    "### -> It essentially makes the connections between latents matrix\n",
    "a = numpy.random.normal(0, 1, size=(250, 25)) # Generate random numbers for the first skinny matrix\n",
    "b = numpy.random.normal(0, 1, size=(25, 250)) # Generate random numbers for the second skinny matrix\n",
    "\n",
    "X = a.dot(b) # Build our real data matrix from the dot product of the two skinny matrices\n",
    "\n",
    "n = 35000\n",
    "i = numpy.random.randint(250, size=n)\n",
    "j = numpy.random.randint(250, size=n)\n",
    "X_values = X[i, j]\n",
    "print(\"Synthetic data set is generated. [row num : {}]\".format(X_values.shape[0]))\n",
    "\n",
    "# distribution of extracted values\n",
    "plt.title(\"Distribution of Data in Matrix\", fontsize=16)\n",
    "plt.ylabel(\"Count\", fontsize=14)\n",
    "plt.xlabel(\"Value\", fontsize=14)\n",
    "plt.hist(X_values, bins=100)\n",
    "plt.show()\n",
    "\n",
    "print('It appears to be a normal distribution that is centered around 0 but lager variance than original synthetic data set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T08:44:47.679426Z",
     "start_time": "2018-02-08T08:44:47.674481Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 10000\n",
    "X_train = gluon.data.DataLoader(gluon.data.ArrayDataset(i[:25000].astype('float32'), j[:25000].astype('float32'), X_values[:25000].astype('float32')),\n",
    "                                      batch_size=batch_size, shuffle=True)\n",
    "\n",
    "X_eval = gluon.data.DataLoader(gluon.data.ArrayDataset(i[25000:].astype('float32'), j[25000:].astype('float32'), X_values[25000:].astype('float32')),\n",
    "                                      batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T08:44:47.688048Z",
     "start_time": "2018-02-08T08:44:47.681114Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model initialization\n",
    "model = Vanilla_MF()\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx=ctx)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "criterion = gluon.loss.L2Loss()\n",
    "optimizer = gluon.Trainer(model.collect_params(), 'adam',\n",
    "                          {'learning_rate': learning_rate,\n",
    "                           'wd': 1e-5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T08:47:01.655536Z",
     "start_time": "2018-02-08T08:44:47.693828Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/20], (rmse) loss:12.8799, (rmse) val_loss:8.8683\n",
      "epoch [2/20], (rmse) loss:12.0156, (rmse) val_loss:8.4129\n",
      "epoch [3/20], (rmse) loss:11.4499, (rmse) val_loss:7.9831\n",
      "epoch [4/20], (rmse) loss:11.0393, (rmse) val_loss:7.5826\n",
      "epoch [5/20], (rmse) loss:10.1923, (rmse) val_loss:7.2090\n",
      "epoch [6/20], (rmse) loss:9.9205, (rmse) val_loss:6.8638\n",
      "epoch [7/20], (rmse) loss:9.1769, (rmse) val_loss:6.5462\n",
      "epoch [8/20], (rmse) loss:8.7725, (rmse) val_loss:6.2545\n",
      "epoch [9/20], (rmse) loss:8.3774, (rmse) val_loss:5.9883\n",
      "epoch [10/20], (rmse) loss:8.0528, (rmse) val_loss:5.7448\n",
      "epoch [11/20], (rmse) loss:7.6219, (rmse) val_loss:5.5244\n",
      "epoch [12/20], (rmse) loss:7.4772, (rmse) val_loss:5.3223\n",
      "epoch [13/20], (rmse) loss:7.0268, (rmse) val_loss:5.1380\n",
      "epoch [14/20], (rmse) loss:6.8421, (rmse) val_loss:4.9706\n",
      "epoch [15/20], (rmse) loss:6.9309, (rmse) val_loss:4.8176\n",
      "epoch [16/20], (rmse) loss:6.5381, (rmse) val_loss:4.6779\n",
      "epoch [17/20], (rmse) loss:6.1961, (rmse) val_loss:4.5530\n",
      "epoch [18/20], (rmse) loss:5.9431, (rmse) val_loss:4.4351\n",
      "epoch [19/20], (rmse) loss:5.9795, (rmse) val_loss:4.3288\n",
      "epoch [20/20], (rmse) loss:5.8344, (rmse) val_loss:4.2294\n"
     ]
    }
   ],
   "source": [
    "## Model Running (Training)\n",
    "model = run_epoch(model, X_train, X_eval, num_epochs= 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **2) Deep matrix Facotrization using synthetic data**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T07:24:16.068903Z",
     "start_time": "2018-02-08T07:24:16.064535Z"
    }
   },
   "source": [
    "- 기존 two latent matrix의 dot product 곱의 형태가 아닌 방식\n",
    "- Two latent layer(matrix)를 concat 후 Flatten하는 방식 사용\n",
    "- 이후 Regression 형태[0,1]의 결과값을 얻기 위해 Fully Connected Layer를 사용하여 Output 산출\n",
    "\n",
    "<br/>\n",
    "<img src=\"deep_mf.png\" width=\"700\">\n",
    "<br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T08:47:01.670538Z",
     "start_time": "2018-02-08T08:47:01.657064Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Deep_MF(gluon.HybridBlock):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Deep_MF, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            # layers created in name_scope will inherit name space\n",
    "            # from parent layer.\n",
    "            self.user = nn.Embedding(input_dim=250, output_dim=25)\n",
    "            self.movie = nn.Embedding(input_dim=250, output_dim=25)\n",
    "            \n",
    "            \n",
    "            self.out = gluon.nn.HybridSequential('output_')\n",
    "            with self.out.name_scope(): # Output layers to make regression result\n",
    "                self.out.add(nn.Flatten())\n",
    "                self.out.add(nn.Dense(64, activation='relu'))\n",
    "                self.out.add(nn.Dense(1)) # the last output layer\n",
    "            \n",
    "    def hybrid_forward(self, F, user_i, movie_i):\n",
    "        # Here `F` can be either mx.nd or mx.sym, x is the input data,\n",
    "        user_i = self.user(user_i)\n",
    "        movie_i = self.movie(movie_i)\n",
    "        \n",
    "        z = F.concat(user_i, movie_i) # Two latent layer(matrix) concat\n",
    "        z = self.out(z) # concat output -> flatten -> dense layer(64 dim) -> output layer(1 dim)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T08:47:01.680265Z",
     "start_time": "2018-02-08T08:47:01.672185Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model initialization\n",
    "model = Deep_MF()\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx=ctx)\n",
    "\n",
    "learning_rate = 1e-2 # 기존 Vanilla 모형에 비해 running 속도가 빠름, 그러나 학습속도가 느리기에 learning_rate를 높임\n",
    "criterion = gluon.loss.L2Loss()\n",
    "optimizer = gluon.Trainer(model.collect_params(), 'adam',\n",
    "                          {'learning_rate': learning_rate,\n",
    "                           'wd': 1e-5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T08:48:48.258564Z",
     "start_time": "2018-02-08T08:47:01.681742Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/500], (rmse) loss:3.5841, (rmse) val_loss:2.2236\n",
      "epoch [2/500], (rmse) loss:3.5731, (rmse) val_loss:2.2232\n",
      "epoch [3/500], (rmse) loss:3.5583, (rmse) val_loss:2.2265\n",
      "epoch [4/500], (rmse) loss:3.5422, (rmse) val_loss:2.2374\n",
      "epoch [5/500], (rmse) loss:3.5320, (rmse) val_loss:2.2434\n",
      "epoch [6/500], (rmse) loss:3.5210, (rmse) val_loss:2.2332\n",
      "epoch [7/500], (rmse) loss:3.5066, (rmse) val_loss:2.2224\n",
      "epoch [8/500], (rmse) loss:3.4913, (rmse) val_loss:2.2159\n",
      "epoch [9/500], (rmse) loss:3.4703, (rmse) val_loss:2.2097\n",
      "epoch [10/500], (rmse) loss:3.4425, (rmse) val_loss:2.2039\n",
      "epoch [11/500], (rmse) loss:3.4083, (rmse) val_loss:2.1948\n",
      "epoch [12/500], (rmse) loss:3.3685, (rmse) val_loss:2.1811\n",
      "epoch [13/500], (rmse) loss:3.3248, (rmse) val_loss:2.1694\n",
      "epoch [14/500], (rmse) loss:3.2774, (rmse) val_loss:2.1591\n",
      "epoch [15/500], (rmse) loss:3.2261, (rmse) val_loss:2.1441\n",
      "epoch [16/500], (rmse) loss:3.1673, (rmse) val_loss:2.1255\n",
      "epoch [17/500], (rmse) loss:3.1077, (rmse) val_loss:2.1110\n",
      "epoch [18/500], (rmse) loss:3.0502, (rmse) val_loss:2.0970\n",
      "epoch [19/500], (rmse) loss:2.9968, (rmse) val_loss:2.0842\n",
      "epoch [20/500], (rmse) loss:2.9393, (rmse) val_loss:2.0742\n",
      "epoch [21/500], (rmse) loss:2.8844, (rmse) val_loss:2.0571\n",
      "epoch [22/500], (rmse) loss:2.8324, (rmse) val_loss:2.0406\n",
      "epoch [23/500], (rmse) loss:2.7828, (rmse) val_loss:2.0293\n",
      "epoch [24/500], (rmse) loss:2.7395, (rmse) val_loss:2.0151\n",
      "epoch [25/500], (rmse) loss:2.6970, (rmse) val_loss:2.0027\n",
      "epoch [26/500], (rmse) loss:2.6599, (rmse) val_loss:1.9943\n",
      "epoch [27/500], (rmse) loss:2.6246, (rmse) val_loss:1.9829\n",
      "epoch [28/500], (rmse) loss:2.5945, (rmse) val_loss:1.9762\n",
      "epoch [29/500], (rmse) loss:2.5662, (rmse) val_loss:1.9683\n",
      "epoch [30/500], (rmse) loss:2.5370, (rmse) val_loss:1.9596\n",
      "epoch [31/500], (rmse) loss:2.5101, (rmse) val_loss:1.9544\n",
      "epoch [32/500], (rmse) loss:2.4863, (rmse) val_loss:1.9484\n",
      "epoch [33/500], (rmse) loss:2.4604, (rmse) val_loss:1.9430\n",
      "epoch [34/500], (rmse) loss:2.4388, (rmse) val_loss:1.9386\n",
      "epoch [35/500], (rmse) loss:2.4168, (rmse) val_loss:1.9312\n",
      "epoch [36/500], (rmse) loss:2.3966, (rmse) val_loss:1.9307\n",
      "epoch [37/500], (rmse) loss:2.3785, (rmse) val_loss:1.9296\n",
      "epoch [38/500], (rmse) loss:2.3598, (rmse) val_loss:1.9224\n",
      "epoch [39/500], (rmse) loss:2.3384, (rmse) val_loss:1.9219\n",
      "epoch [40/500], (rmse) loss:2.3201, (rmse) val_loss:1.9143\n",
      "epoch [41/500], (rmse) loss:2.3024, (rmse) val_loss:1.9099\n",
      "epoch [42/500], (rmse) loss:2.2874, (rmse) val_loss:1.9069\n",
      "epoch [43/500], (rmse) loss:2.2682, (rmse) val_loss:1.8985\n",
      "epoch [44/500], (rmse) loss:2.2489, (rmse) val_loss:1.8921\n",
      "epoch [45/500], (rmse) loss:2.2332, (rmse) val_loss:1.8915\n",
      "epoch [46/500], (rmse) loss:2.2158, (rmse) val_loss:1.8894\n",
      "epoch [47/500], (rmse) loss:2.2011, (rmse) val_loss:1.8853\n",
      "epoch [48/500], (rmse) loss:2.1842, (rmse) val_loss:1.8840\n",
      "epoch [49/500], (rmse) loss:2.1696, (rmse) val_loss:1.8803\n",
      "epoch [50/500], (rmse) loss:2.1517, (rmse) val_loss:1.8762\n",
      "epoch [51/500], (rmse) loss:2.1376, (rmse) val_loss:1.8751\n",
      "epoch [52/500], (rmse) loss:2.1229, (rmse) val_loss:1.8703\n",
      "epoch [53/500], (rmse) loss:2.1061, (rmse) val_loss:1.8640\n",
      "epoch [54/500], (rmse) loss:2.0908, (rmse) val_loss:1.8650\n",
      "epoch [55/500], (rmse) loss:2.0764, (rmse) val_loss:1.8647\n",
      "epoch [56/500], (rmse) loss:2.0622, (rmse) val_loss:1.8578\n",
      "epoch [57/500], (rmse) loss:2.0477, (rmse) val_loss:1.8551\n",
      "epoch [58/500], (rmse) loss:2.0309, (rmse) val_loss:1.8586\n",
      "epoch [59/500], (rmse) loss:2.0180, (rmse) val_loss:1.8539\n",
      "epoch [60/500], (rmse) loss:2.0051, (rmse) val_loss:1.8501\n",
      "epoch [61/500], (rmse) loss:1.9901, (rmse) val_loss:1.8465\n",
      "epoch [62/500], (rmse) loss:1.9731, (rmse) val_loss:1.8474\n",
      "epoch [63/500], (rmse) loss:1.9622, (rmse) val_loss:1.8384\n",
      "epoch [64/500], (rmse) loss:1.9450, (rmse) val_loss:1.8346\n",
      "epoch [65/500], (rmse) loss:1.9307, (rmse) val_loss:1.8320\n",
      "epoch [66/500], (rmse) loss:1.9157, (rmse) val_loss:1.8284\n",
      "epoch [67/500], (rmse) loss:1.8999, (rmse) val_loss:1.8261\n",
      "epoch [68/500], (rmse) loss:1.8862, (rmse) val_loss:1.8219\n",
      "epoch [69/500], (rmse) loss:1.8731, (rmse) val_loss:1.8162\n",
      "epoch [70/500], (rmse) loss:1.8597, (rmse) val_loss:1.8141\n",
      "epoch [71/500], (rmse) loss:1.8458, (rmse) val_loss:1.8114\n",
      "epoch [72/500], (rmse) loss:1.8334, (rmse) val_loss:1.8092\n",
      "epoch [73/500], (rmse) loss:1.8219, (rmse) val_loss:1.8089\n",
      "epoch [74/500], (rmse) loss:1.8091, (rmse) val_loss:1.8055\n",
      "epoch [75/500], (rmse) loss:1.7965, (rmse) val_loss:1.8010\n",
      "epoch [76/500], (rmse) loss:1.7851, (rmse) val_loss:1.7975\n",
      "epoch [77/500], (rmse) loss:1.7706, (rmse) val_loss:1.7975\n",
      "epoch [78/500], (rmse) loss:1.7590, (rmse) val_loss:1.7950\n",
      "epoch [79/500], (rmse) loss:1.7458, (rmse) val_loss:1.7928\n",
      "epoch [80/500], (rmse) loss:1.7353, (rmse) val_loss:1.7888\n",
      "epoch [81/500], (rmse) loss:1.7241, (rmse) val_loss:1.7865\n",
      "epoch [82/500], (rmse) loss:1.7121, (rmse) val_loss:1.7851\n",
      "epoch [83/500], (rmse) loss:1.7023, (rmse) val_loss:1.7814\n",
      "epoch [84/500], (rmse) loss:1.6910, (rmse) val_loss:1.7820\n",
      "epoch [85/500], (rmse) loss:1.6795, (rmse) val_loss:1.7835\n",
      "epoch [86/500], (rmse) loss:1.6710, (rmse) val_loss:1.7762\n",
      "epoch [87/500], (rmse) loss:1.6592, (rmse) val_loss:1.7767\n",
      "epoch [88/500], (rmse) loss:1.6467, (rmse) val_loss:1.7714\n",
      "epoch [89/500], (rmse) loss:1.6381, (rmse) val_loss:1.7709\n",
      "epoch [90/500], (rmse) loss:1.6295, (rmse) val_loss:1.7700\n",
      "epoch [91/500], (rmse) loss:1.6202, (rmse) val_loss:1.7652\n",
      "epoch [92/500], (rmse) loss:1.6083, (rmse) val_loss:1.7648\n",
      "epoch [93/500], (rmse) loss:1.6016, (rmse) val_loss:1.7627\n",
      "epoch [94/500], (rmse) loss:1.5897, (rmse) val_loss:1.7594\n",
      "epoch [95/500], (rmse) loss:1.5802, (rmse) val_loss:1.7592\n",
      "epoch [96/500], (rmse) loss:1.5704, (rmse) val_loss:1.7562\n",
      "epoch [97/500], (rmse) loss:1.5608, (rmse) val_loss:1.7532\n",
      "epoch [98/500], (rmse) loss:1.5523, (rmse) val_loss:1.7535\n",
      "epoch [99/500], (rmse) loss:1.5411, (rmse) val_loss:1.7529\n",
      "epoch [100/500], (rmse) loss:1.5327, (rmse) val_loss:1.7509\n",
      "epoch [101/500], (rmse) loss:1.5270, (rmse) val_loss:1.7481\n",
      "epoch [102/500], (rmse) loss:1.5166, (rmse) val_loss:1.7509\n",
      "epoch [103/500], (rmse) loss:1.5072, (rmse) val_loss:1.7485\n",
      "epoch [104/500], (rmse) loss:1.4970, (rmse) val_loss:1.7458\n",
      "epoch [105/500], (rmse) loss:1.4910, (rmse) val_loss:1.7462\n",
      "epoch [106/500], (rmse) loss:1.4806, (rmse) val_loss:1.7445\n",
      "epoch [107/500], (rmse) loss:1.4730, (rmse) val_loss:1.7438\n",
      "epoch [108/500], (rmse) loss:1.4644, (rmse) val_loss:1.7444\n",
      "epoch [109/500], (rmse) loss:1.4546, (rmse) val_loss:1.7441\n",
      "epoch [110/500], (rmse) loss:1.4476, (rmse) val_loss:1.7425\n",
      "epoch [111/500], (rmse) loss:1.4383, (rmse) val_loss:1.7422\n",
      "epoch [112/500], (rmse) loss:1.4313, (rmse) val_loss:1.7418\n",
      "epoch [113/500], (rmse) loss:1.4234, (rmse) val_loss:1.7410\n",
      "epoch [114/500], (rmse) loss:1.4165, (rmse) val_loss:1.7395\n",
      "epoch [115/500], (rmse) loss:1.4102, (rmse) val_loss:1.7405\n",
      "epoch [116/500], (rmse) loss:1.4038, (rmse) val_loss:1.7442\n",
      "epoch [117/500], (rmse) loss:1.3990, (rmse) val_loss:1.7391\n",
      "epoch [118/500], (rmse) loss:1.3910, (rmse) val_loss:1.7394\n",
      "epoch [119/500], (rmse) loss:1.3832, (rmse) val_loss:1.7417\n",
      "epoch [120/500], (rmse) loss:1.3752, (rmse) val_loss:1.7397\n",
      "epoch [121/500], (rmse) loss:1.3679, (rmse) val_loss:1.7392\n",
      "epoch [122/500], (rmse) loss:1.3613, (rmse) val_loss:1.7419\n",
      "epoch [123/500], (rmse) loss:1.3564, (rmse) val_loss:1.7401\n",
      "epoch [124/500], (rmse) loss:1.3481, (rmse) val_loss:1.7396\n",
      "epoch [125/500], (rmse) loss:1.3407, (rmse) val_loss:1.7388\n",
      "epoch [126/500], (rmse) loss:1.3354, (rmse) val_loss:1.7403\n",
      "epoch [127/500], (rmse) loss:1.3306, (rmse) val_loss:1.7380\n",
      "epoch [128/500], (rmse) loss:1.3231, (rmse) val_loss:1.7434\n",
      "epoch [129/500], (rmse) loss:1.3169, (rmse) val_loss:1.7399\n",
      "epoch [130/500], (rmse) loss:1.3094, (rmse) val_loss:1.7407\n",
      "epoch [131/500], (rmse) loss:1.3058, (rmse) val_loss:1.7459\n",
      "epoch [132/500], (rmse) loss:1.3048, (rmse) val_loss:1.7399\n",
      "epoch [133/500], (rmse) loss:1.2995, (rmse) val_loss:1.7383\n",
      "epoch [134/500], (rmse) loss:1.2937, (rmse) val_loss:1.7444\n",
      "epoch [135/500], (rmse) loss:1.2865, (rmse) val_loss:1.7364\n",
      "epoch [136/500], (rmse) loss:1.2791, (rmse) val_loss:1.7389\n",
      "epoch [137/500], (rmse) loss:1.2733, (rmse) val_loss:1.7392\n",
      "epoch [138/500], (rmse) loss:1.2687, (rmse) val_loss:1.7398\n",
      "epoch [139/500], (rmse) loss:1.2628, (rmse) val_loss:1.7412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [140/500], (rmse) loss:1.2566, (rmse) val_loss:1.7404\n",
      "epoch [141/500], (rmse) loss:1.2516, (rmse) val_loss:1.7408\n",
      "epoch [142/500], (rmse) loss:1.2461, (rmse) val_loss:1.7390\n",
      "epoch [143/500], (rmse) loss:1.2435, (rmse) val_loss:1.7427\n",
      "epoch [144/500], (rmse) loss:1.2412, (rmse) val_loss:1.7416\n",
      "epoch [145/500], (rmse) loss:1.2335, (rmse) val_loss:1.7420\n",
      "epoch [146/500], (rmse) loss:1.2284, (rmse) val_loss:1.7416\n",
      "epoch [147/500], (rmse) loss:1.2232, (rmse) val_loss:1.7437\n",
      "epoch [148/500], (rmse) loss:1.2167, (rmse) val_loss:1.7421\n",
      "epoch [149/500], (rmse) loss:1.2135, (rmse) val_loss:1.7437\n",
      "epoch [150/500], (rmse) loss:1.2087, (rmse) val_loss:1.7454\n",
      "epoch [151/500], (rmse) loss:1.2076, (rmse) val_loss:1.7422\n",
      "epoch [152/500], (rmse) loss:1.2019, (rmse) val_loss:1.7421\n",
      "epoch [153/500], (rmse) loss:1.1982, (rmse) val_loss:1.7448\n",
      "epoch [154/500], (rmse) loss:1.1934, (rmse) val_loss:1.7412\n",
      "epoch [155/500], (rmse) loss:1.1918, (rmse) val_loss:1.7449\n",
      "epoch [156/500], (rmse) loss:1.1855, (rmse) val_loss:1.7429\n",
      "epoch [157/500], (rmse) loss:1.1793, (rmse) val_loss:1.7415\n",
      "epoch [158/500], (rmse) loss:1.1743, (rmse) val_loss:1.7444\n",
      "epoch [159/500], (rmse) loss:1.1706, (rmse) val_loss:1.7408\n",
      "epoch [160/500], (rmse) loss:1.1654, (rmse) val_loss:1.7420\n",
      "epoch [161/500], (rmse) loss:1.1621, (rmse) val_loss:1.7403\n",
      "epoch [162/500], (rmse) loss:1.1580, (rmse) val_loss:1.7411\n",
      "epoch [163/500], (rmse) loss:1.1544, (rmse) val_loss:1.7401\n",
      "epoch [164/500], (rmse) loss:1.1520, (rmse) val_loss:1.7415\n",
      "epoch [165/500], (rmse) loss:1.1462, (rmse) val_loss:1.7395\n",
      "epoch [166/500], (rmse) loss:1.1435, (rmse) val_loss:1.7409\n",
      "epoch [167/500], (rmse) loss:1.1424, (rmse) val_loss:1.7410\n",
      "epoch [168/500], (rmse) loss:1.1400, (rmse) val_loss:1.7449\n",
      "epoch [169/500], (rmse) loss:1.1365, (rmse) val_loss:1.7416\n",
      "epoch [170/500], (rmse) loss:1.1341, (rmse) val_loss:1.7376\n",
      "epoch [171/500], (rmse) loss:1.1279, (rmse) val_loss:1.7423\n",
      "epoch [172/500], (rmse) loss:1.1262, (rmse) val_loss:1.7389\n",
      "epoch [173/500], (rmse) loss:1.1215, (rmse) val_loss:1.7426\n",
      "epoch [174/500], (rmse) loss:1.1180, (rmse) val_loss:1.7380\n",
      "epoch [175/500], (rmse) loss:1.1144, (rmse) val_loss:1.7440\n",
      "epoch [176/500], (rmse) loss:1.1136, (rmse) val_loss:1.7416\n",
      "epoch [177/500], (rmse) loss:1.1080, (rmse) val_loss:1.7379\n",
      "epoch [178/500], (rmse) loss:1.1032, (rmse) val_loss:1.7405\n",
      "epoch [179/500], (rmse) loss:1.1004, (rmse) val_loss:1.7388\n",
      "epoch [180/500], (rmse) loss:1.0986, (rmse) val_loss:1.7399\n",
      "epoch [181/500], (rmse) loss:1.0946, (rmse) val_loss:1.7392\n",
      "epoch [182/500], (rmse) loss:1.0955, (rmse) val_loss:1.7413\n",
      "epoch [183/500], (rmse) loss:1.0921, (rmse) val_loss:1.7387\n",
      "epoch [184/500], (rmse) loss:1.0913, (rmse) val_loss:1.7373\n",
      "epoch [185/500], (rmse) loss:1.0866, (rmse) val_loss:1.7397\n",
      "epoch [186/500], (rmse) loss:1.0800, (rmse) val_loss:1.7381\n",
      "epoch [187/500], (rmse) loss:1.0778, (rmse) val_loss:1.7394\n",
      "epoch [188/500], (rmse) loss:1.0745, (rmse) val_loss:1.7364\n",
      "epoch [189/500], (rmse) loss:1.0699, (rmse) val_loss:1.7375\n",
      "epoch [190/500], (rmse) loss:1.0697, (rmse) val_loss:1.7422\n",
      "epoch [191/500], (rmse) loss:1.0707, (rmse) val_loss:1.7401\n",
      "epoch [192/500], (rmse) loss:1.0662, (rmse) val_loss:1.7381\n",
      "epoch [193/500], (rmse) loss:1.0649, (rmse) val_loss:1.7396\n",
      "epoch [194/500], (rmse) loss:1.0583, (rmse) val_loss:1.7417\n",
      "epoch [195/500], (rmse) loss:1.0569, (rmse) val_loss:1.7398\n",
      "epoch [196/500], (rmse) loss:1.0545, (rmse) val_loss:1.7386\n",
      "epoch [197/500], (rmse) loss:1.0499, (rmse) val_loss:1.7373\n",
      "epoch [198/500], (rmse) loss:1.0475, (rmse) val_loss:1.7394\n",
      "epoch [199/500], (rmse) loss:1.0438, (rmse) val_loss:1.7388\n",
      "epoch [200/500], (rmse) loss:1.0439, (rmse) val_loss:1.7372\n",
      "epoch [201/500], (rmse) loss:1.0394, (rmse) val_loss:1.7400\n",
      "epoch [202/500], (rmse) loss:1.0374, (rmse) val_loss:1.7351\n",
      "epoch [203/500], (rmse) loss:1.0330, (rmse) val_loss:1.7375\n",
      "epoch [204/500], (rmse) loss:1.0314, (rmse) val_loss:1.7358\n",
      "epoch [205/500], (rmse) loss:1.0303, (rmse) val_loss:1.7388\n",
      "epoch [206/500], (rmse) loss:1.0295, (rmse) val_loss:1.7376\n",
      "epoch [207/500], (rmse) loss:1.0250, (rmse) val_loss:1.7379\n",
      "epoch [208/500], (rmse) loss:1.0237, (rmse) val_loss:1.7372\n",
      "epoch [209/500], (rmse) loss:1.0212, (rmse) val_loss:1.7376\n",
      "epoch [210/500], (rmse) loss:1.0190, (rmse) val_loss:1.7383\n",
      "epoch [211/500], (rmse) loss:1.0158, (rmse) val_loss:1.7358\n",
      "epoch [212/500], (rmse) loss:1.0131, (rmse) val_loss:1.7364\n",
      "epoch [213/500], (rmse) loss:1.0098, (rmse) val_loss:1.7357\n",
      "epoch [214/500], (rmse) loss:1.0077, (rmse) val_loss:1.7387\n",
      "epoch [215/500], (rmse) loss:1.0045, (rmse) val_loss:1.7387\n",
      "epoch [216/500], (rmse) loss:1.0026, (rmse) val_loss:1.7379\n",
      "epoch [217/500], (rmse) loss:0.9995, (rmse) val_loss:1.7387\n",
      "epoch [218/500], (rmse) loss:0.9985, (rmse) val_loss:1.7373\n",
      "epoch [219/500], (rmse) loss:0.9963, (rmse) val_loss:1.7388\n",
      "epoch [220/500], (rmse) loss:0.9949, (rmse) val_loss:1.7405\n",
      "epoch [221/500], (rmse) loss:0.9947, (rmse) val_loss:1.7396\n",
      "epoch [222/500], (rmse) loss:0.9960, (rmse) val_loss:1.7409\n",
      "epoch [223/500], (rmse) loss:0.9953, (rmse) val_loss:1.7407\n",
      "epoch [224/500], (rmse) loss:0.9910, (rmse) val_loss:1.7424\n",
      "epoch [225/500], (rmse) loss:0.9897, (rmse) val_loss:1.7432\n",
      "epoch [226/500], (rmse) loss:0.9860, (rmse) val_loss:1.7433\n",
      "epoch [227/500], (rmse) loss:0.9827, (rmse) val_loss:1.7429\n",
      "epoch [228/500], (rmse) loss:0.9825, (rmse) val_loss:1.7445\n",
      "epoch [229/500], (rmse) loss:0.9819, (rmse) val_loss:1.7436\n",
      "epoch [230/500], (rmse) loss:0.9775, (rmse) val_loss:1.7454\n",
      "epoch [231/500], (rmse) loss:0.9801, (rmse) val_loss:1.7419\n",
      "epoch [232/500], (rmse) loss:0.9774, (rmse) val_loss:1.7428\n",
      "epoch [233/500], (rmse) loss:0.9761, (rmse) val_loss:1.7459\n",
      "epoch [234/500], (rmse) loss:0.9756, (rmse) val_loss:1.7417\n",
      "epoch [235/500], (rmse) loss:0.9719, (rmse) val_loss:1.7432\n",
      "epoch [236/500], (rmse) loss:0.9672, (rmse) val_loss:1.7454\n",
      "epoch [237/500], (rmse) loss:0.9659, (rmse) val_loss:1.7418\n",
      "epoch [238/500], (rmse) loss:0.9651, (rmse) val_loss:1.7483\n",
      "epoch [239/500], (rmse) loss:0.9636, (rmse) val_loss:1.7404\n",
      "epoch [240/500], (rmse) loss:0.9655, (rmse) val_loss:1.7481\n",
      "epoch [241/500], (rmse) loss:0.9655, (rmse) val_loss:1.7436\n",
      "epoch [242/500], (rmse) loss:0.9652, (rmse) val_loss:1.7454\n",
      "epoch [243/500], (rmse) loss:0.9598, (rmse) val_loss:1.7436\n",
      "epoch [244/500], (rmse) loss:0.9566, (rmse) val_loss:1.7499\n",
      "epoch [245/500], (rmse) loss:0.9544, (rmse) val_loss:1.7438\n",
      "epoch [246/500], (rmse) loss:0.9524, (rmse) val_loss:1.7470\n",
      "epoch [247/500], (rmse) loss:0.9506, (rmse) val_loss:1.7462\n",
      "epoch [248/500], (rmse) loss:0.9507, (rmse) val_loss:1.7451\n",
      "epoch [249/500], (rmse) loss:0.9465, (rmse) val_loss:1.7469\n",
      "epoch [250/500], (rmse) loss:0.9454, (rmse) val_loss:1.7443\n",
      "epoch [251/500], (rmse) loss:0.9444, (rmse) val_loss:1.7452\n",
      "epoch [252/500], (rmse) loss:0.9431, (rmse) val_loss:1.7469\n",
      "epoch [253/500], (rmse) loss:0.9417, (rmse) val_loss:1.7467\n",
      "epoch [254/500], (rmse) loss:0.9387, (rmse) val_loss:1.7473\n",
      "epoch [255/500], (rmse) loss:0.9357, (rmse) val_loss:1.7464\n",
      "epoch [256/500], (rmse) loss:0.9367, (rmse) val_loss:1.7481\n",
      "epoch [257/500], (rmse) loss:0.9346, (rmse) val_loss:1.7457\n",
      "epoch [258/500], (rmse) loss:0.9333, (rmse) val_loss:1.7494\n",
      "epoch [259/500], (rmse) loss:0.9337, (rmse) val_loss:1.7511\n",
      "epoch [260/500], (rmse) loss:0.9315, (rmse) val_loss:1.7491\n",
      "epoch [261/500], (rmse) loss:0.9306, (rmse) val_loss:1.7526\n",
      "epoch [262/500], (rmse) loss:0.9278, (rmse) val_loss:1.7503\n",
      "epoch [263/500], (rmse) loss:0.9262, (rmse) val_loss:1.7479\n",
      "epoch [264/500], (rmse) loss:0.9248, (rmse) val_loss:1.7535\n",
      "epoch [265/500], (rmse) loss:0.9232, (rmse) val_loss:1.7480\n",
      "epoch [266/500], (rmse) loss:0.9192, (rmse) val_loss:1.7532\n",
      "epoch [267/500], (rmse) loss:0.9169, (rmse) val_loss:1.7486\n",
      "epoch [268/500], (rmse) loss:0.9143, (rmse) val_loss:1.7520\n",
      "epoch [269/500], (rmse) loss:0.9143, (rmse) val_loss:1.7504\n",
      "epoch [270/500], (rmse) loss:0.9141, (rmse) val_loss:1.7514\n",
      "epoch [271/500], (rmse) loss:0.9118, (rmse) val_loss:1.7512\n",
      "epoch [272/500], (rmse) loss:0.9126, (rmse) val_loss:1.7515\n",
      "epoch [273/500], (rmse) loss:0.9118, (rmse) val_loss:1.7528\n",
      "epoch [274/500], (rmse) loss:0.9075, (rmse) val_loss:1.7504\n",
      "epoch [275/500], (rmse) loss:0.9044, (rmse) val_loss:1.7504\n",
      "epoch [276/500], (rmse) loss:0.9029, (rmse) val_loss:1.7528\n",
      "epoch [277/500], (rmse) loss:0.9028, (rmse) val_loss:1.7518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [278/500], (rmse) loss:0.9027, (rmse) val_loss:1.7546\n",
      "epoch [279/500], (rmse) loss:0.9037, (rmse) val_loss:1.7558\n",
      "epoch [280/500], (rmse) loss:0.9054, (rmse) val_loss:1.7541\n",
      "epoch [281/500], (rmse) loss:0.9039, (rmse) val_loss:1.7608\n",
      "epoch [282/500], (rmse) loss:0.9034, (rmse) val_loss:1.7554\n",
      "epoch [283/500], (rmse) loss:0.8998, (rmse) val_loss:1.7552\n",
      "epoch [284/500], (rmse) loss:0.8972, (rmse) val_loss:1.7576\n",
      "epoch [285/500], (rmse) loss:0.8992, (rmse) val_loss:1.7554\n",
      "epoch [286/500], (rmse) loss:0.8999, (rmse) val_loss:1.7553\n",
      "epoch [287/500], (rmse) loss:0.8934, (rmse) val_loss:1.7578\n",
      "epoch [288/500], (rmse) loss:0.8911, (rmse) val_loss:1.7547\n",
      "epoch [289/500], (rmse) loss:0.8907, (rmse) val_loss:1.7565\n",
      "epoch [290/500], (rmse) loss:0.8877, (rmse) val_loss:1.7600\n",
      "epoch [291/500], (rmse) loss:0.8870, (rmse) val_loss:1.7559\n",
      "epoch [292/500], (rmse) loss:0.8867, (rmse) val_loss:1.7617\n",
      "epoch [293/500], (rmse) loss:0.8859, (rmse) val_loss:1.7576\n",
      "epoch [294/500], (rmse) loss:0.8852, (rmse) val_loss:1.7601\n",
      "epoch [295/500], (rmse) loss:0.8850, (rmse) val_loss:1.7607\n",
      "epoch [296/500], (rmse) loss:0.8801, (rmse) val_loss:1.7601\n",
      "epoch [297/500], (rmse) loss:0.8807, (rmse) val_loss:1.7638\n",
      "epoch [298/500], (rmse) loss:0.8820, (rmse) val_loss:1.7609\n",
      "epoch [299/500], (rmse) loss:0.8783, (rmse) val_loss:1.7618\n",
      "epoch [300/500], (rmse) loss:0.8806, (rmse) val_loss:1.7633\n",
      "epoch [301/500], (rmse) loss:0.8781, (rmse) val_loss:1.7615\n",
      "epoch [302/500], (rmse) loss:0.8744, (rmse) val_loss:1.7608\n",
      "epoch [303/500], (rmse) loss:0.8734, (rmse) val_loss:1.7628\n",
      "epoch [304/500], (rmse) loss:0.8726, (rmse) val_loss:1.7629\n",
      "epoch [305/500], (rmse) loss:0.8740, (rmse) val_loss:1.7611\n",
      "epoch [306/500], (rmse) loss:0.8694, (rmse) val_loss:1.7627\n",
      "epoch [307/500], (rmse) loss:0.8709, (rmse) val_loss:1.7630\n",
      "epoch [308/500], (rmse) loss:0.8716, (rmse) val_loss:1.7675\n",
      "epoch [309/500], (rmse) loss:0.8718, (rmse) val_loss:1.7636\n",
      "epoch [310/500], (rmse) loss:0.8718, (rmse) val_loss:1.7694\n",
      "epoch [311/500], (rmse) loss:0.8686, (rmse) val_loss:1.7631\n",
      "epoch [312/500], (rmse) loss:0.8692, (rmse) val_loss:1.7677\n",
      "epoch [313/500], (rmse) loss:0.8694, (rmse) val_loss:1.7665\n",
      "epoch [314/500], (rmse) loss:0.8649, (rmse) val_loss:1.7660\n",
      "epoch [315/500], (rmse) loss:0.8641, (rmse) val_loss:1.7700\n",
      "epoch [316/500], (rmse) loss:0.8639, (rmse) val_loss:1.7681\n",
      "epoch [317/500], (rmse) loss:0.8606, (rmse) val_loss:1.7679\n",
      "epoch [318/500], (rmse) loss:0.8577, (rmse) val_loss:1.7716\n",
      "epoch [319/500], (rmse) loss:0.8572, (rmse) val_loss:1.7677\n",
      "epoch [320/500], (rmse) loss:0.8553, (rmse) val_loss:1.7688\n",
      "epoch [321/500], (rmse) loss:0.8545, (rmse) val_loss:1.7711\n",
      "epoch [322/500], (rmse) loss:0.8561, (rmse) val_loss:1.7712\n",
      "epoch [323/500], (rmse) loss:0.8573, (rmse) val_loss:1.7715\n",
      "epoch [324/500], (rmse) loss:0.8561, (rmse) val_loss:1.7718\n",
      "epoch [325/500], (rmse) loss:0.8508, (rmse) val_loss:1.7723\n",
      "epoch [326/500], (rmse) loss:0.8488, (rmse) val_loss:1.7701\n",
      "epoch [327/500], (rmse) loss:0.8485, (rmse) val_loss:1.7706\n",
      "epoch [328/500], (rmse) loss:0.8471, (rmse) val_loss:1.7709\n",
      "epoch [329/500], (rmse) loss:0.8481, (rmse) val_loss:1.7700\n",
      "epoch [330/500], (rmse) loss:0.8456, (rmse) val_loss:1.7736\n",
      "epoch [331/500], (rmse) loss:0.8452, (rmse) val_loss:1.7732\n",
      "epoch [332/500], (rmse) loss:0.8434, (rmse) val_loss:1.7726\n",
      "epoch [333/500], (rmse) loss:0.8423, (rmse) val_loss:1.7749\n",
      "epoch [334/500], (rmse) loss:0.8423, (rmse) val_loss:1.7764\n",
      "epoch [335/500], (rmse) loss:0.8452, (rmse) val_loss:1.7752\n",
      "epoch [336/500], (rmse) loss:0.8434, (rmse) val_loss:1.7772\n",
      "epoch [337/500], (rmse) loss:0.8424, (rmse) val_loss:1.7766\n",
      "epoch [338/500], (rmse) loss:0.8447, (rmse) val_loss:1.7761\n",
      "epoch [339/500], (rmse) loss:0.8379, (rmse) val_loss:1.7753\n",
      "epoch [340/500], (rmse) loss:0.8371, (rmse) val_loss:1.7765\n",
      "epoch [341/500], (rmse) loss:0.8366, (rmse) val_loss:1.7757\n",
      "epoch [342/500], (rmse) loss:0.8359, (rmse) val_loss:1.7765\n",
      "epoch [343/500], (rmse) loss:0.8366, (rmse) val_loss:1.7771\n",
      "epoch [344/500], (rmse) loss:0.8331, (rmse) val_loss:1.7781\n",
      "epoch [345/500], (rmse) loss:0.8335, (rmse) val_loss:1.7771\n",
      "epoch [346/500], (rmse) loss:0.8324, (rmse) val_loss:1.7759\n",
      "epoch [347/500], (rmse) loss:0.8310, (rmse) val_loss:1.7791\n",
      "epoch [348/500], (rmse) loss:0.8286, (rmse) val_loss:1.7770\n",
      "epoch [349/500], (rmse) loss:0.8284, (rmse) val_loss:1.7797\n",
      "epoch [350/500], (rmse) loss:0.8295, (rmse) val_loss:1.7811\n",
      "epoch [351/500], (rmse) loss:0.8284, (rmse) val_loss:1.7755\n",
      "epoch [352/500], (rmse) loss:0.8298, (rmse) val_loss:1.7798\n",
      "epoch [353/500], (rmse) loss:0.8251, (rmse) val_loss:1.7791\n",
      "epoch [354/500], (rmse) loss:0.8251, (rmse) val_loss:1.7833\n",
      "epoch [355/500], (rmse) loss:0.8260, (rmse) val_loss:1.7809\n",
      "epoch [356/500], (rmse) loss:0.8245, (rmse) val_loss:1.7836\n",
      "epoch [357/500], (rmse) loss:0.8248, (rmse) val_loss:1.7809\n",
      "epoch [358/500], (rmse) loss:0.8225, (rmse) val_loss:1.7841\n",
      "epoch [359/500], (rmse) loss:0.8228, (rmse) val_loss:1.7797\n",
      "epoch [360/500], (rmse) loss:0.8182, (rmse) val_loss:1.7808\n",
      "epoch [361/500], (rmse) loss:0.8162, (rmse) val_loss:1.7805\n",
      "epoch [362/500], (rmse) loss:0.8178, (rmse) val_loss:1.7844\n",
      "epoch [363/500], (rmse) loss:0.8201, (rmse) val_loss:1.7835\n",
      "epoch [364/500], (rmse) loss:0.8223, (rmse) val_loss:1.7828\n",
      "epoch [365/500], (rmse) loss:0.8185, (rmse) val_loss:1.7851\n",
      "epoch [366/500], (rmse) loss:0.8190, (rmse) val_loss:1.7831\n",
      "epoch [367/500], (rmse) loss:0.8169, (rmse) val_loss:1.7881\n",
      "epoch [368/500], (rmse) loss:0.8162, (rmse) val_loss:1.7834\n",
      "epoch [369/500], (rmse) loss:0.8101, (rmse) val_loss:1.7853\n",
      "epoch [370/500], (rmse) loss:0.8093, (rmse) val_loss:1.7859\n",
      "epoch [371/500], (rmse) loss:0.8106, (rmse) val_loss:1.7856\n",
      "epoch [372/500], (rmse) loss:0.8087, (rmse) val_loss:1.7881\n",
      "epoch [373/500], (rmse) loss:0.8112, (rmse) val_loss:1.7853\n",
      "epoch [374/500], (rmse) loss:0.8112, (rmse) val_loss:1.7898\n",
      "epoch [375/500], (rmse) loss:0.8102, (rmse) val_loss:1.7850\n",
      "epoch [376/500], (rmse) loss:0.8089, (rmse) val_loss:1.7873\n",
      "epoch [377/500], (rmse) loss:0.8060, (rmse) val_loss:1.7863\n",
      "epoch [378/500], (rmse) loss:0.8049, (rmse) val_loss:1.7866\n",
      "epoch [379/500], (rmse) loss:0.8021, (rmse) val_loss:1.7905\n",
      "epoch [380/500], (rmse) loss:0.8056, (rmse) val_loss:1.7873\n",
      "epoch [381/500], (rmse) loss:0.8060, (rmse) val_loss:1.7874\n",
      "epoch [382/500], (rmse) loss:0.8057, (rmse) val_loss:1.7872\n",
      "epoch [383/500], (rmse) loss:0.8017, (rmse) val_loss:1.7888\n",
      "epoch [384/500], (rmse) loss:0.7994, (rmse) val_loss:1.7897\n",
      "epoch [385/500], (rmse) loss:0.8009, (rmse) val_loss:1.7878\n",
      "epoch [386/500], (rmse) loss:0.8003, (rmse) val_loss:1.7907\n",
      "epoch [387/500], (rmse) loss:0.7987, (rmse) val_loss:1.7886\n",
      "epoch [388/500], (rmse) loss:0.7960, (rmse) val_loss:1.7894\n",
      "epoch [389/500], (rmse) loss:0.7946, (rmse) val_loss:1.7872\n",
      "epoch [390/500], (rmse) loss:0.7960, (rmse) val_loss:1.7896\n",
      "epoch [391/500], (rmse) loss:0.7953, (rmse) val_loss:1.7904\n",
      "epoch [392/500], (rmse) loss:0.7976, (rmse) val_loss:1.7913\n",
      "epoch [393/500], (rmse) loss:0.7957, (rmse) val_loss:1.7902\n",
      "epoch [394/500], (rmse) loss:0.7940, (rmse) val_loss:1.7927\n",
      "epoch [395/500], (rmse) loss:0.7936, (rmse) val_loss:1.7919\n",
      "epoch [396/500], (rmse) loss:0.7907, (rmse) val_loss:1.7926\n",
      "epoch [397/500], (rmse) loss:0.7918, (rmse) val_loss:1.7916\n",
      "epoch [398/500], (rmse) loss:0.7888, (rmse) val_loss:1.7920\n",
      "epoch [399/500], (rmse) loss:0.7906, (rmse) val_loss:1.7944\n",
      "epoch [400/500], (rmse) loss:0.7924, (rmse) val_loss:1.7958\n",
      "epoch [401/500], (rmse) loss:0.7902, (rmse) val_loss:1.7926\n",
      "epoch [402/500], (rmse) loss:0.7904, (rmse) val_loss:1.7938\n",
      "epoch [403/500], (rmse) loss:0.7917, (rmse) val_loss:1.7946\n",
      "epoch [404/500], (rmse) loss:0.7938, (rmse) val_loss:1.7913\n",
      "epoch [405/500], (rmse) loss:0.7912, (rmse) val_loss:1.7944\n",
      "epoch [406/500], (rmse) loss:0.7896, (rmse) val_loss:1.7918\n",
      "epoch [407/500], (rmse) loss:0.7901, (rmse) val_loss:1.7972\n",
      "epoch [408/500], (rmse) loss:0.7960, (rmse) val_loss:1.7941\n",
      "epoch [409/500], (rmse) loss:0.7915, (rmse) val_loss:1.7954\n",
      "epoch [410/500], (rmse) loss:0.7940, (rmse) val_loss:1.7965\n",
      "epoch [411/500], (rmse) loss:0.7882, (rmse) val_loss:1.7950\n",
      "epoch [412/500], (rmse) loss:0.7895, (rmse) val_loss:1.7971\n",
      "epoch [413/500], (rmse) loss:0.7869, (rmse) val_loss:1.7990\n",
      "epoch [414/500], (rmse) loss:0.7862, (rmse) val_loss:1.7984\n",
      "epoch [415/500], (rmse) loss:0.7867, (rmse) val_loss:1.7982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [416/500], (rmse) loss:0.7853, (rmse) val_loss:1.7972\n",
      "epoch [417/500], (rmse) loss:0.7837, (rmse) val_loss:1.7986\n",
      "epoch [418/500], (rmse) loss:0.7880, (rmse) val_loss:1.7964\n",
      "epoch [419/500], (rmse) loss:0.7885, (rmse) val_loss:1.8015\n",
      "epoch [420/500], (rmse) loss:0.7854, (rmse) val_loss:1.7992\n",
      "epoch [421/500], (rmse) loss:0.7860, (rmse) val_loss:1.7990\n",
      "epoch [422/500], (rmse) loss:0.7837, (rmse) val_loss:1.8000\n",
      "epoch [423/500], (rmse) loss:0.7840, (rmse) val_loss:1.7979\n",
      "epoch [424/500], (rmse) loss:0.7770, (rmse) val_loss:1.7982\n",
      "epoch [425/500], (rmse) loss:0.7778, (rmse) val_loss:1.8029\n",
      "epoch [426/500], (rmse) loss:0.7805, (rmse) val_loss:1.8002\n",
      "epoch [427/500], (rmse) loss:0.7806, (rmse) val_loss:1.7972\n",
      "epoch [428/500], (rmse) loss:0.7763, (rmse) val_loss:1.8004\n",
      "epoch [429/500], (rmse) loss:0.7788, (rmse) val_loss:1.8008\n",
      "epoch [430/500], (rmse) loss:0.7773, (rmse) val_loss:1.7991\n",
      "epoch [431/500], (rmse) loss:0.7742, (rmse) val_loss:1.8015\n",
      "epoch [432/500], (rmse) loss:0.7732, (rmse) val_loss:1.7994\n",
      "epoch [433/500], (rmse) loss:0.7741, (rmse) val_loss:1.8019\n",
      "epoch [434/500], (rmse) loss:0.7717, (rmse) val_loss:1.7997\n",
      "epoch [435/500], (rmse) loss:0.7688, (rmse) val_loss:1.8004\n",
      "epoch [436/500], (rmse) loss:0.7695, (rmse) val_loss:1.8023\n",
      "epoch [437/500], (rmse) loss:0.7674, (rmse) val_loss:1.8018\n",
      "epoch [438/500], (rmse) loss:0.7660, (rmse) val_loss:1.8028\n",
      "epoch [439/500], (rmse) loss:0.7651, (rmse) val_loss:1.8016\n",
      "epoch [440/500], (rmse) loss:0.7638, (rmse) val_loss:1.7998\n",
      "epoch [441/500], (rmse) loss:0.7653, (rmse) val_loss:1.8008\n",
      "epoch [442/500], (rmse) loss:0.7664, (rmse) val_loss:1.8045\n",
      "epoch [443/500], (rmse) loss:0.7692, (rmse) val_loss:1.8018\n",
      "epoch [444/500], (rmse) loss:0.7660, (rmse) val_loss:1.8062\n",
      "epoch [445/500], (rmse) loss:0.7694, (rmse) val_loss:1.8028\n",
      "epoch [446/500], (rmse) loss:0.7670, (rmse) val_loss:1.8040\n",
      "epoch [447/500], (rmse) loss:0.7668, (rmse) val_loss:1.8039\n",
      "epoch [448/500], (rmse) loss:0.7712, (rmse) val_loss:1.8031\n",
      "epoch [449/500], (rmse) loss:0.7703, (rmse) val_loss:1.8046\n",
      "epoch [450/500], (rmse) loss:0.7719, (rmse) val_loss:1.8058\n",
      "epoch [451/500], (rmse) loss:0.7671, (rmse) val_loss:1.8048\n",
      "epoch [452/500], (rmse) loss:0.7638, (rmse) val_loss:1.8038\n",
      "epoch [453/500], (rmse) loss:0.7658, (rmse) val_loss:1.8045\n",
      "epoch [454/500], (rmse) loss:0.7606, (rmse) val_loss:1.8080\n",
      "epoch [455/500], (rmse) loss:0.7631, (rmse) val_loss:1.8048\n",
      "epoch [456/500], (rmse) loss:0.7617, (rmse) val_loss:1.8069\n",
      "epoch [457/500], (rmse) loss:0.7607, (rmse) val_loss:1.8052\n",
      "epoch [458/500], (rmse) loss:0.7607, (rmse) val_loss:1.8044\n",
      "epoch [459/500], (rmse) loss:0.7624, (rmse) val_loss:1.8053\n",
      "epoch [460/500], (rmse) loss:0.7623, (rmse) val_loss:1.8072\n",
      "epoch [461/500], (rmse) loss:0.7612, (rmse) val_loss:1.8067\n",
      "epoch [462/500], (rmse) loss:0.7628, (rmse) val_loss:1.8080\n",
      "epoch [463/500], (rmse) loss:0.7628, (rmse) val_loss:1.8047\n",
      "epoch [464/500], (rmse) loss:0.7637, (rmse) val_loss:1.8084\n",
      "epoch [465/500], (rmse) loss:0.7637, (rmse) val_loss:1.8102\n",
      "epoch [466/500], (rmse) loss:0.7634, (rmse) val_loss:1.8075\n",
      "epoch [467/500], (rmse) loss:0.7622, (rmse) val_loss:1.8064\n",
      "epoch [468/500], (rmse) loss:0.7596, (rmse) val_loss:1.8069\n",
      "epoch [469/500], (rmse) loss:0.7587, (rmse) val_loss:1.8081\n",
      "epoch [470/500], (rmse) loss:0.7570, (rmse) val_loss:1.8062\n",
      "epoch [471/500], (rmse) loss:0.7561, (rmse) val_loss:1.8095\n",
      "epoch [472/500], (rmse) loss:0.7563, (rmse) val_loss:1.8070\n",
      "epoch [473/500], (rmse) loss:0.7532, (rmse) val_loss:1.8048\n",
      "epoch [474/500], (rmse) loss:0.7573, (rmse) val_loss:1.8098\n",
      "epoch [475/500], (rmse) loss:0.7571, (rmse) val_loss:1.8064\n",
      "epoch [476/500], (rmse) loss:0.7561, (rmse) val_loss:1.8086\n",
      "epoch [477/500], (rmse) loss:0.7573, (rmse) val_loss:1.8073\n",
      "epoch [478/500], (rmse) loss:0.7562, (rmse) val_loss:1.8115\n",
      "epoch [479/500], (rmse) loss:0.7568, (rmse) val_loss:1.8062\n",
      "epoch [480/500], (rmse) loss:0.7543, (rmse) val_loss:1.8070\n",
      "epoch [481/500], (rmse) loss:0.7535, (rmse) val_loss:1.8093\n",
      "epoch [482/500], (rmse) loss:0.7523, (rmse) val_loss:1.8080\n",
      "epoch [483/500], (rmse) loss:0.7525, (rmse) val_loss:1.8093\n",
      "epoch [484/500], (rmse) loss:0.7556, (rmse) val_loss:1.8119\n",
      "epoch [485/500], (rmse) loss:0.7557, (rmse) val_loss:1.8113\n",
      "epoch [486/500], (rmse) loss:0.7545, (rmse) val_loss:1.8111\n",
      "epoch [487/500], (rmse) loss:0.7517, (rmse) val_loss:1.8103\n",
      "epoch [488/500], (rmse) loss:0.7460, (rmse) val_loss:1.8109\n",
      "epoch [489/500], (rmse) loss:0.7493, (rmse) val_loss:1.8098\n",
      "epoch [490/500], (rmse) loss:0.7492, (rmse) val_loss:1.8161\n",
      "epoch [491/500], (rmse) loss:0.7488, (rmse) val_loss:1.8087\n",
      "epoch [492/500], (rmse) loss:0.7459, (rmse) val_loss:1.8112\n",
      "epoch [493/500], (rmse) loss:0.7447, (rmse) val_loss:1.8109\n",
      "epoch [494/500], (rmse) loss:0.7437, (rmse) val_loss:1.8102\n",
      "epoch [495/500], (rmse) loss:0.7402, (rmse) val_loss:1.8108\n",
      "epoch [496/500], (rmse) loss:0.7411, (rmse) val_loss:1.8111\n",
      "epoch [497/500], (rmse) loss:0.7417, (rmse) val_loss:1.8111\n",
      "epoch [498/500], (rmse) loss:0.7425, (rmse) val_loss:1.8119\n",
      "epoch [499/500], (rmse) loss:0.7435, (rmse) val_loss:1.8131\n",
      "epoch [500/500], (rmse) loss:0.7430, (rmse) val_loss:1.8137\n"
     ]
    }
   ],
   "source": [
    "## Model Running (Training)\n",
    "model = run_epoch(model, X_train, X_eval, num_epochs= 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- We can see that while the neural network is clearly training due to the validation mse decreasing after each epoch, it is not decreasing as quickly as normal matrix factorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Movielens 0.1M\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Randomly Generated Data가 아닌 실제 데이터를 활용함\n",
    "- Recommendation계의 MNIST 데이터로 Movielnes Data Set을 활용\n",
    "\n",
    "(참고 : https://grouplens.org/datasets/movielens/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T08:48:48.264209Z",
     "start_time": "2018-02-08T08:48:48.259968Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/Juyeong/Documents/gluon_study/Recommendation'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T08:48:48.367834Z",
     "start_time": "2018-02-08T08:48:48.265893Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100004"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now extract the data since we know we have it at this point\n",
    "with zipfile.ZipFile(\"ml-latest-small.zip\", \"r\") as f:\n",
    "    f.extractall(\"./\")\n",
    "\n",
    "# Now load it up using a pandas dataframe\n",
    "data = pandas.read_csv('./ml-latest-small/ratings.csv', sep=',', usecols=(0, 1, 2))\n",
    "data.head()\n",
    "# ratings from 0.5 to 5. by 0.5 inc\n",
    "len(data)\n",
    "\n",
    "#Since only ~20M of these are present, ~99.5% of the matrix is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T08:48:48.378961Z",
     "start_time": "2018-02-08T08:48:48.370084Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min value of rating : 0.5\n",
      "Max value of rating : 5.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Min value of rating : {}\".format(data.rating.min(axis=0)))\n",
    "print(\"Max value of rating : {}\".format(data.rating.max(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T08:48:48.774840Z",
     "start_time": "2018-02-08T08:48:48.381214Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgEAAAFsCAYAAABVZGp4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAZpwvH94hjVpAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118020160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(data['rating'])\n",
    "plt.xlabel(\"Rating\", fontsize=14)\n",
    "plt.ylabel(\"Count\", fontsize=14)\n",
    "plt.title(\"Distribution of Ratings in MovieLens 20M\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T08:48:48.790781Z",
     "start_time": "2018-02-08T08:48:48.776515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user id min/max:  1 671\n",
      "# unique users:  671\n",
      "\n",
      "movie id min/max:  1 163949\n",
      "# unique movies:  9066\n"
     ]
    }
   ],
   "source": [
    "print(\"user id min/max: \", data['userId'].min(), data['userId'].max())\n",
    "print(\"# unique users: \", numpy.unique(data['userId']).shape[0])\n",
    "print(\"\")\n",
    "print(\"movie id min/max: \", data['movieId'].min(), data['movieId'].max())\n",
    "print(\"# unique movies: \", numpy.unique(data['movieId']).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T08:48:48.797247Z",
     "start_time": "2018-02-08T08:48:48.792506Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_users, n_movies = data['userId'].max(), data['movieId'].max()\n",
    "batch_size = 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T08:48:48.815334Z",
     "start_time": "2018-02-08T08:48:48.798638Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 90000\n",
    "\n",
    "data = data.sample(frac=1).reset_index(drop=True) # Shuffle the data in place row-wise\n",
    "\n",
    "# Use the first 19M samples to train the model\n",
    "train_users = data['userId'].values[:n] - 1 # Offset by 1 so that the IDs start at 0\n",
    "train_movies = data['movieId'].values[:n] - 1 # Offset by 1 so that the IDs start at 0\n",
    "train_ratings = data['rating'].values[:n]\n",
    "\n",
    "# Use the remaining ~1M samples for validation of the model\n",
    "valid_users = data['userId'].values[n:] - 1 # Offset by 1 so that the IDs start at 0\n",
    "valid_movies = data['movieId'].values[n:] - 1 # Offset by 1 so that the IDs start at 0\n",
    "valid_ratings = data['rating'].values[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T08:48:48.822869Z",
     "start_time": "2018-02-08T08:48:48.816856Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = gluon.data.DataLoader(gluon.data.ArrayDataset(train_users.astype('float32'), train_movies.astype('float32'), train_ratings.astype('float32')),\n",
    "                                      batch_size=batch_size, shuffle=True)\n",
    "\n",
    "X_eval = gluon.data.DataLoader(gluon.data.ArrayDataset(valid_users.astype('float32'), valid_movies.astype('float32'), valid_ratings.astype('float32')),\n",
    "                                      batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T08:48:48.834442Z",
     "start_time": "2018-02-08T08:48:48.824899Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Vanilla_MF_ml(gluon.HybridBlock):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Vanilla_MF_ml, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            # layers created in name_scope will inherit name space\n",
    "            # from parent layer.\n",
    "            self.user = nn.Embedding(input_dim=n_users, output_dim=25)\n",
    "            self.movie = nn.Embedding(input_dim=n_movies, output_dim=25)\n",
    "            \n",
    "    def hybrid_forward(self, F, user_i, movie_i):\n",
    "        # Here `F` can be either mx.nd or mx.sym, x is the input data,\n",
    "        user_i = self.user(user_i)\n",
    "        movie_i = self.movie(movie_i)\n",
    "        z = F.sum(F.dot(user_i, movie_i, transpose_b=True), axis = 1)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T08:48:48.843745Z",
     "start_time": "2018-02-08T08:48:48.836266Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model initialization\n",
    "model = Vanilla_MF_ml()\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx=ctx)\n",
    "\n",
    "\n",
    "learning_rate = 1e-4\n",
    "criterion = gluon.loss.L2Loss()\n",
    "optimizer = gluon.Trainer(model.collect_params(), 'adam',\n",
    "                          {'learning_rate': learning_rate,\n",
    "                           'wd': 1e-5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T09:00:43.929113Z",
     "start_time": "2018-02-08T08:48:48.845707Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/20], (rmse) loss:2.5596, (rmse) val_loss:0.8348\n",
      "epoch [2/20], (rmse) loss:2.4980, (rmse) val_loss:0.8262\n",
      "epoch [3/20], (rmse) loss:2.4638, (rmse) val_loss:0.8166\n",
      "epoch [4/20], (rmse) loss:2.4215, (rmse) val_loss:0.8068\n",
      "epoch [5/20], (rmse) loss:2.3794, (rmse) val_loss:0.7917\n",
      "epoch [6/20], (rmse) loss:2.3293, (rmse) val_loss:0.7754\n",
      "epoch [7/20], (rmse) loss:2.2767, (rmse) val_loss:0.7610\n",
      "epoch [8/20], (rmse) loss:2.2204, (rmse) val_loss:0.7427\n",
      "epoch [9/20], (rmse) loss:2.1609, (rmse) val_loss:0.7259\n",
      "epoch [10/20], (rmse) loss:2.0992, (rmse) val_loss:0.7075\n",
      "epoch [11/20], (rmse) loss:2.0349, (rmse) val_loss:0.6895\n",
      "epoch [12/20], (rmse) loss:1.9711, (rmse) val_loss:0.6703\n",
      "epoch [13/20], (rmse) loss:1.9043, (rmse) val_loss:0.6527\n",
      "epoch [14/20], (rmse) loss:1.8388, (rmse) val_loss:0.6340\n",
      "epoch [15/20], (rmse) loss:1.7745, (rmse) val_loss:0.6163\n",
      "epoch [16/20], (rmse) loss:1.7100, (rmse) val_loss:0.6001\n",
      "epoch [17/20], (rmse) loss:1.6451, (rmse) val_loss:0.5823\n",
      "epoch [18/20], (rmse) loss:1.5830, (rmse) val_loss:0.5644\n",
      "epoch [19/20], (rmse) loss:1.5223, (rmse) val_loss:0.5517\n",
      "epoch [20/20], (rmse) loss:1.4634, (rmse) val_loss:0.5332\n"
     ]
    }
   ],
   "source": [
    "## Model Running (Training)\n",
    "model = run_epoch(model, X_train, X_eval, num_epochs= 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T09:00:43.950314Z",
     "start_time": "2018-02-08T09:00:43.931660Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Deep_MF_ml(gluon.HybridBlock):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Deep_MF_ml, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            # layers created in name_scope will inherit name space\n",
    "            # from parent layer.\n",
    "            self.user = nn.Embedding(input_dim=n_users, output_dim=25)\n",
    "            self.movie = nn.Embedding(input_dim=n_movies, output_dim=25)\n",
    "            \n",
    "            \n",
    "            self.out = gluon.nn.HybridSequential('output_')\n",
    "            with self.out.name_scope():\n",
    "                self.out.add(nn.Flatten())\n",
    "                self.out.add(nn.Dense(64, activation='relu')) # encoding & latent layer\n",
    "                self.out.add(gluon.nn.BatchNorm(axis=1, center=True, scale=True))\n",
    "                self.out.add(nn.Dense(64, activation='relu')) # encoding & latent layer\n",
    "                self.out.add(gluon.nn.BatchNorm(axis=1, center=True, scale=True))\n",
    "                self.out.add(nn.Dense(1)) # encoding & latent layer\n",
    "            \n",
    "    def hybrid_forward(self, F, user_i, movie_i):\n",
    "        # Here `F` can be either mx.nd or mx.sym, x is the input data,\n",
    "        user_i = self.user(user_i)\n",
    "        movie_i = self.movie(movie_i)\n",
    "        \n",
    "        z = F.concat(user_i, movie_i)\n",
    "        z = self.out(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T09:00:43.966592Z",
     "start_time": "2018-02-08T09:00:43.951925Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model initialization\n",
    "model = Deep_MF_ml()\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx=ctx)\n",
    "\n",
    "\n",
    "learning_rate = 1e-2\n",
    "criterion = gluon.loss.L2Loss()\n",
    "optimizer = gluon.Trainer(model.collect_params(), 'adam',\n",
    "                          {'learning_rate': learning_rate,\n",
    "                           'wd': 1e-5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T09:04:14.852842Z",
     "start_time": "2018-02-08T09:00:43.968426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/100], (rmse) loss:2.5020, (rmse) val_loss:0.7430\n",
      "epoch [2/100], (rmse) loss:1.9978, (rmse) val_loss:0.5525\n",
      "epoch [3/100], (rmse) loss:1.3143, (rmse) val_loss:0.3040\n",
      "epoch [4/100], (rmse) loss:0.6309, (rmse) val_loss:0.2597\n",
      "epoch [5/100], (rmse) loss:0.7364, (rmse) val_loss:0.3032\n",
      "epoch [6/100], (rmse) loss:0.6480, (rmse) val_loss:0.2341\n",
      "epoch [7/100], (rmse) loss:0.4543, (rmse) val_loss:0.2354\n",
      "epoch [8/100], (rmse) loss:0.4829, (rmse) val_loss:0.2416\n",
      "epoch [9/100], (rmse) loss:0.4428, (rmse) val_loss:0.2334\n",
      "epoch [10/100], (rmse) loss:0.3936, (rmse) val_loss:0.2366\n",
      "epoch [11/100], (rmse) loss:0.3915, (rmse) val_loss:0.2378\n",
      "epoch [12/100], (rmse) loss:0.3707, (rmse) val_loss:0.2370\n",
      "epoch [13/100], (rmse) loss:0.3584, (rmse) val_loss:0.2385\n",
      "epoch [14/100], (rmse) loss:0.3496, (rmse) val_loss:0.2400\n",
      "epoch [15/100], (rmse) loss:0.3411, (rmse) val_loss:0.2418\n",
      "epoch [16/100], (rmse) loss:0.3342, (rmse) val_loss:0.2432\n",
      "epoch [17/100], (rmse) loss:0.3287, (rmse) val_loss:0.2438\n",
      "epoch [18/100], (rmse) loss:0.3240, (rmse) val_loss:0.2448\n",
      "epoch [19/100], (rmse) loss:0.3186, (rmse) val_loss:0.2451\n",
      "epoch [20/100], (rmse) loss:0.3139, (rmse) val_loss:0.2471\n",
      "epoch [21/100], (rmse) loss:0.3102, (rmse) val_loss:0.2483\n",
      "epoch [22/100], (rmse) loss:0.3097, (rmse) val_loss:0.2491\n",
      "epoch [23/100], (rmse) loss:0.3068, (rmse) val_loss:0.2494\n",
      "epoch [24/100], (rmse) loss:0.3012, (rmse) val_loss:0.2502\n",
      "epoch [25/100], (rmse) loss:0.2963, (rmse) val_loss:0.2504\n",
      "epoch [26/100], (rmse) loss:0.2927, (rmse) val_loss:0.2508\n",
      "epoch [27/100], (rmse) loss:0.2895, (rmse) val_loss:0.2522\n",
      "epoch [28/100], (rmse) loss:0.2892, (rmse) val_loss:0.2529\n",
      "epoch [29/100], (rmse) loss:0.2889, (rmse) val_loss:0.2530\n",
      "epoch [30/100], (rmse) loss:0.2858, (rmse) val_loss:0.2548\n",
      "epoch [31/100], (rmse) loss:0.2819, (rmse) val_loss:0.2533\n",
      "epoch [32/100], (rmse) loss:0.2777, (rmse) val_loss:0.2546\n",
      "epoch [33/100], (rmse) loss:0.2761, (rmse) val_loss:0.2570\n",
      "epoch [34/100], (rmse) loss:0.2768, (rmse) val_loss:0.2544\n",
      "epoch [35/100], (rmse) loss:0.2740, (rmse) val_loss:0.2553\n",
      "epoch [36/100], (rmse) loss:0.2699, (rmse) val_loss:0.2550\n",
      "epoch [37/100], (rmse) loss:0.2701, (rmse) val_loss:0.2555\n",
      "epoch [38/100], (rmse) loss:0.2707, (rmse) val_loss:0.2583\n",
      "epoch [39/100], (rmse) loss:0.2750, (rmse) val_loss:0.2553\n",
      "epoch [40/100], (rmse) loss:0.2705, (rmse) val_loss:0.2559\n",
      "epoch [41/100], (rmse) loss:0.2660, (rmse) val_loss:0.2556\n",
      "epoch [42/100], (rmse) loss:0.2661, (rmse) val_loss:0.2571\n",
      "epoch [43/100], (rmse) loss:0.2616, (rmse) val_loss:0.2568\n",
      "epoch [44/100], (rmse) loss:0.2582, (rmse) val_loss:0.2579\n",
      "epoch [45/100], (rmse) loss:0.2552, (rmse) val_loss:0.2576\n",
      "epoch [46/100], (rmse) loss:0.2536, (rmse) val_loss:0.2581\n",
      "epoch [47/100], (rmse) loss:0.2532, (rmse) val_loss:0.2578\n",
      "epoch [48/100], (rmse) loss:0.2500, (rmse) val_loss:0.2597\n",
      "epoch [49/100], (rmse) loss:0.2512, (rmse) val_loss:0.2590\n",
      "epoch [50/100], (rmse) loss:0.2483, (rmse) val_loss:0.2592\n",
      "epoch [51/100], (rmse) loss:0.2448, (rmse) val_loss:0.2597\n",
      "epoch [52/100], (rmse) loss:0.2438, (rmse) val_loss:0.2606\n",
      "epoch [53/100], (rmse) loss:0.2418, (rmse) val_loss:0.2616\n",
      "epoch [54/100], (rmse) loss:0.2453, (rmse) val_loss:0.2610\n",
      "epoch [55/100], (rmse) loss:0.2437, (rmse) val_loss:0.2617\n",
      "epoch [56/100], (rmse) loss:0.2454, (rmse) val_loss:0.2609\n",
      "epoch [57/100], (rmse) loss:0.2470, (rmse) val_loss:0.2612\n",
      "epoch [58/100], (rmse) loss:0.2446, (rmse) val_loss:0.2617\n",
      "epoch [59/100], (rmse) loss:0.2424, (rmse) val_loss:0.2618\n",
      "epoch [60/100], (rmse) loss:0.2420, (rmse) val_loss:0.2620\n",
      "epoch [61/100], (rmse) loss:0.2399, (rmse) val_loss:0.2617\n",
      "epoch [62/100], (rmse) loss:0.2418, (rmse) val_loss:0.2616\n",
      "epoch [63/100], (rmse) loss:0.2368, (rmse) val_loss:0.2617\n",
      "epoch [64/100], (rmse) loss:0.2363, (rmse) val_loss:0.2624\n",
      "epoch [65/100], (rmse) loss:0.2350, (rmse) val_loss:0.2629\n",
      "epoch [66/100], (rmse) loss:0.2308, (rmse) val_loss:0.2619\n",
      "epoch [67/100], (rmse) loss:0.2279, (rmse) val_loss:0.2628\n",
      "epoch [68/100], (rmse) loss:0.2274, (rmse) val_loss:0.2650\n",
      "epoch [69/100], (rmse) loss:0.2299, (rmse) val_loss:0.2639\n",
      "epoch [70/100], (rmse) loss:0.2293, (rmse) val_loss:0.2660\n",
      "epoch [71/100], (rmse) loss:0.2341, (rmse) val_loss:0.2635\n",
      "epoch [72/100], (rmse) loss:0.2324, (rmse) val_loss:0.2646\n",
      "epoch [73/100], (rmse) loss:0.2316, (rmse) val_loss:0.2630\n",
      "epoch [74/100], (rmse) loss:0.2298, (rmse) val_loss:0.2638\n",
      "epoch [75/100], (rmse) loss:0.2287, (rmse) val_loss:0.2640\n",
      "epoch [76/100], (rmse) loss:0.2329, (rmse) val_loss:0.2657\n",
      "epoch [77/100], (rmse) loss:0.2323, (rmse) val_loss:0.2661\n",
      "epoch [78/100], (rmse) loss:0.2361, (rmse) val_loss:0.2654\n",
      "epoch [79/100], (rmse) loss:0.2362, (rmse) val_loss:0.2653\n",
      "epoch [80/100], (rmse) loss:0.2313, (rmse) val_loss:0.2631\n",
      "epoch [81/100], (rmse) loss:0.2260, (rmse) val_loss:0.2631\n",
      "epoch [82/100], (rmse) loss:0.2263, (rmse) val_loss:0.2633\n",
      "epoch [83/100], (rmse) loss:0.2240, (rmse) val_loss:0.2629\n",
      "epoch [84/100], (rmse) loss:0.2311, (rmse) val_loss:0.2636\n",
      "epoch [85/100], (rmse) loss:0.2263, (rmse) val_loss:0.2642\n",
      "epoch [86/100], (rmse) loss:0.2194, (rmse) val_loss:0.2640\n",
      "epoch [87/100], (rmse) loss:0.2160, (rmse) val_loss:0.2633\n",
      "epoch [88/100], (rmse) loss:0.2116, (rmse) val_loss:0.2639\n",
      "epoch [89/100], (rmse) loss:0.2103, (rmse) val_loss:0.2649\n",
      "epoch [90/100], (rmse) loss:0.2107, (rmse) val_loss:0.2648\n",
      "epoch [91/100], (rmse) loss:0.2092, (rmse) val_loss:0.2644\n",
      "epoch [92/100], (rmse) loss:0.2121, (rmse) val_loss:0.2656\n",
      "epoch [93/100], (rmse) loss:0.2121, (rmse) val_loss:0.2675\n",
      "epoch [94/100], (rmse) loss:0.2167, (rmse) val_loss:0.2663\n",
      "epoch [95/100], (rmse) loss:0.2174, (rmse) val_loss:0.2649\n",
      "epoch [96/100], (rmse) loss:0.2139, (rmse) val_loss:0.2654\n",
      "epoch [97/100], (rmse) loss:0.2133, (rmse) val_loss:0.2650\n",
      "epoch [98/100], (rmse) loss:0.2126, (rmse) val_loss:0.2649\n",
      "epoch [99/100], (rmse) loss:0.2200, (rmse) val_loss:0.2659\n",
      "epoch [100/100], (rmse) loss:0.2222, (rmse) val_loss:0.2642\n"
     ]
    }
   ],
   "source": [
    "## Model Running (Training)\n",
    "model = run_epoch(model, X_train, X_eval, num_epochs= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T09:04:14.875342Z",
     "start_time": "2018-02-08T09:04:14.854353Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Regression 형태가 아닌 Classification 문제로 변환\n",
    "# movielens의 경우 0.5 ~ 5까지 0.5구간으로 평점이 존재하며 이에 따라 총 10개의 class인 문제로 표현 가능\n",
    "class Deep_MF_ml_class(gluon.HybridBlock):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Deep_MF_ml_class, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            # layers created in name_scope will inherit name space\n",
    "            # from parent layer.\n",
    "            self.user = nn.Embedding(input_dim=n_users, output_dim=25)\n",
    "            self.movie = nn.Embedding(input_dim=n_movies, output_dim=25)\n",
    "            \n",
    "            \n",
    "            self.out = gluon.nn.HybridSequential('output_')\n",
    "            with self.out.name_scope():\n",
    "                self.out.add(nn.Flatten())\n",
    "                self.out.add(nn.Dense(64, activation='relu')) # encoding & latent layer\n",
    "                self.out.add(gluon.nn.BatchNorm(axis=1, center=True, scale=True))\n",
    "                self.out.add(nn.Dense(64, activation='relu')) # encoding & latent layer\n",
    "                self.out.add(gluon.nn.BatchNorm(axis=1, center=True, scale=True))\n",
    "                self.out.add(nn.Dense(10)) # encoding & latent layer\n",
    "            \n",
    "    def hybrid_forward(self, F, user_i, movie_i):\n",
    "        # Here `F` can be either mx.nd or mx.sym, x is the input data,\n",
    "        user_i = self.user(user_i)\n",
    "        movie_i = self.movie(movie_i)\n",
    "        \n",
    "        z = F.concat(user_i, movie_i)\n",
    "        z = self.out(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T09:04:14.892899Z",
     "start_time": "2018-02-08T09:04:14.876875Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model initialization\n",
    "model = Deep_MF_ml_class()\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx=ctx)\n",
    "\n",
    "\n",
    "learning_rate = 1e-3\n",
    "criterion = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "optimizer = gluon.Trainer(model.collect_params(), 'adam',\n",
    "                          {'learning_rate': learning_rate,\n",
    "                           'wd': 1e-5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T09:04:14.902190Z",
     "start_time": "2018-02-08T09:04:14.894784Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    \n",
    "    for user, movie, rating in data_iterator:\n",
    "        user = user.as_in_context(ctx)\n",
    "        movie = movie.as_in_context(ctx)\n",
    "        rating = rating.as_in_context(ctx)\n",
    "    \n",
    "        output = net(user, movie)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=rating)\n",
    "    return acc.get()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T09:20:41.303153Z",
     "start_time": "2018-02-08T09:04:14.904198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/100], loss:1.6553, val_loss:0.5247, train_acc:0.0042, val_acc:0.0029\n",
      "epoch [2/100], loss:1.5102, val_loss:0.5002, train_acc:0.0103, val_acc:0.0091\n",
      "epoch [3/100], loss:1.4309, val_loss:0.4866, train_acc:0.0988, val_acc:0.0920\n",
      "epoch [4/100], loss:1.3723, val_loss:0.4774, train_acc:0.1688, val_acc:0.1607\n",
      "epoch [5/100], loss:1.3213, val_loss:0.4715, train_acc:0.1743, val_acc:0.1660\n",
      "epoch [6/100], loss:1.2755, val_loss:0.4682, train_acc:0.1771, val_acc:0.1691\n",
      "epoch [7/100], loss:1.2333, val_loss:0.4662, train_acc:0.1803, val_acc:0.1701\n",
      "epoch [8/100], loss:1.1942, val_loss:0.4649, train_acc:0.2024, val_acc:0.1752\n",
      "epoch [9/100], loss:1.1573, val_loss:0.4645, train_acc:0.3224, val_acc:0.2262\n",
      "epoch [10/100], loss:1.1229, val_loss:0.4646, train_acc:0.4837, val_acc:0.3095\n",
      "epoch [11/100], loss:1.0901, val_loss:0.4656, train_acc:0.5933, val_acc:0.3638\n",
      "epoch [12/100], loss:1.0594, val_loss:0.4675, train_acc:0.6474, val_acc:0.3917\n",
      "epoch [13/100], loss:1.0304, val_loss:0.4699, train_acc:0.6799, val_acc:0.4061\n",
      "epoch [14/100], loss:1.0033, val_loss:0.4727, train_acc:0.7002, val_acc:0.4123\n",
      "epoch [15/100], loss:0.9786, val_loss:0.4757, train_acc:0.7149, val_acc:0.4131\n",
      "epoch [16/100], loss:0.9553, val_loss:0.4788, train_acc:0.7263, val_acc:0.4107\n",
      "epoch [17/100], loss:0.9344, val_loss:0.4821, train_acc:0.7367, val_acc:0.4082\n",
      "epoch [18/100], loss:0.9150, val_loss:0.4853, train_acc:0.7450, val_acc:0.4101\n",
      "epoch [19/100], loss:0.8977, val_loss:0.4890, train_acc:0.7519, val_acc:0.4047\n",
      "epoch [20/100], loss:0.8815, val_loss:0.4925, train_acc:0.7585, val_acc:0.4074\n",
      "epoch [21/100], loss:0.8668, val_loss:0.4957, train_acc:0.7643, val_acc:0.4054\n",
      "epoch [22/100], loss:0.8533, val_loss:0.4991, train_acc:0.7701, val_acc:0.4046\n",
      "epoch [23/100], loss:0.8407, val_loss:0.5024, train_acc:0.7768, val_acc:0.4018\n",
      "epoch [24/100], loss:0.8296, val_loss:0.5057, train_acc:0.7808, val_acc:0.4005\n",
      "epoch [25/100], loss:0.8188, val_loss:0.5084, train_acc:0.7866, val_acc:0.4014\n",
      "epoch [26/100], loss:0.8085, val_loss:0.5116, train_acc:0.7916, val_acc:0.4001\n",
      "epoch [27/100], loss:0.7991, val_loss:0.5147, train_acc:0.7955, val_acc:0.3985\n",
      "epoch [28/100], loss:0.7906, val_loss:0.5169, train_acc:0.8002, val_acc:0.3966\n",
      "epoch [29/100], loss:0.7830, val_loss:0.5198, train_acc:0.8026, val_acc:0.3960\n",
      "epoch [30/100], loss:0.7755, val_loss:0.5223, train_acc:0.8082, val_acc:0.3988\n",
      "epoch [31/100], loss:0.7684, val_loss:0.5245, train_acc:0.8096, val_acc:0.3971\n",
      "epoch [32/100], loss:0.7614, val_loss:0.5269, train_acc:0.8131, val_acc:0.3996\n",
      "epoch [33/100], loss:0.7555, val_loss:0.5293, train_acc:0.8164, val_acc:0.3958\n",
      "epoch [34/100], loss:0.7491, val_loss:0.5315, train_acc:0.8190, val_acc:0.3991\n",
      "epoch [35/100], loss:0.7433, val_loss:0.5337, train_acc:0.8223, val_acc:0.3916\n",
      "epoch [36/100], loss:0.7373, val_loss:0.5357, train_acc:0.8250, val_acc:0.3917\n",
      "epoch [37/100], loss:0.7322, val_loss:0.5382, train_acc:0.8271, val_acc:0.3949\n",
      "epoch [38/100], loss:0.7270, val_loss:0.5397, train_acc:0.8297, val_acc:0.3892\n",
      "epoch [39/100], loss:0.7219, val_loss:0.5423, train_acc:0.8308, val_acc:0.3952\n",
      "epoch [40/100], loss:0.7167, val_loss:0.5441, train_acc:0.8333, val_acc:0.3876\n",
      "epoch [41/100], loss:0.7128, val_loss:0.5464, train_acc:0.8354, val_acc:0.3915\n",
      "epoch [42/100], loss:0.7084, val_loss:0.5477, train_acc:0.8380, val_acc:0.3880\n",
      "epoch [43/100], loss:0.7039, val_loss:0.5501, train_acc:0.8394, val_acc:0.3892\n",
      "epoch [44/100], loss:0.6997, val_loss:0.5512, train_acc:0.8424, val_acc:0.3881\n",
      "epoch [45/100], loss:0.6958, val_loss:0.5533, train_acc:0.8458, val_acc:0.3874\n",
      "epoch [46/100], loss:0.6914, val_loss:0.5549, train_acc:0.8461, val_acc:0.3844\n",
      "epoch [47/100], loss:0.6886, val_loss:0.5566, train_acc:0.8488, val_acc:0.3870\n",
      "epoch [48/100], loss:0.6845, val_loss:0.5591, train_acc:0.8506, val_acc:0.3848\n",
      "epoch [49/100], loss:0.6808, val_loss:0.5600, train_acc:0.8515, val_acc:0.3833\n",
      "epoch [50/100], loss:0.6776, val_loss:0.5616, train_acc:0.8513, val_acc:0.3856\n",
      "epoch [51/100], loss:0.6738, val_loss:0.5632, train_acc:0.8546, val_acc:0.3830\n",
      "epoch [52/100], loss:0.6708, val_loss:0.5646, train_acc:0.8563, val_acc:0.3809\n",
      "epoch [53/100], loss:0.6670, val_loss:0.5660, train_acc:0.8581, val_acc:0.3822\n",
      "epoch [54/100], loss:0.6640, val_loss:0.5677, train_acc:0.8589, val_acc:0.3790\n",
      "epoch [55/100], loss:0.6614, val_loss:0.5685, train_acc:0.8604, val_acc:0.3808\n",
      "epoch [56/100], loss:0.6588, val_loss:0.5700, train_acc:0.8609, val_acc:0.3814\n",
      "epoch [57/100], loss:0.6556, val_loss:0.5717, train_acc:0.8618, val_acc:0.3775\n",
      "epoch [58/100], loss:0.6525, val_loss:0.5730, train_acc:0.8633, val_acc:0.3761\n",
      "epoch [59/100], loss:0.6497, val_loss:0.5741, train_acc:0.8660, val_acc:0.3761\n",
      "epoch [60/100], loss:0.6471, val_loss:0.5755, train_acc:0.8666, val_acc:0.3800\n",
      "epoch [61/100], loss:0.6433, val_loss:0.5770, train_acc:0.8681, val_acc:0.3761\n",
      "epoch [62/100], loss:0.6416, val_loss:0.5794, train_acc:0.8686, val_acc:0.3753\n",
      "epoch [63/100], loss:0.6393, val_loss:0.5799, train_acc:0.8699, val_acc:0.3791\n",
      "epoch [64/100], loss:0.6356, val_loss:0.5817, train_acc:0.8716, val_acc:0.3765\n",
      "epoch [65/100], loss:0.6337, val_loss:0.5827, train_acc:0.8736, val_acc:0.3770\n",
      "epoch [66/100], loss:0.6304, val_loss:0.5841, train_acc:0.8736, val_acc:0.3771\n",
      "epoch [67/100], loss:0.6280, val_loss:0.5852, train_acc:0.8754, val_acc:0.3763\n",
      "epoch [68/100], loss:0.6251, val_loss:0.5870, train_acc:0.8763, val_acc:0.3777\n",
      "epoch [69/100], loss:0.6226, val_loss:0.5879, train_acc:0.8769, val_acc:0.3766\n",
      "epoch [70/100], loss:0.6205, val_loss:0.5896, train_acc:0.8788, val_acc:0.3775\n",
      "epoch [71/100], loss:0.6175, val_loss:0.5906, train_acc:0.8802, val_acc:0.3744\n",
      "epoch [72/100], loss:0.6157, val_loss:0.5921, train_acc:0.8814, val_acc:0.3759\n",
      "epoch [73/100], loss:0.6129, val_loss:0.5935, train_acc:0.8810, val_acc:0.3784\n",
      "epoch [74/100], loss:0.6111, val_loss:0.5948, train_acc:0.8819, val_acc:0.3750\n",
      "epoch [75/100], loss:0.6089, val_loss:0.5957, train_acc:0.8830, val_acc:0.3758\n",
      "epoch [76/100], loss:0.6074, val_loss:0.5968, train_acc:0.8832, val_acc:0.3787\n",
      "epoch [77/100], loss:0.6059, val_loss:0.5983, train_acc:0.8849, val_acc:0.3755\n",
      "epoch [78/100], loss:0.6033, val_loss:0.5987, train_acc:0.8846, val_acc:0.3765\n",
      "epoch [79/100], loss:0.6017, val_loss:0.6006, train_acc:0.8856, val_acc:0.3739\n",
      "epoch [80/100], loss:0.5991, val_loss:0.6019, train_acc:0.8868, val_acc:0.3780\n",
      "epoch [81/100], loss:0.5972, val_loss:0.6027, train_acc:0.8891, val_acc:0.3722\n",
      "epoch [82/100], loss:0.5950, val_loss:0.6039, train_acc:0.8882, val_acc:0.3760\n",
      "epoch [83/100], loss:0.5931, val_loss:0.6051, train_acc:0.8896, val_acc:0.3713\n",
      "epoch [84/100], loss:0.5911, val_loss:0.6059, train_acc:0.8903, val_acc:0.3775\n",
      "epoch [85/100], loss:0.5896, val_loss:0.6074, train_acc:0.8897, val_acc:0.3709\n",
      "epoch [86/100], loss:0.5870, val_loss:0.6088, train_acc:0.8917, val_acc:0.3750\n",
      "epoch [87/100], loss:0.5859, val_loss:0.6097, train_acc:0.8928, val_acc:0.3754\n",
      "epoch [88/100], loss:0.5837, val_loss:0.6108, train_acc:0.8950, val_acc:0.3706\n",
      "epoch [89/100], loss:0.5813, val_loss:0.6120, train_acc:0.8941, val_acc:0.3750\n",
      "epoch [90/100], loss:0.5794, val_loss:0.6123, train_acc:0.8929, val_acc:0.3713\n",
      "epoch [91/100], loss:0.5782, val_loss:0.6142, train_acc:0.8960, val_acc:0.3724\n",
      "epoch [92/100], loss:0.5762, val_loss:0.6152, train_acc:0.8965, val_acc:0.3758\n",
      "epoch [93/100], loss:0.5751, val_loss:0.6163, train_acc:0.8980, val_acc:0.3745\n",
      "epoch [94/100], loss:0.5734, val_loss:0.6176, train_acc:0.8992, val_acc:0.3713\n",
      "epoch [95/100], loss:0.5712, val_loss:0.6182, train_acc:0.8981, val_acc:0.3724\n",
      "epoch [96/100], loss:0.5709, val_loss:0.6200, train_acc:0.8977, val_acc:0.3727\n",
      "epoch [97/100], loss:0.5698, val_loss:0.6208, train_acc:0.8995, val_acc:0.3703\n",
      "epoch [98/100], loss:0.5678, val_loss:0.6216, train_acc:0.9005, val_acc:0.3719\n",
      "epoch [99/100], loss:0.5649, val_loss:0.6229, train_acc:0.9016, val_acc:0.3704\n",
      "epoch [100/100], loss:0.5640, val_loss:0.6235, train_acc:0.9017, val_acc:0.3707\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100             \n",
    "              \n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    n_total = 0.0\n",
    "\n",
    "    # for training\n",
    "    for user, movie, rating in X_train:\n",
    "        user = user.as_in_context(ctx)\n",
    "        movie = movie.as_in_context(ctx)\n",
    "        rating = rating.as_in_context(ctx)\n",
    "\n",
    "        with mx.autograd.record():\n",
    "            output = model(user, movie)\n",
    "            loss = criterion(output, rating)\n",
    "        loss.backward()\n",
    "        optimizer.step(user.shape[0])\n",
    "        running_loss += mx.nd.sum(loss).asscalar()\n",
    "        n_total += user.shape[0]\n",
    "\n",
    "    for val_user, val_movie, val_rating in X_eval:\n",
    "        val_user = val_user.as_in_context(ctx)\n",
    "        val_movie = val_movie.as_in_context(ctx)\n",
    "\n",
    "        with mx.autograd.record():\n",
    "            val_output = model(val_user, val_movie)\n",
    "            val_loss_tmp = criterion(val_output, val_rating)\n",
    "        val_loss += mx.nd.sum(val_loss_tmp).asscalar()\n",
    "    \n",
    "    train_accuracy = evaluate_accuracy(X_train, model)\n",
    "    test_accuracy = evaluate_accuracy(X_eval, model)\n",
    "    # ===================log========================\n",
    "    print('epoch [{}/{}], loss:{:.4f}, val_loss:{:.4f}, train_acc:{:.4f}, val_acc:{:.4f}'\n",
    "          .format(epoch + 1, num_epochs, np.sqrt(running_loss / n_total), np.sqrt(val_loss / n_total), train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T09:20:41.557515Z",
     "start_time": "2018-02-08T09:20:41.305067Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfwAAAFsCAYAAAApNAtQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1185a52b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_set = mx.nd.array((valid_users.astype('float32')))\n",
    "movie_set = mx.nd.array((valid_movies.astype('float32')))\n",
    "\n",
    "y_pred = model(user_set, movie_set).asnumpy().argmax(axis=1)\n",
    "y_pred = (y_pred + 1.) / 2\n",
    "\n",
    "plt.title(\"Distribution of Predictions\", fontsize=14)\n",
    "plt.ylabel(\"Count\", fontsize=14)\n",
    "plt.xlabel(\"Rating\", fontsize=14)\n",
    "plt.hist(y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T09:20:41.562933Z",
     "start_time": "2018-02-08T09:20:41.558964Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6769615737851655"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.sqrt(((y_pred - valid_ratings) ** 2).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
