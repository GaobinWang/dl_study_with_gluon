{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T07:37:41.655682Z",
     "start_time": "2018-02-01T07:37:40.918676Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/urllib3/contrib/pyopenssl.py:46: DeprecationWarning: OpenSSL.rand is deprecated - you should use os.urandom instead\n",
      "  import OpenSSL.SSL\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "print(mx.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T07:37:41.663157Z",
     "start_time": "2018-02-01T07:37:41.657466Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd\n",
    "import numpy as np\n",
    "mx.random.seed(1)\n",
    "#ctx = mx.gpu(0)\n",
    "ctx = mx.cpu(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset: “The Time Machine”\n",
    "Now mess with some data. I grabbed a copy of the Time Machine, mostly because it’s available freely thanks to the good people at Project Gutenberg and a lot of people are tired of seeing RNNs generate Shakespeare. In case you prefer torturing Shakespeare to torturing H.G. Wells, I’ve also included Andrej Karpathy’s tinyshakespeare.txt in the data folder. Let’s get started by reading in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T07:37:41.995038Z",
     "start_time": "2018-02-01T07:37:41.988599Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"./data/timemachine.txt\") as f:\n",
    "    time_machine = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And you’ll probably want to get a taste for what the text looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T07:37:42.438583Z",
     "start_time": "2018-02-01T07:37:42.434683Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿The Project Gutenberg EBook of Making the Nine, by Albertus T. Dudley\n",
      "\n",
      "This eBook is for the use of anyone anywhere in the United States and most\n",
      "other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever.  You may copy it, give it away or re-use it under the terms of\n",
      "the Project Gutenberg License included with this eBook or online at\n",
      "www.gutenberg.org.  If you are not located in the United States, you'll have\n",
      "to check the laws of the country where you are located before using this ebook.\n",
      "\n",
      "Title: Making the Nine\n",
      "\n",
      "Author: Albertus T. Dudley\n",
      "\n",
      "Illustrator: Charles Copeland\n",
      "\n",
      "Release Date: January 22, 2018 [EBook #56415]\n",
      "\n",
      "Language: English\n",
      "\n",
      "Character set encoding: UTF-8\n",
      "\n",
      "*** START OF THIS PROJECT GUTENBERG EBOOK MAKING THE NINE ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Produced by Barry Abrahamsen and the Online Distributed\n",
      "Proofreading Team at http://www.pgdp.net (This file was\n",
      "produced from images generously made available by The\n",
      "Internet Archive)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                            MAKING THE NINE\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(time_machine[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T07:37:42.683392Z",
     "start_time": "2018-02-01T07:37:42.656922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHAPTER I\n",
      "AN UNWELCOME PROPOSITION\n",
      "“HOW they do yell! Where’s your patriotism, Phil, to be hanging round in\n",
      "this gloomy crowd when all your friends are howling their heads off\n",
      "outside? Don’t you know Yale won the game? Why aren’t you out there with\n",
      "the rest?”\n",
      "Philip Poole looked up with a smile, but did not reply.\n",
      "“He’s comforting the afflicted,” said Dick Melvin, who shared with Poole\n",
      "the ownership of the room. “You don’t want to gloat over us poor\n",
      "Harvardites, do you, Phil? Thank you much for your sympathy.”\n",
      "“That isn’t the reason,” said the lad, after a pause, with the sober\n",
      "look in his big, wide-open eyes that made him seem serious even when his\n",
      "feelings inclined in the opposite direction. “I just don’t see any cause\n",
      "for such a racket. A Yale football victory over Harvard is too ordinary\n",
      "an occurrence to get wild over.”\n",
      "The chorus of hoots and groans that greeted this explanation brought a\n",
      "smile of satisfaction to the boy’s face. He was the youngest of the\n",
      "company, only in his seco\n"
     ]
    }
   ],
   "source": [
    "#time_machine = [x.strip() for x in time_machine.split('\\n')]\n",
    "time_machine = [x.strip() for x in time_machine.split('\\n') if list(set(x.strip())) not in [[], ['-']]]\n",
    "time_machine = '\\n'.join(time_machine[time_machine.index('CHAPTER I'):])\n",
    "print(time_machine[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical representations of characters\n",
    "When we create numerical representations of characters, we’ll use one-hot representations. A one-hot is a vector that takes value 1 in the index corresponding to a character, and 0 elsewhere. Because this vector is as long as the vocab, let’s get a definitive list of characters in this dataset so that our representation is not longer than necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T07:37:43.109722Z",
     "start_time": "2018-02-01T07:37:43.099092Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/', 'X', ']', ',', 'm', '╤', '\\n', 'N', \"'\", '○', 'n', 'K', 'x', 'R', 'r', '[', '┼', '—', '%', 'L', 'A', 'q', '9', '┴', '1', 'F', '”', ';', 'S', 'u', 'É', '3', '’', '│', '\"', ':', 'C', 'Z', 'h', 'V', 'k', '@', '(', 'f', '_', 's', 'ö', 'e', '.', 'D', 'w', '─', '!', 'H', 'g', 't', '4', 'l', '═', 'Q', 'i', 'b', 'é', '0', 'æ', 'M', '$', 'Æ', '-', 'I', 'T', 'P', 'Y', '‘', 'ê', '*', 'c', '7', '6', '“', ')', 'z', 'O', 'U', '–', 'G', '5', 'E', '●', ' ', 'J', '?', 'W', 'y', 'p', 'd', '8', '2', 'v', 'o', 'a', 'j', 'B']\n",
      "Length of vocab: 103\n"
     ]
    }
   ],
   "source": [
    "character_list = list(set(time_machine))\n",
    "vocab_size = len(character_list)\n",
    "print(character_list)\n",
    "print(\"Length of vocab: %s\" % vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We’ll often want to access the index corresponding to each character quickly so let’s store this as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T07:37:43.478926Z",
     "start_time": "2018-02-01T07:37:43.474141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'/': 0, 'X': 1, ']': 2, ',': 3, 'm': 4, '╤': 5, '\\n': 6, 'N': 7, \"'\": 8, '○': 9, 'n': 10, 'K': 11, 'x': 12, 'R': 13, 'r': 14, '[': 15, '┼': 16, '—': 17, '%': 18, 'L': 19, 'A': 20, 'q': 21, '9': 22, '┴': 23, '1': 24, 'F': 25, '”': 26, ';': 27, 'S': 28, 'u': 29, 'É': 30, '3': 31, '’': 32, '│': 33, '\"': 34, ':': 35, 'C': 36, 'Z': 37, 'h': 38, 'V': 39, 'k': 40, '@': 41, '(': 42, 'f': 43, '_': 44, 's': 45, 'ö': 46, 'e': 47, '.': 48, 'D': 49, 'w': 50, '─': 51, '!': 52, 'H': 53, 'g': 54, 't': 55, '4': 56, 'l': 57, '═': 58, 'Q': 59, 'i': 60, 'b': 61, 'é': 62, '0': 63, 'æ': 64, 'M': 65, '$': 66, 'Æ': 67, '-': 68, 'I': 69, 'T': 70, 'P': 71, 'Y': 72, '‘': 73, 'ê': 74, '*': 75, 'c': 76, '7': 77, '6': 78, '“': 79, ')': 80, 'z': 81, 'O': 82, 'U': 83, '–': 84, 'G': 85, '5': 86, 'E': 87, '●': 88, ' ': 89, 'J': 90, '?': 91, 'W': 92, 'y': 93, 'p': 94, 'd': 95, '8': 96, '2': 97, 'v': 98, 'o': 99, 'a': 100, 'j': 101, 'B': 102}\n"
     ]
    }
   ],
   "source": [
    "character_dict = {}\n",
    "for e, char in enumerate(character_list):\n",
    "    character_dict[char] = e\n",
    "print(character_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T07:37:43.685328Z",
     "start_time": "2018-02-01T07:37:43.660002Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36, 53, 20, 71, 70, 87, 13, 89, 69, 6, 20, 7, 89, 83, 7, 92, 87, 19, 36, 82, 65, 87, 89, 71, 13, 82, 71, 82, 28, 69, 70, 69, 82, 7, 6, 79, 53, 82, 92, 89, 55, 38, 47, 93, 89, 95, 99, 89, 93, 47, 57, 57, 52, 89, 92, 38, 47, 14, 47, 32, 45, 89, 93, 99, 29, 14, 89, 94, 100, 55, 14, 60, 99, 55, 60, 45, 4, 3, 89, 71, 38, 60, 57, 3, 89, 55, 99, 89, 61, 47, 89, 38, 100, 10, 54, 60, 10, 54, 89, 14, 99, 29, 10, 95, 89, 60, 10, 6, 55, 38, 60, 45, 89, 54, 57, 99, 99, 4, 93, 89, 76, 14, 99, 50, 95, 89, 50, 38, 47, 10, 89, 100, 57, 57, 89, 93, 99, 29, 14, 89, 43, 14, 60, 47, 10, 95, 45, 89, 100, 14, 47, 89, 38, 99, 50, 57, 60, 10, 54, 89, 55, 38, 47, 60, 14, 89, 38, 47, 100, 95, 45, 89, 99, 43, 43, 6, 99, 29, 55, 45, 60, 95, 47, 91, 89, 49, 99, 10, 32, 55, 89, 93, 99, 29, 89, 40, 10, 99, 50, 89, 72, 100, 57, 47, 89, 50, 99, 10, 89, 55, 38, 47, 89, 54, 100, 4, 47, 91, 89, 92, 38, 93, 89, 100, 14, 47, 10, 32, 55, 89, 93, 99, 29, 89, 99, 29, 55, 89, 55, 38, 47, 14, 47, 89, 50, 60, 55, 38, 6, 55, 38, 47, 89, 14, 47, 45, 55, 91, 26, 6, 71, 38, 60, 57, 60, 94, 89, 71, 99, 99, 57, 47, 89, 57, 99, 99, 40, 47, 95, 89, 29, 94, 89, 50, 60, 55, 38, 89, 100, 89, 45, 4, 60, 57, 47, 3, 89, 61, 29, 55, 89, 95, 60, 95, 89, 10, 99, 55, 89, 14, 47, 94, 57, 93, 48, 6, 79, 53, 47, 32, 45, 89, 76, 99, 4, 43, 99, 14, 55, 60, 10, 54, 89, 55, 38, 47, 89, 100, 43, 43, 57, 60, 76, 55, 47, 95, 3, 26, 89, 45, 100, 60, 95, 89, 49, 60, 76, 40, 89, 65, 47, 57, 98, 60, 10, 3, 89, 50, 38, 99, 89, 45, 38, 100, 14, 47, 95, 89, 50, 60, 55, 38, 89, 71, 99, 99, 57, 47, 6, 55, 38, 47, 89, 99, 50, 10, 47, 14, 45, 38, 60, 94, 89, 99, 43, 89, 55, 38, 47, 89, 14, 99, 99, 4, 48, 89, 79, 72, 99, 29, 89, 95, 99, 10, 32, 55, 89, 50, 100, 10, 55, 89, 55, 99, 89, 54, 57, 99, 100, 55, 89, 99, 98, 47, 14, 89, 29, 45, 89, 94, 99, 99, 14, 6, 53, 100, 14, 98, 100, 14, 95, 60, 55, 47, 45, 3, 89, 95, 99, 89, 93, 99, 29, 3, 89, 71, 38, 60, 57, 91, 89, 70, 38, 100, 10, 40, 89, 93, 99, 29, 89, 4, 29, 76, 38, 89, 43, 99, 14, 89, 93, 99, 29, 14, 89, 45, 93, 4, 94, 100, 55, 38, 93, 48, 26, 6, 79, 70, 38, 100, 55, 89, 60, 45, 10, 32, 55, 89, 55, 38, 47, 89, 14, 47, 100, 45, 99, 10, 3, 26, 89, 45, 100, 60, 95, 89, 55, 38, 47, 89, 57, 100, 95, 3, 89, 100, 43, 55, 47, 14, 89, 100, 89, 94, 100, 29, 45, 47, 3, 89, 50, 60, 55, 38, 89, 55, 38, 47, 89, 45, 99, 61, 47, 14, 6, 57, 99, 99, 40, 89, 60, 10, 89, 38, 60, 45, 89, 61, 60, 54, 3, 89, 50, 60, 95, 47, 68, 99, 94, 47, 10, 89, 47, 93, 47, 45, 89, 55, 38, 100, 55, 89, 4, 100, 95, 47, 89, 38, 60, 4, 89, 45, 47, 47, 4, 89, 45, 47, 14, 60, 99, 29, 45, 89, 47, 98, 47, 10, 89, 50, 38, 47, 10, 89, 38, 60, 45, 6, 43, 47, 47, 57, 60, 10, 54, 45, 89, 60, 10, 76, 57, 60, 10, 47, 95, 89, 60, 10, 89, 55, 38, 47, 89, 99, 94, 94, 99, 45, 60, 55, 47, 89, 95, 60, 14, 47, 76, 55, 60, 99, 10, 48, 89, 79, 69, 89, 101, 29, 45, 55, 89, 95, 99, 10, 32, 55, 89, 45, 47, 47, 89, 100, 10, 93, 89, 76, 100, 29, 45, 47, 6, 43, 99, 14, 89, 45, 29, 76, 38, 89, 100, 89, 14, 100, 76, 40, 47, 55, 48, 89, 20, 89, 72, 100, 57, 47, 89, 43, 99, 99, 55, 61, 100, 57, 57, 89, 98, 60, 76, 55, 99, 14, 93, 89, 99, 98, 47, 14, 89, 53, 100, 14, 98, 100, 14, 95, 89, 60, 45, 89, 55, 99, 99, 89, 99, 14, 95, 60, 10, 100, 14, 93, 6, 100, 10, 89, 99, 76, 76, 29, 14, 14, 47, 10, 76, 47, 89, 55, 99, 89, 54, 47, 55, 89, 50, 60, 57, 95, 89, 99, 98, 47, 14, 48, 26, 6, 70, 38, 47, 89, 76, 38, 99, 14, 29, 45, 89, 99, 43, 89, 38, 99, 99, 55, 45, 89, 100, 10, 95, 89, 54, 14, 99, 100, 10, 45, 89, 55, 38, 100, 55, 89, 54, 14, 47, 47, 55, 47, 95, 89, 55, 38, 60, 45, 89, 47, 12, 94, 57, 100, 10, 100, 55, 60, 99, 10, 89, 61, 14, 99, 29, 54, 38, 55, 89, 100, 6, 45, 4, 60, 57, 47, 89, 99, 43, 89, 45, 100, 55, 60, 45, 43, 100, 76, 55, 60, 99, 10, 89, 55, 99, 89, 55, 38, 47, 89, 61, 99, 93, 32, 45, 89, 43, 100, 76, 47, 48, 89, 53, 47, 89, 50, 100, 45, 89, 55, 38, 47, 89, 93, 99, 29, 10, 54, 47, 45, 55, 89, 99, 43, 89, 55, 38, 47, 6, 76, 99, 4, 94, 100, 10, 93, 3, 89, 99, 10, 57, 93, 89, 60, 10, 89, 38, 60, 45, 89, 45, 47, 76, 99]\n"
     ]
    }
   ],
   "source": [
    "time_numerical = [character_dict[char] for char in time_machine]\n",
    "print(time_numerical[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T07:37:43.861645Z",
     "start_time": "2018-02-01T07:37:43.853333Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336118\n",
      "336118\n",
      "[36, 53, 20, 71, 70, 87, 13, 89, 69, 6, 20, 7, 89, 83, 7, 92, 87, 19, 36, 82, 65, 87, 89, 71, 13]\n",
      "CHAPTER I\n",
      "AN UNWELCOME PR\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "#  Check that the length is right\n",
    "#########################\n",
    "print(len(time_machine))\n",
    "print(len(time_numerical))\n",
    "\n",
    "#########################\n",
    "#  Check that the format looks right\n",
    "#########################\n",
    "print(time_numerical[:25])\n",
    "\n",
    "#########################\n",
    "#  Convert back to text\n",
    "#########################\n",
    "print(\"\".join([character_list[idx] for idx in time_numerical[:25]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot representations\n",
    "We can use NDArray’s one_hot() operation to render a one-hot representation of each character. But frack it, since this is the from scratch tutorial, let’s write this ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T07:37:44.260622Z",
     "start_time": "2018-02-01T07:37:44.255018Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hots(numerical_list, vocab_size=vocab_size):\n",
    "    result = nd.zeros((len(numerical_list), vocab_size), ctx=ctx)\n",
    "    #for i, idx in enumerate(numerical_list):\n",
    "    #    result[i, idx] = 1.0\n",
    "    #Tutorial의 For문 비효율적 --> nd.arange로 수정 (17초 -> 0.2초)\n",
    "    result[nd.arange(len(numerical_list)), numerical_list] = 1.0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T07:37:44.674308Z",
     "start_time": "2018-02-01T07:37:44.457563Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "<NDArray 336118x103 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "print(one_hots(time_numerical))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### That looks about right. Now let’s write a function to convert our one-hots back to readable text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T07:37:44.844521Z",
     "start_time": "2018-02-01T07:37:44.839552Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def textify(embedding):\n",
    "    result = \"\"\n",
    "    indices = nd.argmax(embedding, axis=1).asnumpy()\n",
    "    for idx in indices:\n",
    "        result += character_list[int(idx)]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T07:37:45.030938Z",
     "start_time": "2018-02-01T07:37:45.022774Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHAPTER I\n",
      "AN UNWELCOME PROPOSITION\n",
      "“HOW they do yell! Where’s your patriotism, Phil, to be hanging r\n"
     ]
    }
   ],
   "source": [
    "print(textify(one_hots(time_numerical[0:100])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PreProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the data for training\n",
    "Great, it’s not the most efficient implementation, but we know how it works. So we’re already doing better than the majority of people with job titles in machine learning. Now, let’s chop up our dataset into sequences that we could feed into our model.\n",
    "\n",
    "You might think we could just feed in the entire dataset as one gigantic input and backpropagate across the entire sequence. When you try to backpropagate across thousands of steps a few things go wrong: (1) The time it takes to compute a single gradient update will be unreasonably long (2) The gradient across thousands of recurrent steps has a tendency to either blow up, causing NaN errors due to losing precision, or to vanish.\n",
    "\n",
    "Thus we’re going to look at feeding in our data in reasonably short sequences. Note that this home-brew version is pretty slow; if you’re still running on a CPU, this is the right time to make dinner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T07:37:45.872857Z",
     "start_time": "2018-02-01T07:37:45.786459Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset:  (5251, 64, 103)\n",
      "CHAPTER I\n",
      "AN UNWELCOME PROPOSITION\n",
      "“HOW they do yell! Where’s yo\n"
     ]
    }
   ],
   "source": [
    "seq_length = 64\n",
    "# -1 here so we have enough characters for labels later\n",
    "num_samples = (len(time_numerical) - 1) // seq_length\n",
    "dataset = one_hots(time_numerical[:seq_length*num_samples]).reshape((num_samples, seq_length, vocab_size))\n",
    "print('Shape of dataset: ', dataset.shape) #(num_samples, seq_length, vocab_size)\n",
    "print(textify(dataset[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we’ve chopped our dataset into sequences of length seq_length, at every time step, our input is a single one-hot vector. This means that our computation of the hidden layer would consist of matrix-vector multiplications, which are not especially efficient on GPU. To take advantage of the available computing resources, we’ll want to feed through a batch of sequences at the same time. The following code may look tricky but it’s just some plumbing to make the data look like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T07:37:46.201072Z",
     "start_time": "2018-02-01T07:37:46.197621Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T07:37:46.414071Z",
     "start_time": "2018-02-01T07:37:46.396283Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset:  (5251, 64, 103)\n",
      "# of sequences in dataset:  5251\n",
      "Size of batch:  32\n",
      "# of batches:  164\n",
      "Shape of train_data:  (32, 164, 64, 103)\n",
      "Shape of train_data:  (164, 32, 64, 103)\n",
      "Shape of train_data:  (164, 64, 32, 103)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of dataset: ', dataset.shape) #(num_samples, seq_length, vocab_size)\n",
    "print('# of sequences in dataset: ', len(dataset))\n",
    "\n",
    "num_batches = len(dataset) // batch_size\n",
    "print('Size of batch: ', batch_size)\n",
    "print('# of batches: ', num_batches)\n",
    "\n",
    "train_data = dataset[:num_batches*batch_size].reshape((batch_size, num_batches, seq_length, vocab_size))\n",
    "\n",
    "# swap batch_size and seq_length axis to make later access easier\n",
    "print('Shape of train_data: ', train_data.shape) #(batch_size, num_batches, seq_length, vocab_size)\n",
    "train_data = nd.swapaxes(train_data, 0, 1)\n",
    "print('Shape of train_data: ', train_data.shape) #(num_batches, batch_size, seq_length, vocab_size)\n",
    "train_data = nd.swapaxes(train_data, 1, 2)\n",
    "print('Shape of train_data: ', train_data.shape) #(num_batches, seq_length, batch_size, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let’s sanity check that everything went the way we hope. For each data_row, the second sequence should follow the first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T07:37:46.761853Z",
     "start_time": "2018-02-01T07:37:46.755081Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Batch 0:***\n",
      "\n",
      "CHAPTER I\n",
      "AN UNWELCOME PROPOSITION\n",
      "“HOW they do yell! Where’s yo\n",
      "\n",
      " “He’s scared as death of the mile run. I guess I’ll land\n",
      "him.”\n",
      "\n",
      "\n",
      "\n",
      "***Batch 1:***\n",
      "\n",
      "ur patriotism, Phil, to be hanging round in\n",
      "this gloomy crowd wh\n",
      "\n",
      "CHAPTER II\n",
      "ON THE ICE\n",
      "AS Dickinson foresaw, Melvin yielded to th\n",
      "\n",
      "\n",
      "***Batch 2:***\n",
      "\n",
      "en all your friends are howling their heads off\n",
      "outside? Don’t y\n",
      "\n",
      "e pressure brought to bear\n",
      "upon him, and resigned himself to the\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(\"***Batch %s:***\\n\\n%s\\n\\n%s\\n\\n\" % (i, textify(train_data[i, :, 0]), textify(train_data[i, :, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing our labels\n",
    "Now let’s repurpose the same batching code to create our label batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T07:37:47.283174Z",
     "start_time": "2018-02-01T07:37:47.196004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of labels:  (336064, 103)\n",
      "Shape of train_label_1:  (5251, 64, 103)\n",
      "Shape of train_label_1:  (5248, 64, 103)\n",
      "Shape of train_label_1:  (32, 164, 64, 103)\n",
      "Shape of train_label_1:  (164, 32, 64, 103)\n",
      "Shape of train_label_1:  (164, 64, 32, 103)\n"
     ]
    }
   ],
   "source": [
    "# Preparing the data for training 과 동일한 방법\n",
    "# (num_samples, seq_length, vocab_size)\n",
    "# --> num_samples를 num_batches*batch_size 길이로 고정\n",
    "# --> (batch_size, num_batches, seq_length, vocab_size)\n",
    "# --> (num_batches, seq_length, batch_size, vocab_size)\n",
    "\n",
    "labels = one_hots(time_numerical[1:seq_length*num_samples+1])\n",
    "print('Shape of labels: ', labels.shape) #(num_samples*seq_length, vocab_size)\n",
    "\n",
    "train_label_1 = labels.reshape((num_samples, seq_length, vocab_size))\n",
    "print('Shape of train_label_1: ', train_label_1.shape) #(num_samples, seq_length, vocab_size)\n",
    "train_label_1 = train_label_1[:num_batches*batch_size]\n",
    "print('Shape of train_label_1: ', train_label_1.shape) #(num_batches*batch_size, seq_length, vocab_size)\n",
    "\n",
    "train_label_1 = train_label_1.reshape((batch_size, num_batches, seq_length, vocab_size))\n",
    "print('Shape of train_label_1: ', train_label_1.shape) #(batch_size, num_batches, seq_length, vocab_size)\n",
    "train_label_1 = nd.swapaxes(train_label_1, 0, 1)\n",
    "print('Shape of train_label_1: ', train_label_1.shape) #(num_batches, batch_size, seq_length, vocab_size)\n",
    "train_label_1 = nd.swapaxes(train_label_1, 1, 2)\n",
    "print('Shape of train_label_1: ', train_label_1.shape) #(num_batches, seq_length, batch_size, vocab_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T07:37:47.821811Z",
     "start_time": "2018-02-01T07:37:47.723984Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of labels:  (336064, 103)\n",
      "Shape of train_label_2:  (32, 164, 64, 103)\n",
      "Shape of train_label_2:  (164, 32, 64, 103)\n",
      "Shape of train_label_2:  (164, 64, 32, 103)\n"
     ]
    }
   ],
   "source": [
    "labels = one_hots(time_numerical[1:seq_length*num_samples+1])\n",
    "print('Shape of labels: ', labels.shape) #(num_samples*seq_length, vocab_size)\n",
    "\n",
    "train_label_2 = labels.reshape((batch_size, num_batches, seq_length, vocab_size))\n",
    "print('Shape of train_label_2: ', train_label_2.shape) #(batch_size, num_batches, seq_length, vocab_size)\n",
    "train_label_2 = nd.swapaxes(train_label_2, 0, 1)\n",
    "print('Shape of train_label_2: ', train_label_2.shape) #(num_batches, batch_size, seq_length, vocab_size)\n",
    "train_label_2 = nd.swapaxes(train_label_2, 1, 2)\n",
    "print('Shape of train_label_2: ', train_label_2.shape) #(num_batches, seq_length, batch_size, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T07:37:48.470796Z",
     "start_time": "2018-02-01T07:37:48.246592Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(train_label_1.asnumpy(), train_label_2.asnumpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T07:37:48.489245Z",
     "start_time": "2018-02-01T07:37:48.474365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "np.array: Size 150 into (2,3,4,5)\n",
      "\n",
      "np.array: Cannot reshape array into different shape\n",
      "\n",
      "mx.nd.array: Size 150 into (2,3,4,5)\n",
      "\n",
      "\n",
      "[[[[   0.    1.    2.    3.    4.]\n",
      "   [   5.    6.    7.    8.    9.]\n",
      "   [  10.   11.   12.   13.   14.]\n",
      "   [  15.   16.   17.   18.   19.]]\n",
      "\n",
      "  [[  20.   21.   22.   23.   24.]\n",
      "   [  25.   26.   27.   28.   29.]\n",
      "   [  30.   31.   32.   33.   34.]\n",
      "   [  35.   36.   37.   38.   39.]]\n",
      "\n",
      "  [[  40.   41.   42.   43.   44.]\n",
      "   [  45.   46.   47.   48.   49.]\n",
      "   [  50.   51.   52.   53.   54.]\n",
      "   [  55.   56.   57.   58.   59.]]]\n",
      "\n",
      "\n",
      " [[[  60.   61.   62.   63.   64.]\n",
      "   [  65.   66.   67.   68.   69.]\n",
      "   [  70.   71.   72.   73.   74.]\n",
      "   [  75.   76.   77.   78.   79.]]\n",
      "\n",
      "  [[  80.   81.   82.   83.   84.]\n",
      "   [  85.   86.   87.   88.   89.]\n",
      "   [  90.   91.   92.   93.   94.]\n",
      "   [  95.   96.   97.   98.   99.]]\n",
      "\n",
      "  [[ 100.  101.  102.  103.  104.]\n",
      "   [ 105.  106.  107.  108.  109.]\n",
      "   [ 110.  111.  112.  113.  114.]\n",
      "   [ 115.  116.  117.  118.  119.]]]]\n",
      "<NDArray 2x3x4x5 @cpu(0)>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# (참고) mx.nd.array는 전체 길이가 다른 Shape으로 변경 가능\n",
    "\n",
    "print('\\nnp.array: Size 150 into (2,3,4,5)\\n')\n",
    "try:\n",
    "    print(np.array(range(150)).reshape((2,3,4,5)))\n",
    "except ValueError:\n",
    "    print('np.array: Cannot reshape array into different shape')\n",
    "\n",
    "print('\\nmx.nd.array: Size 150 into (2,3,4,5)\\n')\n",
    "try:\n",
    "    print(print(mx.nd.array(range(150)).reshape((2,3,4,5))))\n",
    "except ValueError:\n",
    "    print('mx.nd.array: Cannot reshape array into different shape')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A final sanity check\n",
    "Remember that our target at every time step is to predict the next character in the sequence. So our labels should look just like our inputs but offset by one character. Let’s look at corresponding inputs and outputs to make sure everything lined up as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T07:37:49.390049Z",
     "start_time": "2018-02-01T07:37:49.387101Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_label = train_label_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T07:37:49.611173Z",
     "start_time": "2018-02-01T07:37:49.606209Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e no more goals.\n",
      "Again Varrell took the puck, and with his famil\n",
      " no more goals.\n",
      "Again Varrell took the puck, and with his famili\n"
     ]
    }
   ],
   "source": [
    "print(textify(train_data[10, :, 3]))\n",
    "print(textify(train_label[10, :, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T07:37:50.328245Z",
     "start_time": "2018-02-01T07:37:50.314893Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch_data(data, seq_length, dims, batch_size, normalize=None, **kwargs):\n",
    "    \n",
    "    data = mx.nd.array(data)\n",
    "    \n",
    "    if normalize!=None:\n",
    "        data = normalize(data, **kwargs)\n",
    "        \n",
    "    data = data.reshape((-1, seq_length, dims))\n",
    "    \n",
    "    num_batches = len(data) // batch_size    \n",
    "    data_batch = data.reshape((batch_size, num_batches, seq_length, dims))\n",
    "    data_batch = nd.swapaxes(data_batch, 0, 1)\n",
    "    data_batch = nd.swapaxes(data_batch, 1, 2)\n",
    "    return data_batch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T07:37:50.932791Z",
     "start_time": "2018-02-01T07:37:50.828049Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_length = 64\n",
    "num_samples = (len(time_numerical) - 1) // seq_length\n",
    "batch_size = 32\n",
    "x = generate_batch_data(data = time_numerical[0:seq_length*num_samples],\n",
    "                        seq_length = seq_length,\n",
    "                        dims = vocab_size,\n",
    "                        batch_size = batch_size,\n",
    "                        normalize = one_hots,\n",
    "                        vocab_size = vocab_size\n",
    "                       )\n",
    "\n",
    "y = generate_batch_data(data = time_numerical[1:seq_length*num_samples+1],\n",
    "                        seq_length = seq_length,\n",
    "                        dims = vocab_size,\n",
    "                        batch_size = batch_size,\n",
    "                        normalize = one_hots,\n",
    "                        vocab_size = vocab_size\n",
    "                       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T07:37:51.753205Z",
     "start_time": "2018-02-01T07:37:51.748276Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1)\n",
    "train_idx = random.sample(range(len(x)), round(len(x) * 0.7))\n",
    "valid_idx = [x for x in range(len(x)) if x not in train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T07:37:52.229093Z",
     "start_time": "2018-02-01T07:37:52.085193Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = x[train_idx]\n",
    "train_y = y[train_idx]\n",
    "valid_x = x[valid_idx]\n",
    "valid_y = y[valid_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Activation, Loss, Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-27T09:04:00.078107Z",
     "start_time": "2018-01-27T09:04:00.071640Z"
    }
   },
   "source": [
    "#### Softmax Activation\n",
    "\n",
    "Softmax Function 형태: exp(f) / ∑exp(f)\n",
    "\n",
    "하지만 softmax function 을 코딩할 경우,\n",
    "\n",
    "큰 숫자들을 나누는 것은 numerically unstable 하기 때문에, 노말리제이션 트릭을 사용한다.\n",
    "\n",
    "(the intermediate terms exp(f) and ∑exp(f) may be very large due to the exponentials.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T07:37:53.241172Z",
     "start_time": "2018-02-01T07:37:53.227369Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[123 456 789]\n",
      "[  0.   0.  nan]\n",
      "[-666 -333    0]\n",
      "[  5.75274406e-290   2.39848787e-145   1.00000000e+000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: RuntimeWarning: overflow encountered in exp\n",
      "  \"\"\"\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# example with 3 classes and each having large scores\n",
    "f = np.array([123, 456, 789])\n",
    "# Bad: Numeric problem, potential blowup\n",
    "# instead: first shift the values of f so that the highest number is 0:\n",
    "p = np.exp(f) / np.sum(np.exp(f))\n",
    "\n",
    "# f becomes [-666, -333, 0]\n",
    "norm_f = f - np.max(f)\n",
    "# safe to do, gives the correct answer\n",
    "norm_p = np.exp(norm_f) / np.sum(np.exp(norm_f))\n",
    "\n",
    "print(f)\n",
    "print(p)\n",
    "print(norm_f)\n",
    "print(norm_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T07:37:53.678794Z",
     "start_time": "2018-02-01T07:37:53.667883Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1. / (1. + nd.exp(-x))\n",
    "# Same as nd.Activation(x, act_type='sigmoid')\n",
    "\n",
    "def tanh(x):\n",
    "    return (nd.exp(x) - nd.exp(-x)) / (nd.exp(x) + nd.exp(-x))\n",
    "# Same as nd.Activation(x, act_type='tanh')\n",
    "\n",
    "def softmax(y_linear, temperature=1.0):\n",
    "    lin = (y_linear-nd.max(y_linear, axis=1).reshape((-1,1))) / temperature # shift each row of y_linear by its max\n",
    "    exp = nd.exp(lin)\n",
    "    partition =nd.sum(exp, axis=1).reshape((-1,1))\n",
    "    return exp / partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T07:37:54.256660Z",
     "start_time": "2018-02-01T07:37:54.250448Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.88079703  0.11920292]\n",
       " [ 0.11920292  0.88079703]]\n",
       "<NDArray 2x2 @cpu(0)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################\n",
    "# With a temperature of 1 (always 1 during training), we get back some set of probabilities\n",
    "####################\n",
    "softmax(nd.array([[1, -1], [-1, 1]]), temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T07:37:54.628898Z",
     "start_time": "2018-02-01T07:37:54.623158Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.50049996  0.49949998]\n",
       " [ 0.49949998  0.50049996]]\n",
       "<NDArray 2x2 @cpu(0)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################\n",
    "# If we set a high temperature, we can get more entropic (*noisier*) probabilities\n",
    "####################\n",
    "softmax(nd.array([[1,-1],[-1,1]]), temperature=1000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T07:37:55.177833Z",
     "start_time": "2018-02-01T07:37:55.171527Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 1.  0.]\n",
       " [ 0.  1.]]\n",
       "<NDArray 2x2 @cpu(0)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################\n",
    "# Often we want to sample with low temperatures to produce sharp probabilities\n",
    "####################\n",
    "softmax(nd.array([[10,-10],[-10,10]]), temperature=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-entropy loss function\n",
    "At every time step our task is to predict the next character, given the string up to that point. This is the familiar multi-task classification that we introduced for handwritten digit classification. Accordingly, we’ll rely on the same loss function, cross-entropy.\n",
    "\n",
    "Cross-Entropy 형태: H(p,q) = -∑plog(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T07:37:55.796400Z",
     "start_time": "2018-02-01T07:37:55.786637Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def cross_entropy(yhat, y):\n",
    "#     return - nd.sum(y * nd.log(yhat))\n",
    "\n",
    "def cross_entropy(yhat, y):\n",
    "    return - nd.mean(nd.sum(y * nd.log(yhat), axis=0, exclude=True))\n",
    "\n",
    "# Define accuracy metric\n",
    "def accuracy(yhat, y):\n",
    "    pred = nd.argmax(yhat, axis = 1)\n",
    "    real = nd.argmax(y, axis = 1)\n",
    "    correct = nd.sum(pred == real)\n",
    "    total = yhat.shape[0]\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T07:37:56.353924Z",
     "start_time": "2018-02-01T07:37:56.347587Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[ 1.15129256]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy(nd.array([[.2,.5,.3], [.2,.5,.3]]), nd.array([[1.,0,0], [0, 1.,0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Averaging the loss over the sequence\n",
    "Because the unfolded RNN has multiple outputs (one at every time step) we can calculate a loss at every time step. The weights corresponding to the net at time step t influence both the loss at time step t and the loss at time step t+1. To combine our losses into a single global loss, we’ll take the average of the losses at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T07:37:57.494229Z",
     "start_time": "2018-02-01T07:37:57.483623Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def average_ce_loss(outputs, labels):\n",
    "    assert(len(outputs) == len(labels))\n",
    "    total_loss = 0.\n",
    "    for (output, label) in zip(outputs,labels):\n",
    "        total_loss = total_loss + cross_entropy(output, label)\n",
    "    return total_loss / len(outputs)\n",
    "\n",
    "def average_acc(outputs, labels):\n",
    "    assert(len(outputs) == len(labels))\n",
    "    total_acc = 0.\n",
    "    for (output, label) in zip(outputs,labels):\n",
    "        total_acc = total_acc + accuracy(output, label)\n",
    "    return total_acc / len(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T07:37:58.190432Z",
     "start_time": "2018-02-01T07:37:58.186703Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SGD(params, lr):\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-27T09:11:31.061746Z",
     "start_time": "2018-01-27T09:11:31.057468Z"
    }
   },
   "source": [
    "## 4. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recurrent neural networks\n",
    "\n",
    "Recall that the update for an ordinary hidden layer in a neural network with activation function ϕ is given by\n",
    "\n",
    "h=ϕ(xW + b)\n",
    "\n",
    "To make this a recurrent neural network, we’re simply going to add a weight sum of the previous hidden state ht−1ht−1:\n",
    "\n",
    "ht=ϕ(xtWxh + ht−1Whh+bh)\n",
    "\n",
    "Then at every time set t, we’ll calculate the output as:\n",
    "\n",
    "ŷt=softmax(htWhy + by)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T07:38:06.199053Z",
     "start_time": "2018-02-01T07:38:03.350982Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNModel_layer1():\n",
    "\n",
    "    def __init__(self, model, train, valid, hidden_dims, ctx):\n",
    "        assert(train[0].shape[1:] == valid[0].shape[1:])\n",
    "        assert(train[1].shape[1:] == valid[1].shape[1:])\n",
    "        \n",
    "        self.model = model\n",
    "        self.train_x = train[0]\n",
    "        self.train_y = train[1]\n",
    "        self.valid_x = valid[0]\n",
    "        self.valid_y = valid[1]\n",
    "        self.num_batches = self.train_x.shape[0]\n",
    "        self.input_dims = self.train_x.shape[3]\n",
    "        self.output_dims = self.train_y.shape[3]\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.seq_outputs = self.train_y.shape[1]\n",
    "        self.ctx = ctx\n",
    "                \n",
    "        \n",
    "    def allocate_params(self):        \n",
    "        if self.model == 'simple_rnn':\n",
    "            ########################\n",
    "            #  Weights connecting the inputs to the hidden layer\n",
    "            ########################\n",
    "            self.Wxh = nd.random_normal(shape=(self.input_dims, self.hidden_dims), ctx=self.ctx) * .01\n",
    "\n",
    "            ########################\n",
    "            #  Recurrent weights connecting the hidden layer across time steps\n",
    "            ########################\n",
    "            self.Whh = nd.random_normal(shape=(self.hidden_dims, self.hidden_dims), ctx=self.ctx) * .01\n",
    "\n",
    "            ########################\n",
    "            #  Bias vector for hidden layer\n",
    "            ########################\n",
    "            self.bh = nd.random_normal(shape=self.hidden_dims, ctx=self.ctx) * .01\n",
    "\n",
    "            ########################\n",
    "            # Weights to the output nodes\n",
    "            ########################\n",
    "            self.Why = nd.random_normal(shape=(self.hidden_dims, self.output_dims), ctx=self.ctx) * .01\n",
    "            self.by = nd.random_normal(shape=self.output_dims, ctx=self.ctx) * .01\n",
    "\n",
    "            # NOTE: to keep notation consistent,\n",
    "            # we should really use capital letters\n",
    "            # for hidden layers and outputs,\n",
    "            # since we are doing batchwise computations]\n",
    "\n",
    "            ########################\n",
    "            # Attach the gradients\n",
    "            ########################\n",
    "            self.params = [self.Wxh, self.Whh, self.bh, self.Why, self.by]\n",
    "            for self.param in self.params:\n",
    "                self.param.attach_grad()\n",
    "        \n",
    "        \n",
    "        elif self.model == 'lstm':\n",
    "            ########################\n",
    "            #  Weights connecting the inputs to the hidden layer\n",
    "            ########################\n",
    "            self.Wxg = nd.random_normal(shape=(self.input_dims, self.hidden_dims), ctx=self.ctx) * .01\n",
    "            self.Wxi = nd.random_normal(shape=(self.input_dims, self.hidden_dims), ctx=self.ctx) * .01\n",
    "            self.Wxf = nd.random_normal(shape=(self.input_dims, self.hidden_dims), ctx=self.ctx) * .01\n",
    "            self.Wxo = nd.random_normal(shape=(self.input_dims, self.hidden_dims), ctx=self.ctx) * .01\n",
    "\n",
    "            ########################\n",
    "            #  Recurrent weights connecting the hidden layer across time steps\n",
    "            ########################\n",
    "            self.Whg = nd.random_normal(shape=(self.hidden_dims, self.hidden_dims), ctx=self.ctx)* .01\n",
    "            self.Whi = nd.random_normal(shape=(self.hidden_dims, self.hidden_dims), ctx=self.ctx)* .01\n",
    "            self.Whf = nd.random_normal(shape=(self.hidden_dims, self.hidden_dims), ctx=self.ctx)* .01\n",
    "            self.Who = nd.random_normal(shape=(self.hidden_dims, self.hidden_dims), ctx=self.ctx)* .01\n",
    "\n",
    "            ########################\n",
    "            #  Bias vector for hidden layer\n",
    "            ########################\n",
    "            self.bg = nd.random_normal(shape=self.hidden_dims, ctx=self.ctx) * .01\n",
    "            self.bi = nd.random_normal(shape=self.hidden_dims, ctx=self.ctx) * .01\n",
    "            self.bf = nd.random_normal(shape=self.hidden_dims, ctx=self.ctx) * .01\n",
    "            self.bo = nd.random_normal(shape=self.hidden_dims, ctx=self.ctx) * .01\n",
    "\n",
    "            ########################\n",
    "            # Weights to the output nodes\n",
    "            ########################\n",
    "            self.Why = nd.random_normal(shape=(self.hidden_dims, self.output_dims), ctx=self.ctx) * .01\n",
    "            self.by = nd.random_normal(shape=self.output_dims, ctx=self.ctx) * .01\n",
    "\n",
    "            ########################\n",
    "            # Attach the gradients\n",
    "            ########################\n",
    "            self.params = [self.Wxg, self.Wxi, self.Wxf, self.Wxo]\n",
    "            self.params = self.params + [self.Whg, self.Whi, self.Whf, self.Who]\n",
    "            self.params = self.params + [self.bg, self.bi, self.bf, self.bo]\n",
    "            self.params = self.params + [self.Why, self.by]\n",
    "            for self.param in self.params:\n",
    "                self.param.attach_grad()\n",
    "\n",
    "        elif self.model == 'gru':\n",
    "            ########################\n",
    "            #  Weights connecting the inputs to the hidden layer\n",
    "            ########################\n",
    "            self.Wxz = nd.random_normal(shape=(self.input_dims, self.hidden_dims), ctx=self.ctx) * .01\n",
    "            self.Wxr = nd.random_normal(shape=(self.input_dims, self.hidden_dims), ctx=self.ctx) * .01\n",
    "            self.Wxh = nd.random_normal(shape=(self.input_dims, self.hidden_dims), ctx=self.ctx) * .01\n",
    "\n",
    "            ########################\n",
    "            #  Recurrent weights connecting the hidden layer across time steps\n",
    "            ########################\n",
    "            self.Whz = nd.random_normal(shape=(self.hidden_dims, self.hidden_dims), ctx=self.ctx)* .01\n",
    "            self.Whr = nd.random_normal(shape=(self.hidden_dims, self.hidden_dims), ctx=self.ctx)* .01\n",
    "            self.Whh = nd.random_normal(shape=(self.hidden_dims, self.hidden_dims), ctx=self.ctx)* .01\n",
    "\n",
    "            ########################\n",
    "            #  Bias vector for hidden layer\n",
    "            ########################\n",
    "            self.bz = nd.random_normal(shape=self.hidden_dims, ctx=self.ctx) * .01\n",
    "            self.br = nd.random_normal(shape=self.hidden_dims, ctx=self.ctx) * .01\n",
    "            self.bh = nd.random_normal(shape=self.hidden_dims, ctx=self.ctx) * .01\n",
    "\n",
    "            ########################\n",
    "            # Weights to the output nodes\n",
    "            ########################\n",
    "            self.Why = nd.random_normal(shape=(self.hidden_dims, self.output_dims), ctx=self.ctx) * .01\n",
    "            self.by = nd.random_normal(shape=self.output_dims, ctx=self.ctx) * .01\n",
    "            \n",
    "            ########################\n",
    "            # Attach the gradients\n",
    "            ########################\n",
    "            self.params = [self.Wxz, self.Wxr, self.Wxh]\n",
    "            self.params = self.params + [self.Whz, self.Whr, self.Whh]\n",
    "            self.params = self.params + [self.bz, self.br, self.bh]\n",
    "            self.params = self.params + [self.Why, self.by]\n",
    "            for self.param in self.params:\n",
    "                self.param.attach_grad()\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode %s. Options are simple_rnn, lstm, and gru\" % self.mode)\n",
    "    \n",
    "    def SGD(self, lr):\n",
    "        for self.param in self.params:\n",
    "            self.param[:] = self.param - lr * self.param.grad\n",
    "    \n",
    "    def rnn_model(self, inputs, h, c=None, mode='train', **kwargs):\n",
    "        outputs = []\n",
    "        for X in inputs:\n",
    "            if self.model == 'simple_rnn':\n",
    "                h_linear = nd.dot(X, self.Wxh) + nd.dot(h, self.Whh) + self.bh\n",
    "                h = nd.tanh(h_linear)\n",
    "                \n",
    "            elif self.model == 'lstm':\n",
    "                g = nd.tanh(nd.dot(X, self.Wxg) + nd.dot(h, self.Whg) + self.bg)\n",
    "                i = nd.sigmoid(nd.dot(X, self.Wxi) + nd.dot(h, self.Whi) + self.bi)\n",
    "                f = nd.sigmoid(nd.dot(X, self.Wxf) + nd.dot(h, self.Whf) + self.bf)\n",
    "                o = nd.sigmoid(nd.dot(X, self.Wxo) + nd.dot(h, self.Who) + self.bo)\n",
    "\n",
    "                c = f * c + i * g\n",
    "                h = o * nd.tanh(c)\n",
    "            elif self.model == 'gru':\n",
    "                z = nd.sigmoid(nd.dot(X, self.Wxz) + nd.dot(h, self.Whz) + self.bz)\n",
    "                r = nd.sigmoid(nd.dot(X, self.Wxr) + nd.dot(h, self.Whr) + self.br)\n",
    "                g = nd.tanh(nd.dot(X, Wxh) + nd.dot(r * h, self.Whh) + self.bh)\n",
    "                \n",
    "                h = z * h + (1 - z) * g\n",
    "                \n",
    "            yhat_linear = nd.dot(h, self.Why) + self.by\n",
    "                            \n",
    "            if(self.output_dims == 2):\n",
    "                yhat = sigmoid(yhat_linear)\n",
    "            elif(self.output_dims > 2):\n",
    "                yhat = softmax(yhat_linear, **kwargs)                \n",
    "            outputs.append(yhat)\n",
    "        \n",
    "        if self.seq_outputs == 1:\n",
    "            outputs = outputs[len(outputs)-1]\n",
    "            \n",
    "        if self.model in ['simple_rnn', 'gru']:\n",
    "            return (outputs, h)\n",
    "        elif self.model == 'lstm':\n",
    "            return (outputs, h, c)\n",
    "\n",
    "        \n",
    "    def set_sample_generation(self, prefix, num_chars, temperature=1.0):\n",
    "        self.prefix = prefix\n",
    "        self.num_chars = num_chars\n",
    "        self.temperature = temperature\n",
    "        \n",
    "    def run_sample_generation(self):\n",
    "        #####################################\n",
    "        # Initialize the string that we'll return to the supplied prefix\n",
    "        #####################################\n",
    "        string = self.prefix\n",
    "\n",
    "        #####################################\n",
    "        # Prepare the prefix as a sequence of one-hots for ingestion by RNN\n",
    "        #####################################\n",
    "        prefix_numerical = [character_dict[char] for char in self.prefix]\n",
    "        input = one_hots(prefix_numerical)\n",
    "\n",
    "        #####################################\n",
    "        # Set the initial state of the hidden representation ($h_0$) to the zero vector\n",
    "        #####################################\n",
    "        h = nd.zeros(shape=(1, self.hidden_dims), ctx=self.ctx)\n",
    "        if self.model == 'lstm':\n",
    "            c = nd.zeros(shape=(1, self.hidden_dims), ctx=self.ctx)\n",
    "\n",
    "\n",
    "\n",
    "        #####################################\n",
    "        # For num_chars iterations,\n",
    "        #     1) feed in the current input\n",
    "        #     2) sample next character from from output distribution\n",
    "        #     3) add sampled character to the decoded string\n",
    "        #     4) prepare the sampled character as a one_hot (to be the next input)\n",
    "        #####################################\n",
    "        for i in range(self.num_chars):\n",
    "            if self.model in ['simple_rnn', 'gru']:\n",
    "                outputs, h = self.rnn_model(input, h, temperature=.1)\n",
    "            elif self.model == 'lstm':\n",
    "                outputs, h, c = self.rnn_model(input, h, c, temperature=.1)\n",
    "\n",
    "            #outputs, h = self.simple_rnn(input, h, temperature=.1)\n",
    "            \n",
    "            choice = np.random.choice(vocab_size, p=outputs[-1][0].asnumpy())\n",
    "            string += character_list[choice]\n",
    "            input = one_hots([choice])\n",
    "        return string\n",
    "\n",
    "    def run(self, epochs, learning_rate):\n",
    "        for e in range(epochs):\n",
    "            ############################\n",
    "            # Attenuate the learning rate by a factor of 2 every 100 epochs.\n",
    "            ############################\n",
    "            if ((e+1) % 10 == 0):\n",
    "                learning_rate = learning_rate / 2.0\n",
    "            \n",
    "            h = nd.zeros(shape=(batch_size, self.hidden_dims), ctx=ctx)\n",
    "            if self.model == 'lstm':\n",
    "                c = nd.zeros(shape=(batch_size, self.hidden_dims), ctx=ctx)\n",
    "            \n",
    "            if self.seq_outputs == 1:\n",
    "                loss_func = cross_entropy\n",
    "                acc_func = accuracy\n",
    "                self.train_y = self.train_y[:,0]\n",
    "                self.valid_y = self.valid_y[:,0]\n",
    "                \n",
    "            elif self.seq_outputs > 1:\n",
    "                loss_func = average_ce_loss\n",
    "                acc_func = average_acc\n",
    "\n",
    "            for i in range(self.num_batches):\n",
    "                with autograd.record():\n",
    "                    if self.model in ['simple_rnn', 'gru']:\n",
    "                        outputs, h = self.rnn_model(self.train_x[i], h, mode='train')\n",
    "                    elif self.model == 'lstm':\n",
    "                        outputs, h, c = self.rnn_model(self.train_x[i], h, c, mode='train')\n",
    "\n",
    "                    loss = loss_func(outputs, self.train_y[i])\n",
    "                    loss.backward()\n",
    "                self.SGD(learning_rate)\n",
    "                \n",
    "                k = random.sample(range(len(valid_x)), 1)[0]\n",
    "                if self.model in ['simple_rnn', 'gru']:\n",
    "                    val_outputs, _ = self.rnn_model(self.valid_x[k], h, mode='valid')\n",
    "                elif self.model == 'lstm':\n",
    "                    val_outputs, _, _ = self.rnn_model(self.valid_x[k], h, c, mode='valid')\n",
    "                \n",
    "                val_loss = loss_func(val_outputs, self.valid_y[k])\n",
    "                val_acc = acc_func(val_outputs, self.valid_y[k])\n",
    "                \n",
    "                ##########################\n",
    "                #  Keep a moving average of the losses\n",
    "                ##########################\n",
    "                tr_loss = round(loss.asnumpy()[0], 3)\n",
    "                val_loss = round(val_loss.asnumpy()[0], 3)\n",
    "                val_acc = round(val_acc.asnumpy()[0], 3)\n",
    "\n",
    "                if (i == 0) and (e == 0):\n",
    "                    tr_moving_loss = round(np.mean(tr_loss) ,3)\n",
    "                    val_moving_loss = round(np.mean(val_loss), 3)\n",
    "                    val_moving_acc = round(np.mean(val_acc) ,3)\n",
    "                else:\n",
    "                    tr_moving_loss = round(.9 * tr_moving_loss + .1 * np.mean(tr_loss) ,3)\n",
    "                    val_moving_loss = round(.9 * val_moving_loss + .1 * np.mean(tr_loss) ,3)\n",
    "                    val_moving_acc = round(.9 * val_moving_acc + .1 * np.mean(val_acc) ,3)\n",
    "                \n",
    "#                 if i % 10 == 0:\n",
    "#                     print(\"Epoch %s. Batch %s. Loss: %s. Moving Loss: %s. Val Loss: %s. Val Moving Loss: %s. Val Acc: %s. Val Moving Acc: %s.\" %\n",
    "#                           (e, i, tr_loss, tr_moving_loss, val_loss, val_moving_loss, val_acc, val_moving_acc))\n",
    "            \n",
    "            print(\"Epoch %s. Loss: %s. Val Loss: %s. Val ACC: %s\" % (e, tr_moving_loss, val_moving_loss, val_moving_acc))\n",
    "            if 'prefix' in dir(self):\n",
    "                print(self.run_sample_generation())\n",
    "                \n",
    "#             print('**************** End of Epoch %s ****************' % (e))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T07:38:06.204569Z",
     "start_time": "2018-02-01T07:38:06.201301Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainset = [train_x, train_y]\n",
    "validset = [valid_x, valid_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T07:57:39.539552Z",
     "start_time": "2018-02-01T07:38:18.555516Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 3.166. Val Loss: 3.166. Val ACC: 0.148\n",
      "Do you know                                                  \n",
      "Epoch 1. Loss: 2.948. Val Loss: 2.948. Val ACC: 0.227\n",
      "Do you know the the the the the the the the the the the the t\n",
      "Epoch 2. Loss: 2.755. Val Loss: 2.755. Val ACC: 0.256\n",
      "Do you know an the the the the the the the the the the the th\n",
      "Epoch 3. Loss: 2.654. Val Loss: 2.654. Val ACC: 0.276\n",
      "Do you know an the the the the the the the the the the the th\n",
      "Epoch 4. Loss: 2.578. Val Loss: 2.578. Val ACC: 0.279\n",
      "Do you know an the the the the the the the the the the the th\n",
      "Epoch 5. Loss: 2.519. Val Loss: 2.519. Val ACC: 0.295\n",
      "Do you know and and and and and and and and and and an the th\n",
      "Epoch 6. Loss: 2.481. Val Loss: 2.481. Val ACC: 0.305\n",
      "Do you know an the the the the the the the the the the the th\n",
      "Epoch 7. Loss: 2.449. Val Loss: 2.449. Val ACC: 0.308\n",
      "Do you know an the the the the the the the the the the the th\n",
      "Epoch 8. Loss: 2.411. Val Loss: 2.411. Val ACC: 0.317\n",
      "Do you know the the the the the the the the the tore to the s\n",
      "Epoch 9. Loss: 2.365. Val Loss: 2.365. Val ACC: 0.332\n",
      "Do you know the the the the the southe the the the the the th\n",
      "Epoch 10. Loss: 2.349. Val Loss: 2.349. Val ACC: 0.339\n",
      "Do you know the sere the sous and the sare the the for the so\n",
      "Epoch 11. Loss: 2.333. Val Loss: 2.333. Val ACC: 0.34\n",
      "Do you know the the sere the sat the the the the sare the sor\n",
      "Epoch 12. Loss: 2.318. Val Loss: 2.318. Val ACC: 0.343\n",
      "Do you know the sere the sere to the sing the soon the sore t\n",
      "Epoch 13. Loss: 2.303. Val Loss: 2.303. Val ACC: 0.345\n",
      "Do you know the sere to the sere to the sere to the sere to t\n",
      "Epoch 14. Loss: 2.287. Val Loss: 2.287. Val ACC: 0.346\n",
      "Do you know the sere to the sere to the sand the sere to the \n",
      "Epoch 15. Loss: 2.274. Val Loss: 2.274. Val ACC: 0.354\n",
      "Do you know the sand the sere to the sing the sere to the sin\n",
      "Epoch 16. Loss: 2.261. Val Loss: 2.261. Val ACC: 0.352\n",
      "Do you know the sere to the sere to the sere to the sere to t\n",
      "Epoch 17. Loss: 2.246. Val Loss: 2.246. Val ACC: 0.359\n",
      "Do you know the sere to the sere to the sand the sere to the \n",
      "Epoch 18. Loss: 2.231. Val Loss: 2.231. Val ACC: 0.363\n",
      "Do you know the sand the seat on the sere to the seat on the \n",
      "Epoch 19. Loss: 2.203. Val Loss: 2.203. Val ACC: 0.374\n",
      "Do you know he ware the seat on the sand the seat on the sere\n",
      "Epoch 20. Loss: 2.195. Val Loss: 2.195. Val ACC: 0.373\n",
      "Do you know the sand the seat on the sain the seat on the sai\n",
      "Epoch 21. Loss: 2.189. Val Loss: 2.189. Val ACC: 0.368\n",
      "Do you know the seat on the sare to the seat on the sare to t\n",
      "Epoch 22. Loss: 2.182. Val Loss: 2.182. Val ACC: 0.38\n",
      "Do you know the sand the sand the sand the sane the sand the \n",
      "Epoch 23. Loss: 2.174. Val Loss: 2.174. Val ACC: 0.382\n",
      "Do you know the wast the sain the seat on the seat on the sto\n",
      "Epoch 24. Loss: 2.167. Val Loss: 2.167. Val ACC: 0.382\n",
      "Do you know the sand the seat on the seat on the sare the sin\n",
      "Epoch 25. Loss: 2.16. Val Loss: 2.16. Val ACC: 0.381\n",
      "Do you know the sand to the seat on the sane the seat of the \n",
      "Epoch 26. Loss: 2.154. Val Loss: 2.154. Val ACC: 0.384\n",
      "Do you know the sain to the sand the pare to the sain the sea\n",
      "Epoch 27. Loss: 2.148. Val Loss: 2.148. Val ACC: 0.387\n",
      "Do you know the stout the sere to the seated the sand the san\n",
      "Epoch 28. Loss: 2.142. Val Loss: 2.142. Val ACC: 0.39\n",
      "Do you know the store the sain the sand to the seat on the sa\n",
      "Epoch 29. Loss: 2.129. Val Loss: 2.129. Val ACC: 0.395\n",
      "Do you know the sand the sain to the sain the sain the seated\n",
      "Epoch 30. Loss: 2.125. Val Loss: 2.125. Val ACC: 0.39\n",
      "Do you know the seated the sand the sain the seated the sane \n",
      "Epoch 31. Loss: 2.122. Val Loss: 2.122. Val ACC: 0.396\n",
      "Do you know the seated the store the sare the seat on the sto\n",
      "Epoch 32. Loss: 2.118. Val Loss: 2.118. Val ACC: 0.395\n",
      "Do you know the sain to the seated the seat of the store the \n",
      "Epoch 33. Loss: 2.114. Val Loss: 2.114. Val ACC: 0.398\n",
      "Do you know the prouther the store the seated the sain the sa\n",
      "Epoch 34. Loss: 2.111. Val Loss: 2.111. Val ACC: 0.398\n",
      "Do you know the seated the sand the sand the store the sain t\n",
      "Epoch 35. Loss: 2.107. Val Loss: 2.107. Val ACC: 0.395\n",
      "Do you know the sain the sand the seated the seated the sand \n",
      "Epoch 36. Loss: 2.104. Val Loss: 2.104. Val ACC: 0.397\n",
      "Do you know the store the stout the stout the stout the store\n",
      "Epoch 37. Loss: 2.101. Val Loss: 2.101. Val ACC: 0.394\n",
      "Do you know the store the store the sand the sand the sain th\n",
      "Epoch 38. Loss: 2.098. Val Loss: 2.098. Val ACC: 0.397\n",
      "Do you know the sain the stout the store the sand the seated \n",
      "Epoch 39. Loss: 2.093. Val Loss: 2.093. Val ACC: 0.397\n",
      "Do you know the seated the store the sain the store the stout\n",
      "Epoch 40. Loss: 2.092. Val Loss: 2.092. Val ACC: 0.4\n",
      "Do you know the sand the said the sane the sand the store the\n",
      "Epoch 41. Loss: 2.09. Val Loss: 2.09. Val ACC: 0.4\n",
      "Do you know the store the sane the sand the sand the said the\n",
      "Epoch 42. Loss: 2.088. Val Loss: 2.088. Val ACC: 0.397\n",
      "Do you know the sand the sand the stout the store the seated \n",
      "Epoch 43. Loss: 2.087. Val Loss: 2.087. Val ACC: 0.398\n",
      "Do you know the seated the sain the sain the store the bane t\n",
      "Epoch 44. Loss: 2.085. Val Loss: 2.085. Val ACC: 0.397\n",
      "Do you know the seated the sand and the store the sand the se\n",
      "Epoch 45. Loss: 2.084. Val Loss: 2.084. Val ACC: 0.4\n",
      "Do you know the seated the said the sain the sand the seated \n",
      "Epoch 46. Loss: 2.082. Val Loss: 2.082. Val ACC: 0.406\n",
      "Do you know the store the sare the sane the beat of the seate\n",
      "Epoch 47. Loss: 2.08. Val Loss: 2.08. Val ACC: 0.402\n",
      "Do you know the sare the sain to the store the seated the sar\n",
      "Epoch 48. Loss: 2.079. Val Loss: 2.079. Val ACC: 0.406\n",
      "Do you know the seated the seated the said the store the sare\n",
      "Epoch 49. Loss: 2.076. Val Loss: 2.076. Val ACC: 0.401\n",
      "Do you know the seated the store the sand the sther he sand t\n"
     ]
    }
   ],
   "source": [
    "\n",
    "RNN = RNNModel_layer1(model = 'simple_rnn', train = trainset, valid = validset, hidden_dims = 256, ctx=ctx)\n",
    "RNN.allocate_params()\n",
    "RNN.set_sample_generation(prefix = 'Do you know', num_chars = 50)\n",
    "RNN.run(epochs = 50, learning_rate = .5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-02-01T05:53:27.108Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "RNN = RNNModel_layer1(model = 'lstm', train = trainset, valid = validset, hidden_dims = 256, ctx=ctx)\n",
    "RNN.allocate_params()\n",
    "RNN.set_sample_generation(prefix = 'Do you know', num_chars = 50)\n",
    "RNN.run(epochs = 50, learning_rate = .5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-02-01T05:53:27.388Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RNN = RNNModel_layer1(model = 'gru', train = trainset, valid = validset, hidden_dims = 256, ctx=ctx)\n",
    "RNN.allocate_params()\n",
    "RNN.set_sample_generation(prefix = 'Do you know', num_chars = 50)\n",
    "RNN.run(epochs = 50, learning_rate = .5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
