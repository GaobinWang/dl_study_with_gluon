{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **Matrix Factorization using Deep Network (Recommendation)**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[Ref.]\n",
    "  - https://hackernoon.com/autoencoders-deep-learning-bits-1-11731e200694\n",
    "  - http://katbailey.github.io/post/matrix-factorization-with-tensorflow/\n",
    "  - https://www.oreilly.com/ideas/deep-matrix-factorization-using-apache-mxnet\n",
    "  - http://nicolas-hug.com/blog/matrix_facto_3\n",
    "\n",
    "<br/>\n",
    "\n",
    "### **1) Recommendation 방식**\n",
    "\n",
    "\n",
    "\n",
    "<br/>\n",
    "- 기본적인 Recommendation의 구조는 아래와 같이 1) 사용자(User) x 2) 아이템(Item) 간의 스코어 matrix 형태로 표현할 수 있음\n",
    "<br/>\n",
    "<img src=\"./images/recomm._mat.png \" width=\"300\">\n",
    "<br/>\n",
    "- Recommendation을 하기 위해 사용자그룹의 기존(과거) 구매/평점기록 데이터를 활용함\n",
    "- 그러나 모든 사용자가 모든 아이템에 대한 구매/평점 기록이 없기에 전체 사용자 x 아이템으로 표현되는 Recommendation Matrix는 Sparse & Incomplete한 형태가 됨\n",
    "- 이 Incomplete Matrix의 Missing Values를 찾는 것이 Recommendation Algorithm의 목적\n",
    "- 크게 2가지 형태의 Recommendation Algorithm이 있음\n",
    "\n",
    "  **1) Collaborative Filtering (User-based or Item-based)**\n",
    "    - 협업필터링 방식으로 추천하고자 하는 대상과 가장 유사한 사용자그룹 혹은 아이템그룹의 유사도와 과거 구매/평점기록을 이용하여 추천하는 방식\n",
    "    - 그룹 간 유사도를 계산하는 방식으로 Cosine Similarity을 일반적으로 많이 이용함 \n",
    "    - Missing value에 대한 예측 시 (User or Item) Similairty와 기 존재하는 구매/평점기록값(구매 or 별점)과의 가중평균으로 계산 함\n",
    "    \n",
    "    (참조 : https://buildingrecommenders.wordpress.com/2015/11/18/overview-of-recommender-algorithms-part-2/)\n",
    "    - 1) User-Based CF\n",
    "        - 추천대상과 유사한 사용자 그룹을 파악하고, 해당 그룹이 많이 사용/구매한 아이템을 추천하는 방식\n",
    "        - 알고리즘 구현이 간단한 편이지만, 유저가 많아질 수록 연산이 복잡함 (nC2 경우의 수가 많아짐)\n",
    "    - 2) Item-Based CF\n",
    "        - 아이템간의 유사도 matrix를 기반으로, 특정 아이템을 추천하는 방식 (가중평균)\n",
    "        - User-Based CF에 비해 연산이 가벼운 편이지만, 개인 성향을 반영하기 어려움\n",
    "<br/>\n",
    "<img src=\"./images/CF_ex.png \" width=\"500\">\n",
    "<br/>\n",
    "\n",
    "  **2) Matrix Factorization**\n",
    "    - Sparse & Incomplete Recommendation을 2개 이상의 Low Rank Matrices로 Decomposition하는 방식\n",
    "    - Decomposition하는 2개의 Matrices는 추천대상(User) / 아이템(Item) Matrix로 나눠 짐\n",
    "    - 협업 필터링과 달리 사용자(User) / 아이템(Item)의 특성을 표현하는 Latent Factor (:= Embedding values)로 표현함\n",
    "    [Ex) 2 Dimension Latent Factor으로 표현한 예시]\n",
    "<br/>\n",
    "<img src=\"./images/mat_fact.png \" width=\"500\">\n",
    "<br/>\n",
    "    - Latent Factor로 표현된 2개의 Matrix의 Dot Product가 Recommendation Matrix의 추천 값이 되며, 이 값 중 과거 기록과의 차이를 최소화 하는 Latent Matrix를 찾는 것이 목적\n",
    "    - Performance Metric으로 RMSE를 주로 사용함\n",
    "    - 일반적으로 Latent Facotr의 Dimension은 User / Item의 Dimension보다 작은 값을 이용\n",
    "    - Matrix Factorization 방법으로 PCA / SVD 방법등이 있지만, Sparse & Incomplete Matrix이기에 Latent Matrices에 초기값을 주고 Iteration 마다 최적값을 찾아가는 Optimization 방법을 주로 이용\n",
    "    - 본 스터디에서는 Optimization 방법 중 Deep Learning Framework를 이용한 Vanila Matrix Factorization 방법과 Deep Matrix Facotrization 방법을 소개 함\n",
    "\n",
    " <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T06:48:59.470250Z",
     "start_time": "2018-02-08T06:48:59.458099Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py35/lib/python3.5/site-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['f']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import mxnet as mx\n",
    "import pandas\n",
    "import seaborn; seaborn.set_style('whitegrid')\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T01:02:57.731599Z",
     "start_time": "2018-02-08T01:02:57.727805Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gluon module load\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from mxnet import nd, autograd, gluon\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **1) Vanila Matrix Facotrization using synthetic data**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2개의 low rank latent matrix의 dot product로 예측하는 일반적인 Matrix Facotrization 방법\n",
    "- 2개의 latent matrix rank는 같음\n",
    "- rank <= input dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T01:02:58.181067Z",
     "start_time": "2018-02-08T01:02:57.737811Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic data set is generated. [row num : 35000]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfwAAAFtCAYAAADiaNj1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118757ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It appears to be a normal distribution that is centered around 0 with a variance of 1\n"
     ]
    }
   ],
   "source": [
    "## Generate random synthetic data set (250 users x 250 movies)\n",
    "X = numpy.random.randn(250, 250) # 250x250 = 62,500 complete data set (no missing values)\n",
    "\n",
    "# extract subset of data (Assume this is only given data)\n",
    "n = 35000\n",
    "i = numpy.random.randint(250, size=n) # Generate random row indexes\n",
    "j = numpy.random.randint(250, size=n) # Generate random column indexes\n",
    "X_values = X[i, j]\n",
    "print(\"Synthetic data set is generated. [row num : {}]\".format(X_values.shape[0]))\n",
    "\n",
    "# distribution of extracted values\n",
    "plt.title(\"Distribution of Data in Matrix\", fontsize=16)\n",
    "plt.ylabel(\"Count\", fontsize=14)\n",
    "plt.xlabel(\"Value\", fontsize=14)\n",
    "plt.hist(X_values, bins=100)\n",
    "plt.show()\n",
    "\n",
    "print('It appears to be a normal distribution that is centered around 0 with a variance of 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T01:03:02.031240Z",
     "start_time": "2018-02-08T01:03:02.021334Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Make Vanila Matrix Factorization(MF) using Gluon hybrid-block \n",
    "class Vanilla_MF(gluon.HybridBlock):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Vanilla_MF, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.user = nn.Embedding(input_dim=250, output_dim=25) # Embedding Layer for Users (25 latent factors)\n",
    "            self.movie = nn.Embedding(input_dim=250, output_dim=25) # Embedding Layer for Movies (25 latent factors)\n",
    "            self.flat = nn.Flatten()\n",
    "            \n",
    "    def hybrid_forward(self, F, user_i, movie_i):\n",
    "        # Here `F` can be either mx.nd or mx.sym, x is the input data,\n",
    "        user_i = self.user(user_i)\n",
    "        movie_i = self.movie(movie_i)\n",
    "        z = F.sum(F.dot(user_i, movie_i, transpose_b=True), axis = 1) # Dot product of users x movies and rowwise sum of result\n",
    "#         z = self.flat(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T01:03:02.231847Z",
     "start_time": "2018-02-08T01:03:02.223759Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model initialization\n",
    "model = Vanilla_MF()\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx=ctx)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "criterion = gluon.loss.L2Loss()\n",
    "optimizer = gluon.Trainer(model.collect_params(), 'adam',\n",
    "                          {'learning_rate': learning_rate,\n",
    "                           'wd': 1e-5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T01:03:03.748163Z",
     "start_time": "2018-02-08T01:03:03.740975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User embedding layer shape : (250, 25)\n",
      "\n",
      "Item embedding layer shape : (250, 25)\n",
      "\n",
      "25 Embedding latents factors of the first user : \n",
      "[ 0.08690061  0.11293826 -0.10049446  0.12735233 -0.03226358  0.10438174\n",
      "  0.12894627 -0.00171298  0.04623531 -0.14136802  0.01747404  0.06442726\n",
      "  0.05782974 -0.0706878  -0.11482002 -0.03529139 -0.03970727 -0.08992843\n",
      "  0.09868617  0.060701   -0.09062057 -0.02898844  0.10589617  0.02242789\n",
      " -0.11005463]\n"
     ]
    }
   ],
   "source": [
    "# Embedding layer latent factors\n",
    "user_latent_v = model.user.weight.data().asnumpy()\n",
    "print(\"User embedding layer shape : {}\\n\".format(user_latent_v.shape))\n",
    "item_latent_v = model.movie.weight.data().asnumpy()\n",
    "print(\"Item embedding layer shape : {}\\n\".format(item_latent_v.shape))\n",
    "\n",
    "print(\"25 Embedding latents factors of the first user : \\n{}\".format(user_latent_v[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T01:03:05.300419Z",
     "start_time": "2018-02-08T01:03:05.291926Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product of 1th user and 2th movie : -0.0330\n",
      "Model result of 1th user and 2th movie : -0.0330\n"
     ]
    }
   ],
   "source": [
    "# User rating sample by dot product of user x item embedding matrix\n",
    "user_i = 1\n",
    "movie_i = 2\n",
    "\n",
    "# naive dot product example\n",
    "user_v = user_latent_v[user_i]\n",
    "movie_v = item_latent_v[movie_i]\n",
    "print(\"Dot product of {}th user and {}th movie : {:.4f}\".format(user_i, movie_i, np.dot(user_v,movie_v)))\n",
    "\n",
    "# dot product using gluon network \n",
    "user_n = mx.nd.array([user_i])\n",
    "item_n = mx.nd.array([movie_i])\n",
    "print(\"Model result of {}th user and {}th movie : {:.4f}\".format(user_i, movie_i, model(user_n, item_n).asnumpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T01:03:05.523418Z",
     "start_time": "2018-02-08T01:03:05.517636Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make batch input using gluon DataLoader\n",
    "batch_size = 10000\n",
    "X_train = gluon.data.DataLoader(gluon.data.ArrayDataset(i[:25000].astype('float32'), j[:25000].astype('float32'), X_values[:25000].astype('float32')),\n",
    "                                      batch_size=batch_size, shuffle=True)\n",
    "\n",
    "X_eval = gluon.data.DataLoader(gluon.data.ArrayDataset(i[25000:].astype('float32'), j[25000:].astype('float32'), X_values[25000:].astype('float32')),\n",
    "                                      batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T01:03:06.335952Z",
     "start_time": "2018-02-08T01:03:06.315920Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Epoch Running Function\n",
    "def run_epoch(model, X_train, X_eval, num_epochs = 20):\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        n_total = 0.0\n",
    "\n",
    "        # for training\n",
    "        for user, movie, rating in X_train:\n",
    "            user = user.as_in_context(ctx)\n",
    "            movie = movie.as_in_context(ctx)\n",
    "            rating = rating.as_in_context(ctx)\n",
    "\n",
    "            with mx.autograd.record():\n",
    "                output = model(user, movie)\n",
    "                loss = criterion(output, rating)\n",
    "            loss.backward()\n",
    "            optimizer.step(user.shape[0])\n",
    "            running_loss += mx.nd.sum(loss).asscalar()\n",
    "            n_total += user.shape[0]\n",
    "\n",
    "        for val_user, val_movie, val_rating in X_eval:\n",
    "            val_user = val_user.as_in_context(ctx)\n",
    "            val_movie = val_movie.as_in_context(ctx)\n",
    "\n",
    "            with mx.autograd.record():\n",
    "                val_output = model(val_user, val_movie)\n",
    "                val_loss_tmp = criterion(val_output, val_rating)\n",
    "            val_loss += mx.nd.sum(val_loss_tmp).asscalar()\n",
    "\n",
    "        # ===================log========================\n",
    "        print('epoch [{}/{}], (rmse) loss:{:.4f}, (rmse) val_loss:{:.4f}'\n",
    "              .format(epoch + 1, num_epochs, np.sqrt(running_loss / n_total), np.sqrt(val_loss / n_total)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T01:05:30.598205Z",
     "start_time": "2018-02-08T01:03:09.195869Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/20], (rmse) loss:15.1786, (rmse) val_loss:9.6498\n",
      "epoch [2/20], (rmse) loss:14.5531, (rmse) val_loss:9.1241\n",
      "epoch [3/20], (rmse) loss:13.7855, (rmse) val_loss:8.6327\n",
      "epoch [4/20], (rmse) loss:13.0947, (rmse) val_loss:8.1688\n",
      "epoch [5/20], (rmse) loss:12.2848, (rmse) val_loss:7.7338\n",
      "epoch [6/20], (rmse) loss:11.7867, (rmse) val_loss:7.3285\n",
      "epoch [7/20], (rmse) loss:11.1770, (rmse) val_loss:6.9522\n",
      "epoch [8/20], (rmse) loss:10.2074, (rmse) val_loss:6.6044\n",
      "epoch [9/20], (rmse) loss:9.9400, (rmse) val_loss:6.2821\n",
      "epoch [10/20], (rmse) loss:9.5112, (rmse) val_loss:5.9847\n",
      "epoch [11/20], (rmse) loss:8.8817, (rmse) val_loss:5.7117\n",
      "epoch [12/20], (rmse) loss:8.5748, (rmse) val_loss:5.4551\n",
      "epoch [13/20], (rmse) loss:7.9662, (rmse) val_loss:5.2195\n",
      "epoch [14/20], (rmse) loss:7.6887, (rmse) val_loss:5.0020\n",
      "epoch [15/20], (rmse) loss:7.5419, (rmse) val_loss:4.7973\n",
      "epoch [16/20], (rmse) loss:6.9641, (rmse) val_loss:4.6076\n",
      "epoch [17/20], (rmse) loss:6.8033, (rmse) val_loss:4.4294\n",
      "epoch [18/20], (rmse) loss:6.4427, (rmse) val_loss:4.2650\n",
      "epoch [19/20], (rmse) loss:6.1965, (rmse) val_loss:4.1110\n",
      "epoch [20/20], (rmse) loss:5.8916, (rmse) val_loss:3.9712\n"
     ]
    }
   ],
   "source": [
    "## Model Running (Training)\n",
    "model = run_epoch(model, X_train, X_eval, num_epochs= 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Problem of the totally and randomly generated data set : no strong connections(dependencies) between two latent matrices\n",
    "- If there does not exist connections(dependencies) between two latent matrices, the performance of Vanilla MF does not seem to get increased well. \n",
    "- And this is because we generated the data using total random normal. \n",
    "- It is like  he \"movie watchers\" don't have any preferences, they just randomly like or don't like movies! It would be impossible to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T01:05:31.060463Z",
     "start_time": "2018-02-08T01:05:30.599777Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic data set is generated. [row num : 35000]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfwAAAFtCAYAAADiaNj1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3X98zfX///H72Waj7cyP3u/eFa0sbdEsTMMbE10yiUgM\ne7cP71IIGWLyu7dfCSProhDK/BjRj49SZMnkx+gQvZep95SPRb5EbAf7+fr+4eK8m80cHDvbXrfr\n5eJycV7Pc87rcZ57bffX6/l6vZ7HYhiGIQAAUKl5uLsAAABw6xH4AACYAIEPAIAJEPgAAJgAgQ8A\ngAkQ+IAT3HkzCzfSFGeGPjHDZ0TZIvBRKcTExCg4ONjxr0GDBmrWrJn69eunnTt3Fnnuhx9+qODg\nYJ0+fdqp9/7222/18ssvX/N5wcHBWrx4sSQpISFBjRs3vv4PcoWffvpJffr0cTxOTU1VcHCwvv/+\n+5t+b1eYPn26mjZtqiZNmshmsxVrv9zXl/89+OCDatSokZ566iktXLhQeXl5173OzZs3a+LEiTdd\n+59/XjfzHsHBwUpMTCyxfc+ePQoODr7ubeFGtjngWrzcXQDgKk2aNFFcXJwkKT8/X//v//0/JSUl\n6Z///KdmzZqlTp06SZIeffRRrV69Wv7+/k6979q1a/Xzzz9f83mrV6/W3XfffeMfoARffPFFkXB/\n6KGHtHr1at1///0uXc+NOHTokN577z316dNHjz/+uOrXr3/V57777ruyWq0yDENZWVnatWuX5s2b\nJ5vNpvnz58vT09Pp9b7//vu67bbbbrp+V/28LBaLNm3apJiYmGJtX3zxxQ29pzu3OVReBD4qDX9/\nfzVq1KjIsg4dOqhPnz6aNGmSWrdurerVq6tWrVqqVauWy9d/5bpvBT8/vzJZjzPOnj0rSerUqZNC\nQ0NLfe5DDz1UpM8jIiIUGBiosWPH6qOPPlL37t1vaa0lcVU/Nm7cWDabTadPny7yGQsLC7Vx40YF\nBwfr6NGjLlnXlcrLtoCKgSF9VGoeHh566aWXlJWV5TjaunJI//Dhw+rXr59jaPr5559Xenq6JGn0\n6NH66KOP9NNPPyk4OFipqan68MMP1axZM7377rtq1qyZ2rRpo/Pnz5c4vPrxxx+rbdu2evjhh9W/\nf38dOXLE0TZ69GjHqMNlmzdvVnBwsDIzM5WQkKC33nrL8d4ffvhhiUP6X375pZ555hk1atRIbdq0\n0dy5c5Wfn+9ob9eunRYtWqSJEycqPDzcMRKSnZ1dat+lp6erX79+Cg8PV3h4uEaOHKlTp05JunTK\n4vIRbY8ePUo8ur2WZ555RrVr19batWsdy7KzszVlyhS1bdtWISEhat68ueLi4nTu3DlJl07d7N69\nW19//bWjnyRp27ZtevbZZ9W4cWM1bNhQXbp00aZNm0pd/5WnYLp166ZPP/1UkZGRatiwoZ555hnt\n3bv3mp+jVatW8vHxUXJycpHle/fuld1uV0RERJHleXl5mjdvniIjIxUSEqJHHnlEgwcP1vHjxyXd\n2DaXn5+vLl26qF27drp48aJjPZ07d1bXrl1v6NQJKh8CH5VeeHi4PD09tW/fvmJthYWFGjhwoAoK\nCjRnzhzNmTNHZ86cUf/+/VVQUKCXXnpJbdq00T333KPVq1froYcekiRlZWVp/fr1mjVrll599dUS\nh5gvXLigWbNm6eWXX9Ybb7yhX375RX379tX58+edqrtHjx7q3r27qlatqtWrV+vRRx8t9pzVq1dr\n8ODBCg0N1VtvvaVnn31WS5Ys0ejRo4s8b8GCBTp37pzi4+MVGxurzz77TG+//fZV133w4EH17NlT\neXl5ev311zVmzBh9++23evbZZ3X+/Hn16NFDEyZMkHTpPP6NnFO3WCxq1qyZvv/+e0cgjRgxQl99\n9ZVGjBihxYsX67nnntOnn36q+fPnS5ImTpyoBg0aqEmTJlq9erXuuOMOHThwQC+++KIeeOABzZ8/\nX3PmzFG1atU0YsQIp6/TkKRffvlF8+bN0+DBg5WQkKCcnBwNHTq0yM5TSapWraqIiIhiOxhffPGF\n2rVrJx8fnyLLp0+fruXLl+uFF17QkiVLFBsbq507d2ratGmSdEPbnJeXl6ZOnarffvtNCxYskCS9\n8847+vnnnzVjxgxVqVLF6X5A5cWQPio9T09P1ahRw3F0+me///67fvnlFw0ZMkStW7eWJN111136\n9NNPdf78eQUEBKhWrVo6duxYkeHTgoICDR482PGakhiGoZkzZ6pFixaSpMDAQHXu3FmfffaZevTo\ncc2677zzTt15553y8PAocei2oKBAc+fO1ZNPPukI3FatWslqtWrixInq16+fHnzwQcd7xcfHy2Kx\nqFWrVtq9e7dSUlI0cuTIEtc9f/581apVS4sWLZK3t7ckKSQkRJ07d9a6desUExOjevXqSZIeeOAB\nx/+vV61atZSfn6+zZ8/KarUqLy9PkyZNchwVN2vWTPv27dPu3bslSfXq1ZOfn59uu+02R5/89NNP\nevzxx4vsdNx99916+umntX//frVt29apWux2u9577z3H6YnLO3zp6ekKCQkp9bWRkZEaNWqUsrKy\nHNcqbNy4URMmTHCMFl12+vRpjRo1ynEaIzw8XD///LPWr18vSTe8zYWEhOi5557Tu+++q4cfflgL\nFizQkCFDFBwc7NTnR+XHET5M7fbbb9d9992n8ePHa8yYMdq4caNq166t4cOHy2q1lvraunXrltpu\ntVodYS9dCsZ77rmnxKvZb8Thw4d1+vRpdejQocjyJ598UtKlK70va9iwoSwWi+PxnXfeWepIw549\ne/TYY485wl66FLbBwcHas2ePS+q/ko+Pj5YsWaKIiAhlZmbqm2++0dKlS5WRkVHqkPQzzzyjefPm\n6fz58/r++++1fv16rVixQpKUm5vr9Pq9vLyKBPudd94p6dJIzbW0adNGHh4e2rJliyTJZrMpOzu7\n2HC+JM2dO1fdu3fXiRMntHPnTq1YsUJ79+51qtZrbXNDhgzR3XffrYEDB+qhhx5Sv379rvmeMA+O\n8FHp5eTk6OzZs/rb3/5WrM3Dw0PvvfeeEhISlJycrHXr1qlq1arq1auX4uLi5OFx9X3ia134d/vt\nt5f4mqysrOv/ECW4fNHcleuxWq3y9vYuco6+WrVqRZ5jsVhKvc/73LlzJdZ/++23X/Pc//U4ceKE\nvL29VaNGDUlScnKypk+frqNHj6pmzZoKCQlR1apVVVhYeNX3OH/+vCZMmKDPP/9c0qVQvDyycT33\nsnt7exf5eV/+f2nrvszX11etW7fWl19+qaeeekobN25U27Ztiw3nS5fO7U+aNEmHDh2S1WpV/fr1\nS3xeSa61zfn4+CgyMlILFixQy5Ytr+vuB1R+HOGj0vv222+Vn5+vsLCwEtvvuusuTZs2TTt37tSq\nVavUsWNHvffeezd8S9Vlly80+7NTp045/mhbLJZiYWK3251+/8sh+fvvvxdbb25urqP9RlSvXr3Y\n+0qX6r+Z9/2zwsJC7dmzR40aNZKXl5d++eUXDR06VC1atNDWrVu1a9cuvfvuu9c8qp08ebK2b9+u\nhQsXat++ffr00081YMAAl9R4Pdq3b69t27bp/Pnz2rRpk5544oliz8nKytKAAQN09913a9OmTfr2\n22+VmJjokjkbJOnIkSN6//33FRwcrHfffdepW/tgHgQ+KjXDMLRw4ULVqFFD7du3L9aenp6uVq1a\nKS0tTR4eHmrSpImmTJkiLy8vHTt2TJJKPcovzenTp5WWluZ4nJaWpszMTIWHh0u6dFT4+++/Fwn9\nK4f7S1t33bp1VbNmzWI7Jhs2bJB0aV6CGxUWFqbk5OQiw8wZGRn68ccfb+p9/+yTTz7Rb7/95rie\n4YcfflBeXp5efPFFx3D6+fPnZbPZihypX9kn3333nVq3bq2WLVs6TkFs27ZNUtnOVteuXTvl5+fr\nnXfe0blz50oczj98+LDOnj2rPn366N5775V0acdnx44dpX5GZxiGoXHjxqlOnTpKSkpS7dq1NW7c\nOGbsgwND+qg0zp07p++++07SpYl3Tpw4oQ8++EB79uzRrFmz5OfnV+w19erVk6+vr+Li4jR48GBV\nr15dH3/8sSwWi+OqeH9/f/3222/avn37NS/e+jNvb28NHz5cr7zyivLy8jRr1iw9+OCDioyMlHTp\nXvTExES99tpr6tixo3bt2qXNmzcXeQ9/f39duHBBmzdvLnavu6enpwYPHqzJkyerevXqeuyxx3To\n0CElJCSoQ4cOCgoKup7uK2LAgAHq1auXXnjhBfXt21dZWVmaO3euateura5du173+6WlpTkuZjt3\n7pxSU1O1bNkytWvXTp07d5Yk1a9fX56enpo5c6Z69+6tM2fOaMmSJTp16lSRawn8/f118OBBpaam\n6uGHH1bDhg311Vdf6aOPPtJdd92lXbt2OW63u3yLWlmwWq36+9//riVLlqh9+/YlDtMHBgbK19dX\n8+fPV2FhoS5evKiVK1cqPT3dcZrFYrHc0Da3evVq7d69W4mJibrttts0YcIE9e3bVytXrtQ//vEP\nV39cVEAc4aPS2Lt3r3r27KmePXsqJiZGkydPVtWqVbVs2TJ17NixxNd4eXlp0aJFuvfeezVp0iT1\n799fhw8f1oIFCxxXnvfs2VO33367+vfvr+3btztdT+3atfXPf/5Tr732msaOHavQ0FAtWbLEEV4R\nEREaNmyYkpOT9eKLL+rgwYN6/fXXi7zHk08+qYceekixsbH65JNPiq3j2Wef1dSpU5WamqoBAwZo\nxYoVjpkFb0ZISIjef/995efna+jQoZo6daqaNm2qVatWlbjjdC39+vVTz5491atXLw0dOlS7du3S\nK6+8ooSEBMfFhHXr1tWMGTN06NAhvfjii5o1a5YaNmyoiRMn6vjx4zpx4oQkqW/fvsrNzVW/fv30\nww8/aPTo0fr73/+uadOmaciQIdq1a5feeust3XfffSXeinkrtW/fXnl5ecUupLzMarUqISFB586d\n08CBA/Wvf/1LNWrU0JtvvqnCwkLt379f0vVvcydOnNDMmTPVpUsXxwhSixYt1KlTJ82ePdtxjz/M\nzWIw3gMAQKXHET4AACZA4AMAYAIEPgAAJkDgAwBgAgQ+AAAmUKnvw3fVnOUAAFQUV5tVtFIHvnT1\nDy5d2iEorR3Ooy9dg350HfrSNehH1ymLviztQJchfQAATIDABwDABAh8AABMgMAHAMAECHwAAEyA\nwAcAwAQIfAAATIDABwDABAh8AABMgMAHAMAECHwAAEyAwAcAwAQq/ZfnAKj8Oo/4pMjj9bO7uKkS\noPziCB8AABMg8AEAMAECHwAAEyDwAQAwAQIfAAAT4Cp9AJWO46r9lZkltnMVP8yoTI/w9+/fr5iY\nmCLL1q9fr549ezoer1mzRt26dVNUVJS2bNkiSbp48aKGDBmi6OhovfDCCzp9+nRZlg0AQIVXZoG/\naNEijRs3Tjk5OY5lP/zwg9auXSvDMCRJJ0+eVGJiopKSkrR48WLFx8crNzdXq1atUlBQkFauXKmu\nXbtq/vz5ZVU2AACVQpkFfkBAgBISEhyPz5w5o/j4eI0ZM8ax7MCBA2rcuLG8vb1ltVoVEBCg9PR0\n2Ww2tW7dWpIUERGhnTt3llXZAABUCmV2Dj8yMlKZmZfOpxUUFGjs2LF69dVX5ePj43hOdna2rFar\n47Gvr6+ys7OLLPf19VVWVpbT67XZbDfVDufRl65BP9569PH1ob9cx5196ZaL9tLS0nTkyBFNmjRJ\nOTk5+s9//qOpU6eqefPmstvtjufZ7XZZrVb5+fk5ltvtdvn7+zu9rrCwsKu22Wy2UtvhPPrSNejH\nG3SVi/Ouhj52Htuk65RFX5a2Q+GWwA8NDdVnn30mScrMzNTw4cM1duxYnTx5UnPnzlVOTo5yc3OV\nkZGhoKAgNWnSRFu3blVoaKhSUlLY+AAAuE7l6ra8v/71r4qJiVF0dLQMw9CwYcPk4+Oj3r17Ky4u\nTr1791aVKlU0e/Zsd5cKAECFUqaBX6dOHa1Zs6bUZVFRUYqKiirynGrVqmnevHllUiMAAJURM+0B\nAGACBD4AACZA4AMAYAIEPgAAJkDgAwBgAuXqtjwAcIbj2/AAOI0jfAAATIDABwDABAh8AABMgHP4\nAEynpGsA1s/u4oZKgLLDET4AACZA4AMAYAIEPgAAJkDgAwBgAgQ+AAAmQOADAGACBD4AACZA4AMA\nYAIEPgAAJkDgAwBgAkytC8DtrpzqlmluAdcj8AGUeyXNfe/uGtgpQUVD4AMod9wR8AQ6KjvO4QMA\nYAIEPgAAJkDgAwBgAgQ+AAAmQOADAGACBD4AACZA4AMAYALchw+gzJWHiXSupSLUCFyPMj3C379/\nv2JiYiRJBw8eVHR0tGJiYvT888/r1KlTkqQ1a9aoW7duioqK0pYtWyRJFy9e1JAhQxQdHa0XXnhB\np0+fLsuyAQCo8Mos8BctWqRx48YpJydHkjR16lSNHz9eiYmJevzxx7Vo0SKdPHlSiYmJSkpK0uLF\nixUfH6/c3FytWrVKQUFBWrlypbp27ar58+eXVdkAAFQKZRb4AQEBSkhIcDyOj49X/fr1JUkFBQXy\n8fHRgQMH1LhxY3l7e8tqtSogIEDp6emy2Wxq3bq1JCkiIkI7d+4sq7IBAKgUyuwcfmRkpDIzMx2P\n77jjDknS3r17tXz5cq1YsULbtm2T1Wp1PMfX11fZ2dnKzs52LPf19VVWVpbT67XZbDfVDufRl65B\nP1YMZvo5memz3mru7Eu3XrS3YcMGvf3221q4cKFq1aolPz8/2e12R7vdbpfVai2y3G63y9/f3+l1\nhIWFXbXNZrOV2g7n0ZeuYZp+XJl57eeUc6b4OclE22QZKIu+LG2Hwm235X3yySdavny5EhMTdc89\n90iSQkNDZbPZlJOTo6ysLGVkZCgoKEhNmjTR1q1bJUkpKSlsfAAAXCe3HOEXFBRo6tSpuuuuuzRk\nyBBJ0iOPPKKXX35ZMTExio6OlmEYGjZsmHx8fNS7d2/FxcWpd+/eqlKlimbPnu2OsgEAqLDKNPDr\n1KmjNWvWSJJ2795d4nOioqIUFRVVZFm1atU0b968W14fAACVFRPvAMANuHJinvWzu7ipEsA5TK0L\nAIAJEPgAAJgAgQ8AgAkQ+AAAmACBDwCACRD4AACYAIEPAIAJEPgAAJgAgQ8AgAkQ+AAAmACBDwCA\nCRD4AACYAIEPAIAJ8G15AG65K79ZDkDZI/ABuBwBD5Q/DOkDAGACBD4AACZA4AMAYAIEPgAAJkDg\nAwBgAgQ+AAAmQOADAGACBD4AACZA4AMAYAIEPgAAJkDgAwBgAgQ+AAAmQOADAGACBD4AACbA1+MC\nuGl8HS5Q/pXpEf7+/fsVExMjSTpy5Ih69+6t6OhoTZw4UYWFhZKkNWvWqFu3boqKitKWLVskSRcv\nXtSQIUMUHR2tF154QadPny7LsgEAqPDKLPAXLVqkcePGKScnR5I0ffp0xcbGauXKlTIMQ8nJyTp5\n8qQSExOVlJSkxYsXKz4+Xrm5uVq1apWCgoK0cuVKde3aVfPnzy+rsgEAqBTKLPADAgKUkJDgeJyW\nlqbw8HBJUkREhHbs2KEDBw6ocePG8vb2ltVqVUBAgNLT02Wz2dS6dWvHc3fu3FlWZQMAUCmUWeBH\nRkbKy+u/lwwYhiGLxSJJ8vX1VVZWlrKzs2W1Wh3P8fX1VXZ2dpHll58LAACc57aL9jw8/ruvYbfb\n5e/vLz8/P9nt9iLLrVZrkeWXn+ssm812U+1wHn3pGvRjxVSZf26V+bOVNXf2pdsCv0GDBkpNTVWz\nZs2UkpKi5s2bKzQ0VHPnzlVOTo5yc3OVkZGhoKAgNWnSRFu3blVoaKhSUlIUFhbm9HpKe67NZruu\n98LV0ZeuUWH7cWWmuytwuwr5c3NChd0my6Gy6MvSdijcFvhxcXEaP3684uPjFRgYqMjISHl6eiom\nJkbR0dEyDEPDhg2Tj4+Pevfurbi4OPXu3VtVqlTR7Nmz3VU2ADjlylsV18/u4qZKgEvKNPDr1Kmj\nNWvWSJLq1q2r5cuXF3tOVFSUoqKiiiyrVq2a5s2bVyY1AsCNYC4ClHfMtAcAgAkQ+AAAmACBDwCA\nCRD4AACYAIEPAIAJEPgAAJgAgQ8AgAkQ+AAAmACBDwCACRD4AACYAIEPAIAJEPgAAJgAgQ8AgAkQ\n+AAAmACBDwCACRD4AACYAIEPAIAJEPgAAJgAgQ8AgAkQ+AAAmICXuwsAUPF0HvGJu0sAcJ0IfAAo\nA1fuJK2f3cVNlcCsGNIHAMAECHwAAEyAwAcAwARcEvinT592xdsAAIBbxOnAr1+/fonBnpmZqcce\ne8ylRQEAANcq9Sr9jz76SGvXrpUkGYahgQMHysur6EtOnjypO+6449ZVCAAAblqpgR8ZGalff/1V\nkmSz2dSkSRP5+voWeY6vr6/at29/6yoEAAA3rdTAv+222zR48GBJUu3atdWxY0f5+PiUSWEAAMB1\nnJ545+mnn1ZGRob+/e9/Kz8/X4ZhFGnv3r27y4sDAACu4XTgL1y4UPHx8apevXqxYX2LxULgAwBQ\njjkd+EuXLtXIkSP1/PPPu2zleXl5Gj16tH799Vd5eHho8uTJ8vLy0ujRo2WxWPTAAw9o4sSJ8vDw\n0Jo1a5SUlCQvLy8NHDhQbdu2dVkdAABUdk4Hfl5enssvztu6davy8/OVlJSk7du3a+7cucrLy1Ns\nbKyaNWumCRMmKDk5WY0aNVJiYqLWrVunnJwcRUdHq2XLlvL29nZpPQAAVFZO34ffpUsXrVixoti5\n+5tRt25dFRQUqLCwUNnZ2fLy8lJaWprCw8MlSREREdqxY4cOHDigxo0by9vbW1arVQEBAUpPT3dZ\nHQAAVHZOH+GfOXNGmzZt0vr161W7dm1VqVKlSPuKFSuue+W33Xabfv31Vz3xxBM6c+aM3nnnHe3Z\ns0cWi0XSpVv+srKylJ2dLavV6nidr6+vsrOznVqHzWa7qXY4j750DfrRHCrSz7ki1VreubMvnQ78\nwMBADRgwwKUrf++999SqVSuNGDFCx48fV58+fZSXl+dot9vt8vf3l5+fn+x2e5Hlf94BKE1YWNhV\n22w2W6ntcB596RoVph9XZrq7ggqvQvycVYG2yQqgLPqytB0KpwP/8v34ruTv7+8YKahevbry8/PV\noEEDpaamqlmzZkpJSVHz5s0VGhqquXPnKicnR7m5ucrIyFBQUJDL6wFQsiu/yx1AxeN04I8aNarU\n9jfeeOO6V963b1+NGTNG0dHRysvL07BhwxQSEqLx48crPj5egYGBioyMlKenp2JiYhQdHS3DMDRs\n2DAmAAIA4Do4Hfienp5FHufn5+vo0aM6ePCg+vTpc0Mr9/X11Ztvvlls+fLly4sti4qKUlRU1A2t\nBwAAs3M68KdPn17i8qVLl+qHH35wWUEAAMD1nL4t72oef/xxbd682RW1AACAW8TpI/zCwsJiy+x2\nu5KSklSzZk2XFgUAAFzL6cBv0KCB4/74P/Px8dGUKVNcWhQAAHAtpwN/2bJlRR5bLBZVqVJF9erV\nk5+fn8sLAwAAruN04F+e7jYjI0MZGRkqKChQ3bp1CXsAACoApwP/7NmziouL09dff63q1auroKBA\ndrtdTZs21fz5852e+Q4AUHwyo/Wzu7ipEpiF01fpT548WSdPntSGDRuUmpqqb7/9VuvXr9eFCxeu\nesseAAAoH5wO/C1btui1115TYGCgY1m9evUcX2ELAADKL6cDv2rVqiUut1gsKigocFlBAADA9ZwO\n/Hbt2ulf//qXfv75Z8eyw4cPa/LkyWrbtu0tKQ4AALiG0xftjRw5UoMGDdITTzzhuDLfbrerTZs2\nGj9+/C0rEEDZ49vxgMrHqcA/cOCAgoODlZiYqEOHDikjI0O5ubmqU6eOmjZteqtrBAAAN6nUIf38\n/HyNHDlSPXv21P79+yVJwcHB6tixo7Zu3aqYmBiNGzeOc/gAAJRzpQb+kiVLlJqaqmXLljkm3rls\nzpw5Wrp0qZKTk5WYmHhLiwQAADen1MD/6KOPNH78eD3yyCMltjdv3lyjRo3S2rVrb0lxAADANUoN\n/OPHj6tBgwalvkHTpk2VmZnp0qIAAIBrlRr4f/nLX64Z5seOHePrcQEAKOdKDfzHH39cCQkJysvL\nK7E9Ly9Pb731liIiIm5JcQAAwDVKvS3vpZdeUvfu3dWtWzfFxMQoJCREVqtVZ8+e1YEDB7RixQrl\n5OQoPj6+rOoFAAA3oNTAt1qtWrNmjWbOnKnXX39dFy5ckCQZhqHq1aurU6dOGjRokGrVqlUmxQIA\ngBtzzYl3qlevrilTpmjChAk6evSozp07p5o1ayogIEAeHk7PzAsAANzI6al1vb29df/999/KWgDA\ntK6cznj97C5uqgSVFYfoAACYAIEPAIAJEPgAAJgAgQ8AgAkQ+AAAmACBDwCACRD4AACYAIEPAIAJ\nOD3xzq2yYMECffXVV8rLy1Pv3r0VHh6u0aNHy2Kx6IEHHtDEiRPl4eGhNWvWKCkpSV5eXho4cKDa\ntm3r7tIBAKgw3Br4qamp2rdvn1atWqULFy5oyZIlmj59umJjY9WsWTNNmDBBycnJatSokRITE7Vu\n3Trl5OQoOjpaLVu2lLe3tzvLByqNK2d5g/sx8x5cza1D+t98842CgoI0aNAgDRgwQI8++qjS0tIU\nHh4uSYqIiNCOHTt04MABNW7cWN7e3rJarQoICFB6ero7SwcAoEJx6xH+mTNndOzYMb3zzjvKzMzU\nwIEDZRiGLBaLJMnX11dZWVnKzs6W1Wp1vM7X11fZ2dlOrcNms91UO5xHX7oG/YiSuHO7YJt0HXf2\npVsDv0aNGgoMDJS3t7cCAwPl4+Oj3377zdFut9vl7+8vPz8/2e32Isv/vANQmrCwsKu22Wy2Utvh\nPPrSNdzWjyszy36duC7u+v3id9t1yqIvS9uhcOuQflhYmLZt2ybDMHTixAlduHBBLVq0UGpqqiQp\nJSVFTZtNNi0WAAAPk0lEQVQ2VWhoqGw2m3JycpSVlaWMjAwFBQW5s3QAACoUtx7ht23bVnv27FH3\n7t1lGIYmTJigOnXqaPz48YqPj1dgYKAiIyPl6empmJgYRUdHyzAMDRs2TD4+Pu4sHQCACsXtt+WN\nGjWq2LLly5cXWxYVFaWoqKiyKAkAgEqHiXcAADABAh8AABMg8AEAMAECHwAAEyDwAQAwAQIfAAAT\nIPABADABAh8AABMg8AEAMAECHwAAE3D71LoAgGvrPOKTIo/Xz+7ipkpQUXGEDwCACRD4AACYAIEP\nAIAJcA4fMKErzwcDqPw4wgcAwAQIfAAATIDABwDABDiHD1RynK8HIHGEDwCAKXCEDwAVEDPv4Xpx\nhA8AgAkQ+AAAmACBDwCACRD4AACYAIEPAIAJcJU+AFQCXLWPayHwgUqGiXYAlIQhfQAATIDABwDA\nBAh8AABMoFwE/u+//642bdooIyNDR44cUe/evRUdHa2JEyeqsLBQkrRmzRp169ZNUVFR2rJli5sr\nBgCgYnF74Ofl5WnChAmqWrWqJGn69OmKjY3VypUrZRiGkpOTdfLkSSUmJiopKUmLFy9WfHy8cnNz\n3Vw5AAAVh9sDf8aMGerVq5fuuOMOSVJaWprCw8MlSREREdqxY4cOHDigxo0by9vbW1arVQEBAUpP\nT3dn2QAAVChuvS3vww8/VK1atdS6dWstXLhQkmQYhiwWiyTJ19dXWVlZys7OltVqdbzO19dX2dnZ\nTq3DZrPdVDucR1+6Bv0IV3DldsQ26Tru7Eu3Bv66detksVi0c+dOHTx4UHFxcTp9+rSj3W63y9/f\nX35+frLb7UWW/3kHoDRhYWFXbbPZbKW2w3n0pWu4pB9XZrqmGFRorvp95HfbdcqiL0vboXDrkP6K\nFSu0fPlyJSYmqn79+poxY4YiIiKUmpoqSUpJSVHTpk0VGhoqm82mnJwcZWVlKSMjQ0FBQe4sHQCA\nCqXczbQXFxen8ePHKz4+XoGBgYqMjJSnp6diYmIUHR0twzA0bNgw+fj4uLtUAAAqjHIT+ImJiY7/\nL1++vFh7VFSUoqKiyrIkAAAqDbdfpQ8AAG49Ah8AABMoN0P6AG4M346HkvB1ubgSR/gAAJgAgQ8A\ngAkQ+AAAmACBDwCACRD4AACYAIEPAIAJEPgAAJgAgQ8AgAkQ+AAAmACBDwCACRD4AACYAIEPAIAJ\n8OU5QAXDl+UAuBEc4QMAYAIEPgAAJkDgAwBgAgQ+AAAmQOADAGACXKUPACZw5d0d62d3cVMlcBcC\nHwBMiB0A82FIHwAAEyDwAQAwAYb0gXKOmfUAuAJH+AAAmACBDwCACRD4AACYAIEPAIAJcNEeUM5w\nkR6AW8GtgZ+Xl6cxY8bo119/VW5urgYOHKh69epp9OjRslgseuCBBzRx4kR5eHhozZo1SkpKkpeX\nlwYOHKi2bdu6s3QAACoUtwb+//7v/6pGjRqaOXOm/vjjD3Xt2lUPPvigYmNj1axZM02YMEHJyclq\n1KiREhMTtW7dOuXk5Cg6OlotW7aUt7e3O8sHAKDCcGvgd+jQQZGRkZIkwzDk6emptLQ0hYeHS5Ii\nIiK0fft2eXh4qHHjxvL29pa3t7cCAgKUnp6u0NBQd5YPAECF4daL9nx9feXn56fs7Gy9/PLLio2N\nlWEYslgsjvasrCxlZ2fLarUWeV12dra7ygYAoMJx+0V7x48f16BBgxQdHa3OnTtr5syZjja73S5/\nf3/5+fnJbrcXWf7nHYDS2Gy2m2qH8+hLoOIq7feX323XcWdfujXwT506peeee04TJkxQixYtJEkN\nGjRQamqqmjVrppSUFDVv3lyhoaGaO3eucnJylJubq4yMDAUFBTm1jrCwsKu22Wy2UtvhPPrSNfjD\nCne52u8vv9uuUxZ9WdrfELcG/jvvvKNz585p/vz5mj9/viRp7NixmjJliuLj4xUYGKjIyEh5enoq\nJiZG0dHRMgxDw4YNk4+PjztLB4BKpaTbQfnK3MrFrYE/btw4jRs3rtjy5cuXF1sWFRWlqKiosigL\nAIBKh5n2AAAwAbdftAcAKJ8cw/wrM0tsZ8i/YiHwgTJ25blS/mgCKAsM6QMAYAIEPgAAJsCQPuBm\nfDsegLLAET4AACZA4AMAYAIEPgAAJkDgAwBgAly0BwC4IcwpUbFwhA8AgAkQ+AAAmACBDwCACXAO\nH3AxzmvCrNj2yzeO8AEAMAGO8IFbjKlzAZQHHOEDAGACBD4AACZA4AMAYAIEPgAAJsBFe8BN4qI8\nABUBgQ9cJwIeuDHcp+9eBD4A4Ja43p1jdghuLQIfuAJ/dABURgQ+cA0M4QPuwc63axH4AAC3YMi/\nbHFbHgAAJkDgAwBgAgzpw/Q4Rw/ADAh8mA4BD1QOnNO/PgQ+KrSSwptfesCc2AEoXYUJ/MLCQk2a\nNEmHDh2St7e3pkyZonvvvdfdZaEc4ggeAIqrMIG/efNm5ebmavXq1fruu+/0+uuv6+2333Z3WbhJ\nhDOAW+Vm/75UthGCChP4NptNrVu3liQ1atRI//73v8u8BjMMF13rM5b6C7Qy81aUBABu4cwpw+ve\nqbji72RZ5ojFMAyjzNZ2E8aOHav27durTZs2kqRHH31UmzdvlpfX1fdZbDZbWZUHAEC5EBYWVuLy\nCnOE7+fnJ7vd7nhcWFhYathLV//QAACYTYWZeKdJkyZKSUmRJH333XcKCgpyc0UAAFQcFWZI//JV\n+j/++KMMw9C0adN0//33u7ssAAAqhAoT+AAA4MZVmCF9AABw4wh8AABMwHSBf/78eQ0cOFD/+Mc/\n1LdvX504cULSpQsBe/TooV69eumtt95yc5XlX1ZWlgYMGKBnn31WPXv21L59+yTRjzfjyy+/1IgR\nIxyP6cvrV1hYqAkTJqhnz56KiYnRkSNH3F1ShbR//37FxMRIko4cOaLevXsrOjpaEydOVGFhoZur\nK//y8vI0cuRIRUdHq3v37kpOTi4f/WiYzNKlS42EhATDMAxj3bp1xuTJkw3DMIynnnrKOHLkiFFY\nWGj069fPSEtLc2eZ5d6bb75pLF261DAMw8jIyDC6du1qGAb9eKMmT55sREZGGrGxsY5l9OX127hx\noxEXF2cYhmHs27fPGDBggJsrqngWLlxodOrUyejRo4dhGIbRv39/Y9euXYZhGMb48eONTZs2ubO8\nCmHt2rXGlClTDMMwjDNnzhht2rQpF/1ouiP8vn37auDAgZKkY8eOyd/fX9nZ2crNzVVAQIAsFota\ntWqlHTt2uLnS8q1v377q1auXJKmgoEA+Pj70401o0qSJJk2a5HhMX96Y8jAjZ0UXEBCghIQEx+O0\ntDSFh4dLkiIiItgOndChQwcNHTpUkmQYhjw9PctFP1aYiXduxAcffKD333+/yLJp06YpNDRU//M/\n/6Mff/xRS5cuVXZ2tvz8/BzP8fX11dGjR8u63HKrtH48efKkRo4cqTFjxtCPTrhaX3bs2FGpqamO\nZfTljbmy3zw9PZWfn3/NSbrwX5GRkcrM/O/0r4ZhyGKxSLq0HWZlZbmrtArD19dX0qXt8eWXX1Zs\nbKxmzJjh9n6s1L8FPXr0UI8ePUpsW7ZsmTIyMtS/f399/PHHRWbxs9vt8vf3L6syy72r9eOhQ4c0\nfPhwjRo1SuHh4crOzqYfr6G0bfLPrpxZkr50zo3MyInSeXj8dyCY7dB5x48f16BBgxQdHa3OnTtr\n5syZjjZ39aPphvQXLFigjz/+WNKlvSxPT0/5+fmpSpUq+r//+z8ZhqFvvvlGTZs2dXOl5dt//vMf\nDR06VLNnz3Z8vwH96Dr05Y1hRk7Xa9CggWP0KSUlhe3QCadOndJzzz2nkSNHqnv37pLKRz+abuKd\nU6dOKS4uTrm5uSooKNCIESMUFham7777TtOmTVNBQYFatWqlYcOGubvUcm3gwIE6dOiQateuLelS\nQL399tv0401ITU1VUlKS5syZI0n05Q1gRk7XyMzM1PDhw7VmzRr9/PPPGj9+vPLy8hQYGKgpU6bI\n09PT3SWWa1OmTNHnn3+uwMBAx7KxY8dqypQpbu1H0wU+AABmZLohfQAAzIjABwDABAh8AABMgMAH\nAMAECHwAAEyAwAfgEB0drdjY2BLbvv76a4WEhOjMmTNXfX1+fr6Cg4OLzBoIoHwg8AE4dO7cWVu3\nblVOTk6xtg0bNqhVq1aqWbOmGyoDcLMIfAAOHTp0UF5enrZt21ZkeW5urr766is99dRTbqoMwM0i\n8AE41KxZU61atdIXX3xRZHlKSooKCwvVrl07ZWdn69VXX1WLFi0UEhKiDh06aPPmzSW+X0REhD78\n8EPH4x07dig4ONjx+NixYxowYIAaNWqktm3bas6cOcrLy7s1Hw4wOQIfQBGdOnXS119/rdzcXMey\nzz//XO3bt1fVqlU1ZcoUHTlyREuXLtWnn36qxo0ba+zYsUWe74zCwkK99NJLuv3227Vu3Tq98cYb\n+vLLLzV37lxXfyQAIvABXOGxxx5TQUGB4/u6c3Jy9NVXX6lz586SpKZNm+q1117Tgw8+qPvuu0/P\nPfec/vjjD/3+++/XtZ7t27frxIkTmjx5su6//3498sgjGj9+vJYvX67CwkKXfy7A7PjeSABFVKtW\nTY899pg2btyoRx99VFu3bpWvr6+aN28uSerWrZs2bdqkpKQkHT58WGlpaZKkgoKC61pPRkaG/vjj\nD4WFhTmWGYahixcv6vjx444vZgLgGgQ+gGI6d+6sUaNGKT8/Xxs2bFDHjh0d3+w1YsQIHThwQF26\ndFF0dLRq1aql6OjoEt/HYrEUeZyfn+/4f0FBgQIDAzV//vxir/vrX//qwk8DQCLwAZSgZcuW8vDw\n0M6dO7V161YlJiZKks6ePasNGzbogw8+UGhoqCQpOTlZ0qWj8ytVqVJFdrvd8fjo0aOO/9etW1fH\njh1TrVq1ZLVaJUm7d+/WypUr9cYbb9yyzwaYFefwARTj5eWlJ554QrNnz9bf/vY3hYSESJKqVq2q\natWqadOmTcrMzFRKSoqmTp0qSSVetNewYUOtW7dOP/30k1JTU7Vs2TJHW0REhO6880698sorSk9P\n1969ezVu3Dh5enrK29u7bD4oYCIEPoASde7cWQcPHnRcrCdJPj4+euONN/TFF1/oySef1IwZMzRo\n0CD95S9/0cGDB4u9x/Dhw+Xr66unn35aU6dO1dChQx1tXl5eWrBggQzDUK9evfTSSy+pefPmmjx5\ncpl8PsBsLEZJ43AAAKBS4QgfAAATIPABADABAh8AABMg8AEAMAECHwAAEyDwAQAwAQIfAAATIPAB\nADABAh8AABP4/4Yon3UfD/g3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11899b898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It appears to be a normal distribution that is centered around 0 but lager variance than original synthetic data set\n"
     ]
    }
   ],
   "source": [
    "## Generate synthetic data set using two row rank latent matrix (randomly generated) \n",
    "### -> It essentially makes the connections between latents matrix\n",
    "a = numpy.random.normal(0, 1, size=(250, 25)) # Generate random numbers for the first skinny matrix\n",
    "b = numpy.random.normal(0, 1, size=(25, 250)) # Generate random numbers for the second skinny matrix\n",
    "\n",
    "X = a.dot(b) # Build our real data matrix from the dot product of the two skinny matrices\n",
    "\n",
    "n = 35000\n",
    "i = numpy.random.randint(250, size=n)\n",
    "j = numpy.random.randint(250, size=n)\n",
    "X_values = X[i, j]\n",
    "print(\"Synthetic data set is generated. [row num : {}]\".format(X_values.shape[0]))\n",
    "\n",
    "# distribution of extracted values\n",
    "plt.title(\"Distribution of Data in Matrix\", fontsize=16)\n",
    "plt.ylabel(\"Count\", fontsize=14)\n",
    "plt.xlabel(\"Value\", fontsize=14)\n",
    "plt.hist(X_values, bins=100)\n",
    "plt.show()\n",
    "\n",
    "print('It appears to be a normal distribution that is centered around 0 but lager variance than original synthetic data set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T01:05:31.066867Z",
     "start_time": "2018-02-08T01:05:31.061948Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 10000\n",
    "X_train = gluon.data.DataLoader(gluon.data.ArrayDataset(i[:25000].astype('float32'), j[:25000].astype('float32'), X_values[:25000].astype('float32')),\n",
    "                                      batch_size=batch_size, shuffle=True)\n",
    "\n",
    "X_eval = gluon.data.DataLoader(gluon.data.ArrayDataset(i[25000:].astype('float32'), j[25000:].astype('float32'), X_values[25000:].astype('float32')),\n",
    "                                      batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T01:05:31.075200Z",
     "start_time": "2018-02-08T01:05:31.068541Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model initialization\n",
    "model = Vanilla_MF()\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx=ctx)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "criterion = gluon.loss.L2Loss()\n",
    "optimizer = gluon.Trainer(model.collect_params(), 'adam',\n",
    "                          {'learning_rate': learning_rate,\n",
    "                           'wd': 1e-5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T01:07:50.173931Z",
     "start_time": "2018-02-08T01:05:31.076708Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/20], (rmse) loss:13.0143, (rmse) val_loss:8.5653\n",
      "epoch [2/20], (rmse) loss:12.1355, (rmse) val_loss:8.1030\n",
      "epoch [3/20], (rmse) loss:11.4225, (rmse) val_loss:7.6768\n",
      "epoch [4/20], (rmse) loss:10.8096, (rmse) val_loss:7.2726\n",
      "epoch [5/20], (rmse) loss:10.1731, (rmse) val_loss:6.8977\n",
      "epoch [6/20], (rmse) loss:9.7191, (rmse) val_loss:6.5480\n",
      "epoch [7/20], (rmse) loss:9.4110, (rmse) val_loss:6.2270\n",
      "epoch [8/20], (rmse) loss:8.9542, (rmse) val_loss:5.9279\n",
      "epoch [9/20], (rmse) loss:8.4806, (rmse) val_loss:5.6597\n",
      "epoch [10/20], (rmse) loss:8.2105, (rmse) val_loss:5.4153\n",
      "epoch [11/20], (rmse) loss:7.8882, (rmse) val_loss:5.1952\n",
      "epoch [12/20], (rmse) loss:7.4368, (rmse) val_loss:4.9964\n",
      "epoch [13/20], (rmse) loss:7.2718, (rmse) val_loss:4.8134\n",
      "epoch [14/20], (rmse) loss:6.9272, (rmse) val_loss:4.6496\n",
      "epoch [15/20], (rmse) loss:6.8328, (rmse) val_loss:4.5039\n",
      "epoch [16/20], (rmse) loss:6.3506, (rmse) val_loss:4.3687\n",
      "epoch [17/20], (rmse) loss:6.2487, (rmse) val_loss:4.2481\n",
      "epoch [18/20], (rmse) loss:6.2101, (rmse) val_loss:4.1356\n",
      "epoch [19/20], (rmse) loss:5.9164, (rmse) val_loss:4.0330\n",
      "epoch [20/20], (rmse) loss:5.6928, (rmse) val_loss:3.9448\n"
     ]
    }
   ],
   "source": [
    "## Model Running (Training)\n",
    "model = run_epoch(model, X_train, X_eval, num_epochs= 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **2) Deep matrix Facotrization using synthetic data**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T07:24:16.068903Z",
     "start_time": "2018-02-08T07:24:16.064535Z"
    }
   },
   "source": [
    "- 기존 two latent matrix의 dot product 곱의 형태가 아닌 방식\n",
    "- Two latent layer(matrix)를 concat 후 Flatten하는 방식 사용\n",
    "- 이후 Regression 형태[0,1]의 결과값을 얻기 위해 Fully Connected Layer를 사용하여 Output 산출\n",
    "\n",
    "<br/>\n",
    "<img src=\"./images/deep_mf.png \" width=\"500\">\n",
    "<br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T01:07:50.206924Z",
     "start_time": "2018-02-08T01:07:50.192455Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Deep_MF(gluon.HybridBlock):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Deep_MF, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            # layers created in name_scope will inherit name space\n",
    "            # from parent layer.\n",
    "            self.user = nn.Embedding(input_dim=250, output_dim=25)\n",
    "            self.movie = nn.Embedding(input_dim=250, output_dim=25)\n",
    "            \n",
    "            \n",
    "            self.out = gluon.nn.HybridSequential('output_')\n",
    "            with self.out.name_scope(): # Output layers to make regression result\n",
    "                self.out.add(nn.Flatten())\n",
    "                self.out.add(nn.Dense(64, activation='relu'))\n",
    "                self.out.add(nn.Dense(1)) # the last output layer\n",
    "            \n",
    "    def hybrid_forward(self, F, user_i, movie_i):\n",
    "        # Here `F` can be either mx.nd or mx.sym, x is the input data,\n",
    "        user_i = self.user(user_i)\n",
    "        movie_i = self.movie(movie_i)\n",
    "        \n",
    "        z = F.concat(user_i, movie_i) # Two latent layer(matrix) concat\n",
    "        z = self.out(z) # concat output -> flatten -> dense layer(64 dim) -> output layer(1 dim)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T01:07:50.216954Z",
     "start_time": "2018-02-08T01:07:50.209249Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model initialization\n",
    "model = Deep_MF()\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx=ctx)\n",
    "\n",
    "learning_rate = 1e-2 # 기존 Vanilla 모형에 비해 running 속도가 빠름, 그러나 학습속도가 느리기에 learning_rate를 높임\n",
    "criterion = gluon.loss.L2Loss()\n",
    "optimizer = gluon.Trainer(model.collect_params(), 'adam',\n",
    "                          {'learning_rate': learning_rate,\n",
    "                           'wd': 1e-5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T01:09:36.446979Z",
     "start_time": "2018-02-08T01:07:50.218702Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/500], (rmse) loss:3.6049, (rmse) val_loss:2.2543\n",
      "epoch [2/500], (rmse) loss:3.5951, (rmse) val_loss:2.2545\n",
      "epoch [3/500], (rmse) loss:3.5844, (rmse) val_loss:2.2558\n",
      "epoch [4/500], (rmse) loss:3.5689, (rmse) val_loss:2.2610\n",
      "epoch [5/500], (rmse) loss:3.5543, (rmse) val_loss:2.2709\n",
      "epoch [6/500], (rmse) loss:3.5427, (rmse) val_loss:2.2720\n",
      "epoch [7/500], (rmse) loss:3.5259, (rmse) val_loss:2.2619\n",
      "epoch [8/500], (rmse) loss:3.5043, (rmse) val_loss:2.2499\n",
      "epoch [9/500], (rmse) loss:3.4769, (rmse) val_loss:2.2384\n",
      "epoch [10/500], (rmse) loss:3.4391, (rmse) val_loss:2.2286\n",
      "epoch [11/500], (rmse) loss:3.3903, (rmse) val_loss:2.2167\n",
      "epoch [12/500], (rmse) loss:3.3325, (rmse) val_loss:2.2044\n",
      "epoch [13/500], (rmse) loss:3.2741, (rmse) val_loss:2.1938\n",
      "epoch [14/500], (rmse) loss:3.2206, (rmse) val_loss:2.1853\n",
      "epoch [15/500], (rmse) loss:3.1733, (rmse) val_loss:2.1749\n",
      "epoch [16/500], (rmse) loss:3.1291, (rmse) val_loss:2.1595\n",
      "epoch [17/500], (rmse) loss:3.0888, (rmse) val_loss:2.1425\n",
      "epoch [18/500], (rmse) loss:3.0474, (rmse) val_loss:2.1221\n",
      "epoch [19/500], (rmse) loss:3.0070, (rmse) val_loss:2.1081\n",
      "epoch [20/500], (rmse) loss:2.9686, (rmse) val_loss:2.0985\n",
      "epoch [21/500], (rmse) loss:2.9322, (rmse) val_loss:2.0858\n",
      "epoch [22/500], (rmse) loss:2.8974, (rmse) val_loss:2.0782\n",
      "epoch [23/500], (rmse) loss:2.8597, (rmse) val_loss:2.0695\n",
      "epoch [24/500], (rmse) loss:2.8243, (rmse) val_loss:2.0615\n",
      "epoch [25/500], (rmse) loss:2.7890, (rmse) val_loss:2.0526\n",
      "epoch [26/500], (rmse) loss:2.7554, (rmse) val_loss:2.0411\n",
      "epoch [27/500], (rmse) loss:2.7222, (rmse) val_loss:2.0312\n",
      "epoch [28/500], (rmse) loss:2.6887, (rmse) val_loss:2.0256\n",
      "epoch [29/500], (rmse) loss:2.6570, (rmse) val_loss:2.0105\n",
      "epoch [30/500], (rmse) loss:2.6237, (rmse) val_loss:1.9998\n",
      "epoch [31/500], (rmse) loss:2.5897, (rmse) val_loss:1.9906\n",
      "epoch [32/500], (rmse) loss:2.5600, (rmse) val_loss:1.9798\n",
      "epoch [33/500], (rmse) loss:2.5271, (rmse) val_loss:1.9737\n",
      "epoch [34/500], (rmse) loss:2.4964, (rmse) val_loss:1.9681\n",
      "epoch [35/500], (rmse) loss:2.4699, (rmse) val_loss:1.9542\n",
      "epoch [36/500], (rmse) loss:2.4409, (rmse) val_loss:1.9470\n",
      "epoch [37/500], (rmse) loss:2.4137, (rmse) val_loss:1.9391\n",
      "epoch [38/500], (rmse) loss:2.3865, (rmse) val_loss:1.9320\n",
      "epoch [39/500], (rmse) loss:2.3582, (rmse) val_loss:1.9248\n",
      "epoch [40/500], (rmse) loss:2.3351, (rmse) val_loss:1.9133\n",
      "epoch [41/500], (rmse) loss:2.3078, (rmse) val_loss:1.9089\n",
      "epoch [42/500], (rmse) loss:2.2875, (rmse) val_loss:1.9006\n",
      "epoch [43/500], (rmse) loss:2.2603, (rmse) val_loss:1.8927\n",
      "epoch [44/500], (rmse) loss:2.2367, (rmse) val_loss:1.8836\n",
      "epoch [45/500], (rmse) loss:2.2140, (rmse) val_loss:1.8778\n",
      "epoch [46/500], (rmse) loss:2.1923, (rmse) val_loss:1.8710\n",
      "epoch [47/500], (rmse) loss:2.1725, (rmse) val_loss:1.8627\n",
      "epoch [48/500], (rmse) loss:2.1527, (rmse) val_loss:1.8570\n",
      "epoch [49/500], (rmse) loss:2.1330, (rmse) val_loss:1.8519\n",
      "epoch [50/500], (rmse) loss:2.1128, (rmse) val_loss:1.8454\n",
      "epoch [51/500], (rmse) loss:2.0946, (rmse) val_loss:1.8380\n",
      "epoch [52/500], (rmse) loss:2.0760, (rmse) val_loss:1.8336\n",
      "epoch [53/500], (rmse) loss:2.0595, (rmse) val_loss:1.8313\n",
      "epoch [54/500], (rmse) loss:2.0417, (rmse) val_loss:1.8261\n",
      "epoch [55/500], (rmse) loss:2.0248, (rmse) val_loss:1.8239\n",
      "epoch [56/500], (rmse) loss:2.0077, (rmse) val_loss:1.8178\n",
      "epoch [57/500], (rmse) loss:1.9906, (rmse) val_loss:1.8135\n",
      "epoch [58/500], (rmse) loss:1.9747, (rmse) val_loss:1.8097\n",
      "epoch [59/500], (rmse) loss:1.9574, (rmse) val_loss:1.8077\n",
      "epoch [60/500], (rmse) loss:1.9436, (rmse) val_loss:1.8009\n",
      "epoch [61/500], (rmse) loss:1.9303, (rmse) val_loss:1.8004\n",
      "epoch [62/500], (rmse) loss:1.9162, (rmse) val_loss:1.7963\n",
      "epoch [63/500], (rmse) loss:1.8992, (rmse) val_loss:1.7919\n",
      "epoch [64/500], (rmse) loss:1.8876, (rmse) val_loss:1.7865\n",
      "epoch [65/500], (rmse) loss:1.8738, (rmse) val_loss:1.7840\n",
      "epoch [66/500], (rmse) loss:1.8587, (rmse) val_loss:1.7803\n",
      "epoch [67/500], (rmse) loss:1.8470, (rmse) val_loss:1.7777\n",
      "epoch [68/500], (rmse) loss:1.8357, (rmse) val_loss:1.7773\n",
      "epoch [69/500], (rmse) loss:1.8205, (rmse) val_loss:1.7738\n",
      "epoch [70/500], (rmse) loss:1.8084, (rmse) val_loss:1.7710\n",
      "epoch [71/500], (rmse) loss:1.7942, (rmse) val_loss:1.7690\n",
      "epoch [72/500], (rmse) loss:1.7834, (rmse) val_loss:1.7656\n",
      "epoch [73/500], (rmse) loss:1.7709, (rmse) val_loss:1.7658\n",
      "epoch [74/500], (rmse) loss:1.7577, (rmse) val_loss:1.7659\n",
      "epoch [75/500], (rmse) loss:1.7471, (rmse) val_loss:1.7611\n",
      "epoch [76/500], (rmse) loss:1.7388, (rmse) val_loss:1.7589\n",
      "epoch [77/500], (rmse) loss:1.7253, (rmse) val_loss:1.7581\n",
      "epoch [78/500], (rmse) loss:1.7136, (rmse) val_loss:1.7570\n",
      "epoch [79/500], (rmse) loss:1.7040, (rmse) val_loss:1.7514\n",
      "epoch [80/500], (rmse) loss:1.6915, (rmse) val_loss:1.7503\n",
      "epoch [81/500], (rmse) loss:1.6815, (rmse) val_loss:1.7491\n",
      "epoch [82/500], (rmse) loss:1.6702, (rmse) val_loss:1.7457\n",
      "epoch [83/500], (rmse) loss:1.6575, (rmse) val_loss:1.7461\n",
      "epoch [84/500], (rmse) loss:1.6485, (rmse) val_loss:1.7446\n",
      "epoch [85/500], (rmse) loss:1.6386, (rmse) val_loss:1.7430\n",
      "epoch [86/500], (rmse) loss:1.6290, (rmse) val_loss:1.7400\n",
      "epoch [87/500], (rmse) loss:1.6171, (rmse) val_loss:1.7399\n",
      "epoch [88/500], (rmse) loss:1.6088, (rmse) val_loss:1.7377\n",
      "epoch [89/500], (rmse) loss:1.5987, (rmse) val_loss:1.7381\n",
      "epoch [90/500], (rmse) loss:1.5906, (rmse) val_loss:1.7374\n",
      "epoch [91/500], (rmse) loss:1.5812, (rmse) val_loss:1.7361\n",
      "epoch [92/500], (rmse) loss:1.5750, (rmse) val_loss:1.7378\n",
      "epoch [93/500], (rmse) loss:1.5648, (rmse) val_loss:1.7361\n",
      "epoch [94/500], (rmse) loss:1.5538, (rmse) val_loss:1.7338\n",
      "epoch [95/500], (rmse) loss:1.5442, (rmse) val_loss:1.7332\n",
      "epoch [96/500], (rmse) loss:1.5372, (rmse) val_loss:1.7317\n",
      "epoch [97/500], (rmse) loss:1.5282, (rmse) val_loss:1.7298\n",
      "epoch [98/500], (rmse) loss:1.5202, (rmse) val_loss:1.7309\n",
      "epoch [99/500], (rmse) loss:1.5112, (rmse) val_loss:1.7275\n",
      "epoch [100/500], (rmse) loss:1.5005, (rmse) val_loss:1.7293\n",
      "epoch [101/500], (rmse) loss:1.4945, (rmse) val_loss:1.7270\n",
      "epoch [102/500], (rmse) loss:1.4845, (rmse) val_loss:1.7262\n",
      "epoch [103/500], (rmse) loss:1.4769, (rmse) val_loss:1.7248\n",
      "epoch [104/500], (rmse) loss:1.4677, (rmse) val_loss:1.7265\n",
      "epoch [105/500], (rmse) loss:1.4621, (rmse) val_loss:1.7254\n",
      "epoch [106/500], (rmse) loss:1.4544, (rmse) val_loss:1.7252\n",
      "epoch [107/500], (rmse) loss:1.4476, (rmse) val_loss:1.7265\n",
      "epoch [108/500], (rmse) loss:1.4399, (rmse) val_loss:1.7237\n",
      "epoch [109/500], (rmse) loss:1.4320, (rmse) val_loss:1.7264\n",
      "epoch [110/500], (rmse) loss:1.4237, (rmse) val_loss:1.7243\n",
      "epoch [111/500], (rmse) loss:1.4172, (rmse) val_loss:1.7233\n",
      "epoch [112/500], (rmse) loss:1.4078, (rmse) val_loss:1.7268\n",
      "epoch [113/500], (rmse) loss:1.4018, (rmse) val_loss:1.7222\n",
      "epoch [114/500], (rmse) loss:1.3946, (rmse) val_loss:1.7220\n",
      "epoch [115/500], (rmse) loss:1.3882, (rmse) val_loss:1.7208\n",
      "epoch [116/500], (rmse) loss:1.3812, (rmse) val_loss:1.7231\n",
      "epoch [117/500], (rmse) loss:1.3755, (rmse) val_loss:1.7224\n",
      "epoch [118/500], (rmse) loss:1.3696, (rmse) val_loss:1.7234\n",
      "epoch [119/500], (rmse) loss:1.3625, (rmse) val_loss:1.7240\n",
      "epoch [120/500], (rmse) loss:1.3557, (rmse) val_loss:1.7233\n",
      "epoch [121/500], (rmse) loss:1.3495, (rmse) val_loss:1.7209\n",
      "epoch [122/500], (rmse) loss:1.3418, (rmse) val_loss:1.7228\n",
      "epoch [123/500], (rmse) loss:1.3357, (rmse) val_loss:1.7207\n",
      "epoch [124/500], (rmse) loss:1.3291, (rmse) val_loss:1.7240\n",
      "epoch [125/500], (rmse) loss:1.3243, (rmse) val_loss:1.7243\n",
      "epoch [126/500], (rmse) loss:1.3197, (rmse) val_loss:1.7216\n",
      "epoch [127/500], (rmse) loss:1.3128, (rmse) val_loss:1.7239\n",
      "epoch [128/500], (rmse) loss:1.3077, (rmse) val_loss:1.7235\n",
      "epoch [129/500], (rmse) loss:1.3004, (rmse) val_loss:1.7223\n",
      "epoch [130/500], (rmse) loss:1.2930, (rmse) val_loss:1.7241\n",
      "epoch [131/500], (rmse) loss:1.2869, (rmse) val_loss:1.7230\n",
      "epoch [132/500], (rmse) loss:1.2799, (rmse) val_loss:1.7236\n",
      "epoch [133/500], (rmse) loss:1.2773, (rmse) val_loss:1.7229\n",
      "epoch [134/500], (rmse) loss:1.2707, (rmse) val_loss:1.7259\n",
      "epoch [135/500], (rmse) loss:1.2651, (rmse) val_loss:1.7245\n",
      "epoch [136/500], (rmse) loss:1.2594, (rmse) val_loss:1.7267\n",
      "epoch [137/500], (rmse) loss:1.2549, (rmse) val_loss:1.7260\n",
      "epoch [138/500], (rmse) loss:1.2517, (rmse) val_loss:1.7290\n",
      "epoch [139/500], (rmse) loss:1.2471, (rmse) val_loss:1.7305\n",
      "epoch [140/500], (rmse) loss:1.2412, (rmse) val_loss:1.7278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [141/500], (rmse) loss:1.2359, (rmse) val_loss:1.7317\n",
      "epoch [142/500], (rmse) loss:1.2296, (rmse) val_loss:1.7277\n",
      "epoch [143/500], (rmse) loss:1.2254, (rmse) val_loss:1.7307\n",
      "epoch [144/500], (rmse) loss:1.2207, (rmse) val_loss:1.7314\n",
      "epoch [145/500], (rmse) loss:1.2174, (rmse) val_loss:1.7278\n",
      "epoch [146/500], (rmse) loss:1.2100, (rmse) val_loss:1.7326\n",
      "epoch [147/500], (rmse) loss:1.2063, (rmse) val_loss:1.7330\n",
      "epoch [148/500], (rmse) loss:1.2021, (rmse) val_loss:1.7332\n",
      "epoch [149/500], (rmse) loss:1.1979, (rmse) val_loss:1.7316\n",
      "epoch [150/500], (rmse) loss:1.1954, (rmse) val_loss:1.7335\n",
      "epoch [151/500], (rmse) loss:1.1883, (rmse) val_loss:1.7319\n",
      "epoch [152/500], (rmse) loss:1.1829, (rmse) val_loss:1.7331\n",
      "epoch [153/500], (rmse) loss:1.1798, (rmse) val_loss:1.7349\n",
      "epoch [154/500], (rmse) loss:1.1761, (rmse) val_loss:1.7351\n",
      "epoch [155/500], (rmse) loss:1.1738, (rmse) val_loss:1.7346\n",
      "epoch [156/500], (rmse) loss:1.1668, (rmse) val_loss:1.7387\n",
      "epoch [157/500], (rmse) loss:1.1635, (rmse) val_loss:1.7353\n",
      "epoch [158/500], (rmse) loss:1.1579, (rmse) val_loss:1.7374\n",
      "epoch [159/500], (rmse) loss:1.1545, (rmse) val_loss:1.7374\n",
      "epoch [160/500], (rmse) loss:1.1495, (rmse) val_loss:1.7357\n",
      "epoch [161/500], (rmse) loss:1.1485, (rmse) val_loss:1.7399\n",
      "epoch [162/500], (rmse) loss:1.1412, (rmse) val_loss:1.7391\n",
      "epoch [163/500], (rmse) loss:1.1380, (rmse) val_loss:1.7390\n",
      "epoch [164/500], (rmse) loss:1.1361, (rmse) val_loss:1.7452\n",
      "epoch [165/500], (rmse) loss:1.1315, (rmse) val_loss:1.7408\n",
      "epoch [166/500], (rmse) loss:1.1269, (rmse) val_loss:1.7426\n",
      "epoch [167/500], (rmse) loss:1.1249, (rmse) val_loss:1.7400\n",
      "epoch [168/500], (rmse) loss:1.1238, (rmse) val_loss:1.7417\n",
      "epoch [169/500], (rmse) loss:1.1188, (rmse) val_loss:1.7429\n",
      "epoch [170/500], (rmse) loss:1.1138, (rmse) val_loss:1.7445\n",
      "epoch [171/500], (rmse) loss:1.1116, (rmse) val_loss:1.7449\n",
      "epoch [172/500], (rmse) loss:1.1067, (rmse) val_loss:1.7427\n",
      "epoch [173/500], (rmse) loss:1.1019, (rmse) val_loss:1.7432\n",
      "epoch [174/500], (rmse) loss:1.1005, (rmse) val_loss:1.7434\n",
      "epoch [175/500], (rmse) loss:1.0957, (rmse) val_loss:1.7453\n",
      "epoch [176/500], (rmse) loss:1.0914, (rmse) val_loss:1.7436\n",
      "epoch [177/500], (rmse) loss:1.0870, (rmse) val_loss:1.7455\n",
      "epoch [178/500], (rmse) loss:1.0844, (rmse) val_loss:1.7441\n",
      "epoch [179/500], (rmse) loss:1.0819, (rmse) val_loss:1.7438\n",
      "epoch [180/500], (rmse) loss:1.0782, (rmse) val_loss:1.7445\n",
      "epoch [181/500], (rmse) loss:1.0771, (rmse) val_loss:1.7468\n",
      "epoch [182/500], (rmse) loss:1.0735, (rmse) val_loss:1.7459\n",
      "epoch [183/500], (rmse) loss:1.0714, (rmse) val_loss:1.7491\n",
      "epoch [184/500], (rmse) loss:1.0671, (rmse) val_loss:1.7477\n",
      "epoch [185/500], (rmse) loss:1.0643, (rmse) val_loss:1.7464\n",
      "epoch [186/500], (rmse) loss:1.0615, (rmse) val_loss:1.7459\n",
      "epoch [187/500], (rmse) loss:1.0596, (rmse) val_loss:1.7469\n",
      "epoch [188/500], (rmse) loss:1.0560, (rmse) val_loss:1.7455\n",
      "epoch [189/500], (rmse) loss:1.0519, (rmse) val_loss:1.7479\n",
      "epoch [190/500], (rmse) loss:1.0491, (rmse) val_loss:1.7474\n",
      "epoch [191/500], (rmse) loss:1.0518, (rmse) val_loss:1.7492\n",
      "epoch [192/500], (rmse) loss:1.0488, (rmse) val_loss:1.7490\n",
      "epoch [193/500], (rmse) loss:1.0449, (rmse) val_loss:1.7481\n",
      "epoch [194/500], (rmse) loss:1.0400, (rmse) val_loss:1.7482\n",
      "epoch [195/500], (rmse) loss:1.0364, (rmse) val_loss:1.7473\n",
      "epoch [196/500], (rmse) loss:1.0330, (rmse) val_loss:1.7471\n",
      "epoch [197/500], (rmse) loss:1.0322, (rmse) val_loss:1.7506\n",
      "epoch [198/500], (rmse) loss:1.0302, (rmse) val_loss:1.7500\n",
      "epoch [199/500], (rmse) loss:1.0255, (rmse) val_loss:1.7484\n",
      "epoch [200/500], (rmse) loss:1.0241, (rmse) val_loss:1.7487\n",
      "epoch [201/500], (rmse) loss:1.0197, (rmse) val_loss:1.7494\n",
      "epoch [202/500], (rmse) loss:1.0159, (rmse) val_loss:1.7507\n",
      "epoch [203/500], (rmse) loss:1.0181, (rmse) val_loss:1.7493\n",
      "epoch [204/500], (rmse) loss:1.0149, (rmse) val_loss:1.7512\n",
      "epoch [205/500], (rmse) loss:1.0120, (rmse) val_loss:1.7516\n",
      "epoch [206/500], (rmse) loss:1.0095, (rmse) val_loss:1.7520\n",
      "epoch [207/500], (rmse) loss:1.0053, (rmse) val_loss:1.7516\n",
      "epoch [208/500], (rmse) loss:1.0040, (rmse) val_loss:1.7517\n",
      "epoch [209/500], (rmse) loss:1.0036, (rmse) val_loss:1.7521\n",
      "epoch [210/500], (rmse) loss:1.0046, (rmse) val_loss:1.7504\n",
      "epoch [211/500], (rmse) loss:1.0008, (rmse) val_loss:1.7524\n",
      "epoch [212/500], (rmse) loss:1.0020, (rmse) val_loss:1.7518\n",
      "epoch [213/500], (rmse) loss:0.9986, (rmse) val_loss:1.7577\n",
      "epoch [214/500], (rmse) loss:1.0044, (rmse) val_loss:1.7524\n",
      "epoch [215/500], (rmse) loss:1.0023, (rmse) val_loss:1.7555\n",
      "epoch [216/500], (rmse) loss:0.9990, (rmse) val_loss:1.7507\n",
      "epoch [217/500], (rmse) loss:0.9906, (rmse) val_loss:1.7539\n",
      "epoch [218/500], (rmse) loss:0.9863, (rmse) val_loss:1.7512\n",
      "epoch [219/500], (rmse) loss:0.9822, (rmse) val_loss:1.7541\n",
      "epoch [220/500], (rmse) loss:0.9812, (rmse) val_loss:1.7504\n",
      "epoch [221/500], (rmse) loss:0.9769, (rmse) val_loss:1.7521\n",
      "epoch [222/500], (rmse) loss:0.9724, (rmse) val_loss:1.7507\n",
      "epoch [223/500], (rmse) loss:0.9720, (rmse) val_loss:1.7519\n",
      "epoch [224/500], (rmse) loss:0.9708, (rmse) val_loss:1.7546\n",
      "epoch [225/500], (rmse) loss:0.9689, (rmse) val_loss:1.7559\n",
      "epoch [226/500], (rmse) loss:0.9665, (rmse) val_loss:1.7544\n",
      "epoch [227/500], (rmse) loss:0.9646, (rmse) val_loss:1.7556\n",
      "epoch [228/500], (rmse) loss:0.9595, (rmse) val_loss:1.7560\n",
      "epoch [229/500], (rmse) loss:0.9594, (rmse) val_loss:1.7546\n",
      "epoch [230/500], (rmse) loss:0.9590, (rmse) val_loss:1.7564\n",
      "epoch [231/500], (rmse) loss:0.9546, (rmse) val_loss:1.7563\n",
      "epoch [232/500], (rmse) loss:0.9538, (rmse) val_loss:1.7560\n",
      "epoch [233/500], (rmse) loss:0.9523, (rmse) val_loss:1.7581\n",
      "epoch [234/500], (rmse) loss:0.9511, (rmse) val_loss:1.7564\n",
      "epoch [235/500], (rmse) loss:0.9481, (rmse) val_loss:1.7565\n",
      "epoch [236/500], (rmse) loss:0.9498, (rmse) val_loss:1.7563\n",
      "epoch [237/500], (rmse) loss:0.9480, (rmse) val_loss:1.7576\n",
      "epoch [238/500], (rmse) loss:0.9447, (rmse) val_loss:1.7581\n",
      "epoch [239/500], (rmse) loss:0.9427, (rmse) val_loss:1.7613\n",
      "epoch [240/500], (rmse) loss:0.9444, (rmse) val_loss:1.7564\n",
      "epoch [241/500], (rmse) loss:0.9391, (rmse) val_loss:1.7587\n",
      "epoch [242/500], (rmse) loss:0.9395, (rmse) val_loss:1.7606\n",
      "epoch [243/500], (rmse) loss:0.9352, (rmse) val_loss:1.7615\n",
      "epoch [244/500], (rmse) loss:0.9366, (rmse) val_loss:1.7599\n",
      "epoch [245/500], (rmse) loss:0.9332, (rmse) val_loss:1.7588\n",
      "epoch [246/500], (rmse) loss:0.9311, (rmse) val_loss:1.7606\n",
      "epoch [247/500], (rmse) loss:0.9285, (rmse) val_loss:1.7594\n",
      "epoch [248/500], (rmse) loss:0.9264, (rmse) val_loss:1.7609\n",
      "epoch [249/500], (rmse) loss:0.9242, (rmse) val_loss:1.7591\n",
      "epoch [250/500], (rmse) loss:0.9213, (rmse) val_loss:1.7581\n",
      "epoch [251/500], (rmse) loss:0.9213, (rmse) val_loss:1.7617\n",
      "epoch [252/500], (rmse) loss:0.9202, (rmse) val_loss:1.7597\n",
      "epoch [253/500], (rmse) loss:0.9198, (rmse) val_loss:1.7612\n",
      "epoch [254/500], (rmse) loss:0.9184, (rmse) val_loss:1.7634\n",
      "epoch [255/500], (rmse) loss:0.9197, (rmse) val_loss:1.7598\n",
      "epoch [256/500], (rmse) loss:0.9178, (rmse) val_loss:1.7637\n",
      "epoch [257/500], (rmse) loss:0.9153, (rmse) val_loss:1.7592\n",
      "epoch [258/500], (rmse) loss:0.9127, (rmse) val_loss:1.7638\n",
      "epoch [259/500], (rmse) loss:0.9099, (rmse) val_loss:1.7624\n",
      "epoch [260/500], (rmse) loss:0.9077, (rmse) val_loss:1.7637\n",
      "epoch [261/500], (rmse) loss:0.9094, (rmse) val_loss:1.7643\n",
      "epoch [262/500], (rmse) loss:0.9060, (rmse) val_loss:1.7633\n",
      "epoch [263/500], (rmse) loss:0.9042, (rmse) val_loss:1.7623\n",
      "epoch [264/500], (rmse) loss:0.9025, (rmse) val_loss:1.7669\n",
      "epoch [265/500], (rmse) loss:0.9008, (rmse) val_loss:1.7645\n",
      "epoch [266/500], (rmse) loss:0.9005, (rmse) val_loss:1.7644\n",
      "epoch [267/500], (rmse) loss:0.9006, (rmse) val_loss:1.7642\n",
      "epoch [268/500], (rmse) loss:0.8992, (rmse) val_loss:1.7671\n",
      "epoch [269/500], (rmse) loss:0.8966, (rmse) val_loss:1.7674\n",
      "epoch [270/500], (rmse) loss:0.8986, (rmse) val_loss:1.7684\n",
      "epoch [271/500], (rmse) loss:0.8939, (rmse) val_loss:1.7635\n",
      "epoch [272/500], (rmse) loss:0.8911, (rmse) val_loss:1.7678\n",
      "epoch [273/500], (rmse) loss:0.8923, (rmse) val_loss:1.7648\n",
      "epoch [274/500], (rmse) loss:0.8899, (rmse) val_loss:1.7681\n",
      "epoch [275/500], (rmse) loss:0.8885, (rmse) val_loss:1.7716\n",
      "epoch [276/500], (rmse) loss:0.8898, (rmse) val_loss:1.7671\n",
      "epoch [277/500], (rmse) loss:0.8877, (rmse) val_loss:1.7722\n",
      "epoch [278/500], (rmse) loss:0.8871, (rmse) val_loss:1.7689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [279/500], (rmse) loss:0.8819, (rmse) val_loss:1.7713\n",
      "epoch [280/500], (rmse) loss:0.8810, (rmse) val_loss:1.7716\n",
      "epoch [281/500], (rmse) loss:0.8800, (rmse) val_loss:1.7709\n",
      "epoch [282/500], (rmse) loss:0.8809, (rmse) val_loss:1.7727\n",
      "epoch [283/500], (rmse) loss:0.8742, (rmse) val_loss:1.7686\n",
      "epoch [284/500], (rmse) loss:0.8730, (rmse) val_loss:1.7709\n",
      "epoch [285/500], (rmse) loss:0.8714, (rmse) val_loss:1.7719\n",
      "epoch [286/500], (rmse) loss:0.8711, (rmse) val_loss:1.7724\n",
      "epoch [287/500], (rmse) loss:0.8728, (rmse) val_loss:1.7739\n",
      "epoch [288/500], (rmse) loss:0.8751, (rmse) val_loss:1.7697\n",
      "epoch [289/500], (rmse) loss:0.8732, (rmse) val_loss:1.7685\n",
      "epoch [290/500], (rmse) loss:0.8704, (rmse) val_loss:1.7734\n",
      "epoch [291/500], (rmse) loss:0.8690, (rmse) val_loss:1.7733\n",
      "epoch [292/500], (rmse) loss:0.8686, (rmse) val_loss:1.7736\n",
      "epoch [293/500], (rmse) loss:0.8679, (rmse) val_loss:1.7728\n",
      "epoch [294/500], (rmse) loss:0.8646, (rmse) val_loss:1.7697\n",
      "epoch [295/500], (rmse) loss:0.8631, (rmse) val_loss:1.7719\n",
      "epoch [296/500], (rmse) loss:0.8608, (rmse) val_loss:1.7703\n",
      "epoch [297/500], (rmse) loss:0.8599, (rmse) val_loss:1.7729\n",
      "epoch [298/500], (rmse) loss:0.8593, (rmse) val_loss:1.7734\n",
      "epoch [299/500], (rmse) loss:0.8603, (rmse) val_loss:1.7747\n",
      "epoch [300/500], (rmse) loss:0.8588, (rmse) val_loss:1.7748\n",
      "epoch [301/500], (rmse) loss:0.8567, (rmse) val_loss:1.7756\n",
      "epoch [302/500], (rmse) loss:0.8594, (rmse) val_loss:1.7769\n",
      "epoch [303/500], (rmse) loss:0.8565, (rmse) val_loss:1.7763\n",
      "epoch [304/500], (rmse) loss:0.8526, (rmse) val_loss:1.7764\n",
      "epoch [305/500], (rmse) loss:0.8522, (rmse) val_loss:1.7767\n",
      "epoch [306/500], (rmse) loss:0.8524, (rmse) val_loss:1.7774\n",
      "epoch [307/500], (rmse) loss:0.8504, (rmse) val_loss:1.7778\n",
      "epoch [308/500], (rmse) loss:0.8533, (rmse) val_loss:1.7806\n",
      "epoch [309/500], (rmse) loss:0.8564, (rmse) val_loss:1.7779\n",
      "epoch [310/500], (rmse) loss:0.8491, (rmse) val_loss:1.7779\n",
      "epoch [311/500], (rmse) loss:0.8450, (rmse) val_loss:1.7794\n",
      "epoch [312/500], (rmse) loss:0.8474, (rmse) val_loss:1.7796\n",
      "epoch [313/500], (rmse) loss:0.8472, (rmse) val_loss:1.7803\n",
      "epoch [314/500], (rmse) loss:0.8464, (rmse) val_loss:1.7798\n",
      "epoch [315/500], (rmse) loss:0.8445, (rmse) val_loss:1.7813\n",
      "epoch [316/500], (rmse) loss:0.8434, (rmse) val_loss:1.7796\n",
      "epoch [317/500], (rmse) loss:0.8430, (rmse) val_loss:1.7816\n",
      "epoch [318/500], (rmse) loss:0.8425, (rmse) val_loss:1.7796\n",
      "epoch [319/500], (rmse) loss:0.8420, (rmse) val_loss:1.7798\n",
      "epoch [320/500], (rmse) loss:0.8372, (rmse) val_loss:1.7809\n",
      "epoch [321/500], (rmse) loss:0.8349, (rmse) val_loss:1.7799\n",
      "epoch [322/500], (rmse) loss:0.8381, (rmse) val_loss:1.7800\n",
      "epoch [323/500], (rmse) loss:0.8337, (rmse) val_loss:1.7802\n",
      "epoch [324/500], (rmse) loss:0.8333, (rmse) val_loss:1.7848\n",
      "epoch [325/500], (rmse) loss:0.8358, (rmse) val_loss:1.7826\n",
      "epoch [326/500], (rmse) loss:0.8332, (rmse) val_loss:1.7804\n",
      "epoch [327/500], (rmse) loss:0.8333, (rmse) val_loss:1.7824\n",
      "epoch [328/500], (rmse) loss:0.8338, (rmse) val_loss:1.7795\n",
      "epoch [329/500], (rmse) loss:0.8299, (rmse) val_loss:1.7821\n",
      "epoch [330/500], (rmse) loss:0.8273, (rmse) val_loss:1.7834\n",
      "epoch [331/500], (rmse) loss:0.8280, (rmse) val_loss:1.7839\n",
      "epoch [332/500], (rmse) loss:0.8303, (rmse) val_loss:1.7841\n",
      "epoch [333/500], (rmse) loss:0.8300, (rmse) val_loss:1.7853\n",
      "epoch [334/500], (rmse) loss:0.8277, (rmse) val_loss:1.7860\n",
      "epoch [335/500], (rmse) loss:0.8260, (rmse) val_loss:1.7859\n",
      "epoch [336/500], (rmse) loss:0.8239, (rmse) val_loss:1.7861\n",
      "epoch [337/500], (rmse) loss:0.8227, (rmse) val_loss:1.7855\n",
      "epoch [338/500], (rmse) loss:0.8233, (rmse) val_loss:1.7843\n",
      "epoch [339/500], (rmse) loss:0.8171, (rmse) val_loss:1.7838\n",
      "epoch [340/500], (rmse) loss:0.8177, (rmse) val_loss:1.7873\n",
      "epoch [341/500], (rmse) loss:0.8164, (rmse) val_loss:1.7841\n",
      "epoch [342/500], (rmse) loss:0.8172, (rmse) val_loss:1.7880\n",
      "epoch [343/500], (rmse) loss:0.8162, (rmse) val_loss:1.7857\n",
      "epoch [344/500], (rmse) loss:0.8152, (rmse) val_loss:1.7869\n",
      "epoch [345/500], (rmse) loss:0.8171, (rmse) val_loss:1.7852\n",
      "epoch [346/500], (rmse) loss:0.8139, (rmse) val_loss:1.7848\n",
      "epoch [347/500], (rmse) loss:0.8124, (rmse) val_loss:1.7850\n",
      "epoch [348/500], (rmse) loss:0.8124, (rmse) val_loss:1.7873\n",
      "epoch [349/500], (rmse) loss:0.8112, (rmse) val_loss:1.7863\n",
      "epoch [350/500], (rmse) loss:0.8123, (rmse) val_loss:1.7892\n",
      "epoch [351/500], (rmse) loss:0.8100, (rmse) val_loss:1.7901\n",
      "epoch [352/500], (rmse) loss:0.8090, (rmse) val_loss:1.7891\n",
      "epoch [353/500], (rmse) loss:0.8047, (rmse) val_loss:1.7894\n",
      "epoch [354/500], (rmse) loss:0.8060, (rmse) val_loss:1.7889\n",
      "epoch [355/500], (rmse) loss:0.8057, (rmse) val_loss:1.7876\n",
      "epoch [356/500], (rmse) loss:0.8037, (rmse) val_loss:1.7916\n",
      "epoch [357/500], (rmse) loss:0.8050, (rmse) val_loss:1.7926\n",
      "epoch [358/500], (rmse) loss:0.8056, (rmse) val_loss:1.7902\n",
      "epoch [359/500], (rmse) loss:0.8034, (rmse) val_loss:1.7928\n",
      "epoch [360/500], (rmse) loss:0.8024, (rmse) val_loss:1.7941\n",
      "epoch [361/500], (rmse) loss:0.8033, (rmse) val_loss:1.7922\n",
      "epoch [362/500], (rmse) loss:0.8027, (rmse) val_loss:1.7919\n",
      "epoch [363/500], (rmse) loss:0.7980, (rmse) val_loss:1.7911\n",
      "epoch [364/500], (rmse) loss:0.7979, (rmse) val_loss:1.7890\n",
      "epoch [365/500], (rmse) loss:0.7999, (rmse) val_loss:1.7932\n",
      "epoch [366/500], (rmse) loss:0.7976, (rmse) val_loss:1.7910\n",
      "epoch [367/500], (rmse) loss:0.7986, (rmse) val_loss:1.7908\n",
      "epoch [368/500], (rmse) loss:0.7962, (rmse) val_loss:1.7922\n",
      "epoch [369/500], (rmse) loss:0.7943, (rmse) val_loss:1.7924\n",
      "epoch [370/500], (rmse) loss:0.7984, (rmse) val_loss:1.7928\n",
      "epoch [371/500], (rmse) loss:0.7946, (rmse) val_loss:1.7954\n",
      "epoch [372/500], (rmse) loss:0.7964, (rmse) val_loss:1.7927\n",
      "epoch [373/500], (rmse) loss:0.7966, (rmse) val_loss:1.7935\n",
      "epoch [374/500], (rmse) loss:0.7915, (rmse) val_loss:1.7946\n",
      "epoch [375/500], (rmse) loss:0.7891, (rmse) val_loss:1.7924\n",
      "epoch [376/500], (rmse) loss:0.7880, (rmse) val_loss:1.7949\n",
      "epoch [377/500], (rmse) loss:0.7895, (rmse) val_loss:1.7929\n",
      "epoch [378/500], (rmse) loss:0.7895, (rmse) val_loss:1.7938\n",
      "epoch [379/500], (rmse) loss:0.7931, (rmse) val_loss:1.7937\n",
      "epoch [380/500], (rmse) loss:0.7886, (rmse) val_loss:1.7957\n",
      "epoch [381/500], (rmse) loss:0.7897, (rmse) val_loss:1.7959\n",
      "epoch [382/500], (rmse) loss:0.7880, (rmse) val_loss:1.7968\n",
      "epoch [383/500], (rmse) loss:0.7870, (rmse) val_loss:1.7971\n",
      "epoch [384/500], (rmse) loss:0.7831, (rmse) val_loss:1.7970\n",
      "epoch [385/500], (rmse) loss:0.7850, (rmse) val_loss:1.7976\n",
      "epoch [386/500], (rmse) loss:0.7868, (rmse) val_loss:1.8003\n",
      "epoch [387/500], (rmse) loss:0.7894, (rmse) val_loss:1.7975\n",
      "epoch [388/500], (rmse) loss:0.7878, (rmse) val_loss:1.7966\n",
      "epoch [389/500], (rmse) loss:0.7865, (rmse) val_loss:1.7975\n",
      "epoch [390/500], (rmse) loss:0.7851, (rmse) val_loss:1.7996\n",
      "epoch [391/500], (rmse) loss:0.7879, (rmse) val_loss:1.8020\n",
      "epoch [392/500], (rmse) loss:0.7897, (rmse) val_loss:1.7988\n",
      "epoch [393/500], (rmse) loss:0.7852, (rmse) val_loss:1.7977\n",
      "epoch [394/500], (rmse) loss:0.7821, (rmse) val_loss:1.7977\n",
      "epoch [395/500], (rmse) loss:0.7808, (rmse) val_loss:1.7978\n",
      "epoch [396/500], (rmse) loss:0.7802, (rmse) val_loss:1.8000\n",
      "epoch [397/500], (rmse) loss:0.7824, (rmse) val_loss:1.7993\n",
      "epoch [398/500], (rmse) loss:0.7814, (rmse) val_loss:1.7999\n",
      "epoch [399/500], (rmse) loss:0.7812, (rmse) val_loss:1.8004\n",
      "epoch [400/500], (rmse) loss:0.7796, (rmse) val_loss:1.8012\n",
      "epoch [401/500], (rmse) loss:0.7766, (rmse) val_loss:1.8008\n",
      "epoch [402/500], (rmse) loss:0.7770, (rmse) val_loss:1.7999\n",
      "epoch [403/500], (rmse) loss:0.7779, (rmse) val_loss:1.7998\n",
      "epoch [404/500], (rmse) loss:0.7750, (rmse) val_loss:1.7995\n",
      "epoch [405/500], (rmse) loss:0.7732, (rmse) val_loss:1.7997\n",
      "epoch [406/500], (rmse) loss:0.7722, (rmse) val_loss:1.8010\n",
      "epoch [407/500], (rmse) loss:0.7700, (rmse) val_loss:1.7996\n",
      "epoch [408/500], (rmse) loss:0.7711, (rmse) val_loss:1.8008\n",
      "epoch [409/500], (rmse) loss:0.7702, (rmse) val_loss:1.7986\n",
      "epoch [410/500], (rmse) loss:0.7697, (rmse) val_loss:1.7999\n",
      "epoch [411/500], (rmse) loss:0.7680, (rmse) val_loss:1.8004\n",
      "epoch [412/500], (rmse) loss:0.7698, (rmse) val_loss:1.8032\n",
      "epoch [413/500], (rmse) loss:0.7711, (rmse) val_loss:1.8056\n",
      "epoch [414/500], (rmse) loss:0.7721, (rmse) val_loss:1.8048\n",
      "epoch [415/500], (rmse) loss:0.7702, (rmse) val_loss:1.8075\n",
      "epoch [416/500], (rmse) loss:0.7715, (rmse) val_loss:1.8042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [417/500], (rmse) loss:0.7720, (rmse) val_loss:1.8053\n",
      "epoch [418/500], (rmse) loss:0.7695, (rmse) val_loss:1.8039\n",
      "epoch [419/500], (rmse) loss:0.7678, (rmse) val_loss:1.8047\n",
      "epoch [420/500], (rmse) loss:0.7719, (rmse) val_loss:1.8052\n",
      "epoch [421/500], (rmse) loss:0.7732, (rmse) val_loss:1.8046\n",
      "epoch [422/500], (rmse) loss:0.7698, (rmse) val_loss:1.8071\n",
      "epoch [423/500], (rmse) loss:0.7674, (rmse) val_loss:1.8073\n",
      "epoch [424/500], (rmse) loss:0.7671, (rmse) val_loss:1.8062\n",
      "epoch [425/500], (rmse) loss:0.7627, (rmse) val_loss:1.8081\n",
      "epoch [426/500], (rmse) loss:0.7623, (rmse) val_loss:1.8097\n",
      "epoch [427/500], (rmse) loss:0.7613, (rmse) val_loss:1.8071\n",
      "epoch [428/500], (rmse) loss:0.7588, (rmse) val_loss:1.8079\n",
      "epoch [429/500], (rmse) loss:0.7635, (rmse) val_loss:1.8104\n",
      "epoch [430/500], (rmse) loss:0.7645, (rmse) val_loss:1.8063\n",
      "epoch [431/500], (rmse) loss:0.7615, (rmse) val_loss:1.8090\n",
      "epoch [432/500], (rmse) loss:0.7612, (rmse) val_loss:1.8089\n",
      "epoch [433/500], (rmse) loss:0.7630, (rmse) val_loss:1.8113\n",
      "epoch [434/500], (rmse) loss:0.7594, (rmse) val_loss:1.8077\n",
      "epoch [435/500], (rmse) loss:0.7647, (rmse) val_loss:1.8110\n",
      "epoch [436/500], (rmse) loss:0.7591, (rmse) val_loss:1.8088\n",
      "epoch [437/500], (rmse) loss:0.7592, (rmse) val_loss:1.8108\n",
      "epoch [438/500], (rmse) loss:0.7578, (rmse) val_loss:1.8095\n",
      "epoch [439/500], (rmse) loss:0.7605, (rmse) val_loss:1.8099\n",
      "epoch [440/500], (rmse) loss:0.7591, (rmse) val_loss:1.8075\n",
      "epoch [441/500], (rmse) loss:0.7605, (rmse) val_loss:1.8115\n",
      "epoch [442/500], (rmse) loss:0.7605, (rmse) val_loss:1.8093\n",
      "epoch [443/500], (rmse) loss:0.7577, (rmse) val_loss:1.8130\n",
      "epoch [444/500], (rmse) loss:0.7604, (rmse) val_loss:1.8113\n",
      "epoch [445/500], (rmse) loss:0.7537, (rmse) val_loss:1.8104\n",
      "epoch [446/500], (rmse) loss:0.7529, (rmse) val_loss:1.8131\n",
      "epoch [447/500], (rmse) loss:0.7552, (rmse) val_loss:1.8102\n",
      "epoch [448/500], (rmse) loss:0.7510, (rmse) val_loss:1.8123\n",
      "epoch [449/500], (rmse) loss:0.7510, (rmse) val_loss:1.8087\n",
      "epoch [450/500], (rmse) loss:0.7494, (rmse) val_loss:1.8117\n",
      "epoch [451/500], (rmse) loss:0.7511, (rmse) val_loss:1.8118\n",
      "epoch [452/500], (rmse) loss:0.7505, (rmse) val_loss:1.8125\n",
      "epoch [453/500], (rmse) loss:0.7486, (rmse) val_loss:1.8122\n",
      "epoch [454/500], (rmse) loss:0.7499, (rmse) val_loss:1.8110\n",
      "epoch [455/500], (rmse) loss:0.7502, (rmse) val_loss:1.8108\n",
      "epoch [456/500], (rmse) loss:0.7456, (rmse) val_loss:1.8128\n",
      "epoch [457/500], (rmse) loss:0.7459, (rmse) val_loss:1.8122\n",
      "epoch [458/500], (rmse) loss:0.7432, (rmse) val_loss:1.8125\n",
      "epoch [459/500], (rmse) loss:0.7429, (rmse) val_loss:1.8123\n",
      "epoch [460/500], (rmse) loss:0.7477, (rmse) val_loss:1.8131\n",
      "epoch [461/500], (rmse) loss:0.7440, (rmse) val_loss:1.8124\n",
      "epoch [462/500], (rmse) loss:0.7421, (rmse) val_loss:1.8155\n",
      "epoch [463/500], (rmse) loss:0.7414, (rmse) val_loss:1.8128\n",
      "epoch [464/500], (rmse) loss:0.7457, (rmse) val_loss:1.8142\n",
      "epoch [465/500], (rmse) loss:0.7456, (rmse) val_loss:1.8144\n",
      "epoch [466/500], (rmse) loss:0.7448, (rmse) val_loss:1.8139\n",
      "epoch [467/500], (rmse) loss:0.7434, (rmse) val_loss:1.8136\n",
      "epoch [468/500], (rmse) loss:0.7424, (rmse) val_loss:1.8164\n",
      "epoch [469/500], (rmse) loss:0.7417, (rmse) val_loss:1.8154\n",
      "epoch [470/500], (rmse) loss:0.7398, (rmse) val_loss:1.8196\n",
      "epoch [471/500], (rmse) loss:0.7450, (rmse) val_loss:1.8159\n",
      "epoch [472/500], (rmse) loss:0.7443, (rmse) val_loss:1.8176\n",
      "epoch [473/500], (rmse) loss:0.7390, (rmse) val_loss:1.8151\n",
      "epoch [474/500], (rmse) loss:0.7374, (rmse) val_loss:1.8171\n",
      "epoch [475/500], (rmse) loss:0.7384, (rmse) val_loss:1.8171\n",
      "epoch [476/500], (rmse) loss:0.7401, (rmse) val_loss:1.8173\n",
      "epoch [477/500], (rmse) loss:0.7403, (rmse) val_loss:1.8156\n",
      "epoch [478/500], (rmse) loss:0.7396, (rmse) val_loss:1.8184\n",
      "epoch [479/500], (rmse) loss:0.7379, (rmse) val_loss:1.8187\n",
      "epoch [480/500], (rmse) loss:0.7400, (rmse) val_loss:1.8181\n",
      "epoch [481/500], (rmse) loss:0.7376, (rmse) val_loss:1.8183\n",
      "epoch [482/500], (rmse) loss:0.7375, (rmse) val_loss:1.8174\n",
      "epoch [483/500], (rmse) loss:0.7375, (rmse) val_loss:1.8165\n",
      "epoch [484/500], (rmse) loss:0.7377, (rmse) val_loss:1.8222\n",
      "epoch [485/500], (rmse) loss:0.7423, (rmse) val_loss:1.8203\n",
      "epoch [486/500], (rmse) loss:0.7452, (rmse) val_loss:1.8234\n",
      "epoch [487/500], (rmse) loss:0.7431, (rmse) val_loss:1.8182\n",
      "epoch [488/500], (rmse) loss:0.7414, (rmse) val_loss:1.8196\n",
      "epoch [489/500], (rmse) loss:0.7407, (rmse) val_loss:1.8207\n",
      "epoch [490/500], (rmse) loss:0.7415, (rmse) val_loss:1.8214\n",
      "epoch [491/500], (rmse) loss:0.7389, (rmse) val_loss:1.8208\n",
      "epoch [492/500], (rmse) loss:0.7350, (rmse) val_loss:1.8204\n",
      "epoch [493/500], (rmse) loss:0.7343, (rmse) val_loss:1.8222\n",
      "epoch [494/500], (rmse) loss:0.7377, (rmse) val_loss:1.8244\n",
      "epoch [495/500], (rmse) loss:0.7381, (rmse) val_loss:1.8198\n",
      "epoch [496/500], (rmse) loss:0.7323, (rmse) val_loss:1.8198\n",
      "epoch [497/500], (rmse) loss:0.7318, (rmse) val_loss:1.8195\n",
      "epoch [498/500], (rmse) loss:0.7310, (rmse) val_loss:1.8199\n",
      "epoch [499/500], (rmse) loss:0.7301, (rmse) val_loss:1.8181\n",
      "epoch [500/500], (rmse) loss:0.7295, (rmse) val_loss:1.8194\n"
     ]
    }
   ],
   "source": [
    "## Model Running (Training)\n",
    "model = run_epoch(model, X_train, X_eval, num_epochs= 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- We can see that while the neural network is clearly training due to the validation mse decreasing after each epoch, it is not decreasing as quickly as normal matrix factorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Movielens 0.1M\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Randomly Generated Data가 아닌 실제 데이터를 활용함\n",
    "- Recommendation계의 MNIST 데이터로 Movielnes Data Set을 활용\n",
    "\n",
    "(참고 : https://grouplens.org/datasets/movielens/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T02:00:24.830463Z",
     "start_time": "2018-02-08T02:00:24.825731Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/Juyeong/Documents/gluon_study/Recommendation'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T02:00:25.191109Z",
     "start_time": "2018-02-08T02:00:25.077785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100004"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now extract the data since we know we have it at this point\n",
    "with zipfile.ZipFile(\"ml-latest-small.zip\", \"r\") as f:\n",
    "    f.extractall(\"./\")\n",
    "\n",
    "# Now load it up using a pandas dataframe\n",
    "data = pandas.read_csv('./ml-latest-small/ratings.csv', sep=',', usecols=(0, 1, 2))\n",
    "data.head()\n",
    "# ratings from 0.5 to 5. by 0.5 inc\n",
    "len(data)\n",
    "\n",
    "#Since only ~20M of these are present, ~99.5% of the matrix is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T02:00:25.200867Z",
     "start_time": "2018-02-08T02:00:25.192995Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min value of rating : 0.5\n",
      "Max value of rating : 5.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Min value of rating : {}\".format(data.rating.min(axis=0)))\n",
    "print(\"Max value of rating : {}\".format(data.rating.max(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T02:00:25.456820Z",
     "start_time": "2018-02-08T02:00:25.202560Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgEAAAFsCAYAAABVZGp4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAZpwvH94hjVpAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x106322cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(data['rating'])\n",
    "plt.xlabel(\"Rating\", fontsize=14)\n",
    "plt.ylabel(\"Count\", fontsize=14)\n",
    "plt.title(\"Distribution of Ratings in MovieLens 20M\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T02:00:25.953800Z",
     "start_time": "2018-02-08T02:00:25.937812Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user id min/max:  1 671\n",
      "# unique users:  671\n",
      "\n",
      "movie id min/max:  1 163949\n",
      "# unique movies:  9066\n"
     ]
    }
   ],
   "source": [
    "print(\"user id min/max: \", data['userId'].min(), data['userId'].max())\n",
    "print(\"# unique users: \", numpy.unique(data['userId']).shape[0])\n",
    "print(\"\")\n",
    "print(\"movie id min/max: \", data['movieId'].min(), data['movieId'].max())\n",
    "print(\"# unique movies: \", numpy.unique(data['movieId']).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T02:00:26.228084Z",
     "start_time": "2018-02-08T02:00:26.219490Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_users, n_movies = numpy.unique(data['userId']).shape[0], numpy.unique(data['movieId']).shape[0]\n",
    "batch_size = 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T02:00:26.635175Z",
     "start_time": "2018-02-08T02:00:26.619199Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 90000\n",
    "\n",
    "data = data.sample(frac=1).reset_index(drop=True) # Shuffle the data in place row-wise\n",
    "\n",
    "# Use the first 19M samples to train the model\n",
    "train_users = data['userId'].values[:n] - 1 # Offset by 1 so that the IDs start at 0\n",
    "train_movies = data['movieId'].values[:n] - 1 # Offset by 1 so that the IDs start at 0\n",
    "train_ratings = data['rating'].values[:n]\n",
    "\n",
    "# Use the remaining ~1M samples for validation of the model\n",
    "valid_users = data['userId'].values[n:] - 1 # Offset by 1 so that the IDs start at 0\n",
    "valid_movies = data['movieId'].values[n:] - 1 # Offset by 1 so that the IDs start at 0\n",
    "valid_ratings = data['rating'].values[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T02:00:27.072807Z",
     "start_time": "2018-02-08T02:00:27.066921Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = gluon.data.DataLoader(gluon.data.ArrayDataset(train_users.astype('float32'), train_movies.astype('float32'), train_ratings.astype('float32')),\n",
    "                                      batch_size=batch_size, shuffle=True)\n",
    "\n",
    "X_eval = gluon.data.DataLoader(gluon.data.ArrayDataset(valid_users.astype('float32'), valid_movies.astype('float32'), valid_ratings.astype('float32')),\n",
    "                                      batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T02:00:27.721786Z",
     "start_time": "2018-02-08T02:00:27.713003Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Vanilla_MF_ml(gluon.HybridBlock):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Vanilla_MF_ml, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            # layers created in name_scope will inherit name space\n",
    "            # from parent layer.\n",
    "            self.user = nn.Embedding(input_dim=n_users, output_dim=25)\n",
    "            self.movie = nn.Embedding(input_dim=n_movies, output_dim=25)\n",
    "            \n",
    "    def hybrid_forward(self, F, user_i, movie_i):\n",
    "        # Here `F` can be either mx.nd or mx.sym, x is the input data,\n",
    "        user_i = self.user(user_i)\n",
    "        movie_i = self.movie(movie_i)\n",
    "        z = F.sum(F.dot(user_i, movie_i, transpose_b=True), axis = 1)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T02:00:28.361420Z",
     "start_time": "2018-02-08T02:00:28.353205Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model initialization\n",
    "model = Vanilla_MF_ml()\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx=ctx)\n",
    "\n",
    "\n",
    "learning_rate = 1e-4\n",
    "criterion = gluon.loss.L2Loss()\n",
    "optimizer = gluon.Trainer(model.collect_params(), 'adam',\n",
    "                          {'learning_rate': learning_rate,\n",
    "                           'wd': 1e-5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T02:12:21.471762Z",
     "start_time": "2018-02-08T02:00:28.752239Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/20], (rmse) loss:7.6300, (rmse) val_loss:1.6164\n",
      "epoch [2/20], (rmse) loss:6.1213, (rmse) val_loss:1.3527\n",
      "epoch [3/20], (rmse) loss:4.9149, (rmse) val_loss:1.1550\n",
      "epoch [4/20], (rmse) loss:3.9896, (rmse) val_loss:1.0140\n",
      "epoch [5/20], (rmse) loss:3.3258, (rmse) val_loss:0.9242\n",
      "epoch [6/20], (rmse) loss:2.8980, (rmse) val_loss:0.8739\n",
      "epoch [7/20], (rmse) loss:2.6638, (rmse) val_loss:0.8484\n",
      "epoch [8/20], (rmse) loss:2.5382, (rmse) val_loss:0.8369\n",
      "epoch [9/20], (rmse) loss:2.4860, (rmse) val_loss:0.8313\n",
      "epoch [10/20], (rmse) loss:2.4569, (rmse) val_loss:0.8260\n",
      "epoch [11/20], (rmse) loss:2.4403, (rmse) val_loss:0.8204\n",
      "epoch [12/20], (rmse) loss:2.4198, (rmse) val_loss:0.8138\n",
      "epoch [13/20], (rmse) loss:2.3995, (rmse) val_loss:0.8067\n",
      "epoch [14/20], (rmse) loss:2.3748, (rmse) val_loss:0.7993\n",
      "epoch [15/20], (rmse) loss:2.3450, (rmse) val_loss:0.7917\n",
      "epoch [16/20], (rmse) loss:2.3237, (rmse) val_loss:0.7842\n",
      "epoch [17/20], (rmse) loss:2.2948, (rmse) val_loss:0.7770\n",
      "epoch [18/20], (rmse) loss:2.2641, (rmse) val_loss:0.7682\n",
      "epoch [19/20], (rmse) loss:2.2359, (rmse) val_loss:0.7590\n",
      "epoch [20/20], (rmse) loss:2.2025, (rmse) val_loss:0.7499\n"
     ]
    }
   ],
   "source": [
    "## Model Running (Training)\n",
    "model = run_epoch(model, X_train, X_eval, num_epochs= 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T02:12:21.491537Z",
     "start_time": "2018-02-08T02:12:21.473390Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Deep_MF_ml(gluon.HybridBlock):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Deep_MF_ml, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            # layers created in name_scope will inherit name space\n",
    "            # from parent layer.\n",
    "            self.user = nn.Embedding(input_dim=n_users, output_dim=25)\n",
    "            self.movie = nn.Embedding(input_dim=n_movies, output_dim=25)\n",
    "            \n",
    "            \n",
    "            self.out = gluon.nn.HybridSequential('output_')\n",
    "            with self.out.name_scope():\n",
    "                self.out.add(nn.Flatten())\n",
    "                self.out.add(nn.Dense(64, activation='relu')) # encoding & latent layer\n",
    "                self.out.add(gluon.nn.BatchNorm(axis=1, center=True, scale=True))\n",
    "                self.out.add(nn.Dense(64, activation='relu')) # encoding & latent layer\n",
    "                self.out.add(gluon.nn.BatchNorm(axis=1, center=True, scale=True))\n",
    "                self.out.add(nn.Dense(1)) # encoding & latent layer\n",
    "            \n",
    "    def hybrid_forward(self, F, user_i, movie_i):\n",
    "        # Here `F` can be either mx.nd or mx.sym, x is the input data,\n",
    "        user_i = self.user(user_i)\n",
    "        movie_i = self.movie(movie_i)\n",
    "        \n",
    "        z = F.concat(user_i, movie_i)\n",
    "        z = self.out(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T02:12:21.504023Z",
     "start_time": "2018-02-08T02:12:21.493167Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model initialization\n",
    "model = Deep_MF_ml()\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx=ctx)\n",
    "\n",
    "\n",
    "learning_rate = 1e-2\n",
    "criterion = gluon.loss.L2Loss()\n",
    "optimizer = gluon.Trainer(model.collect_params(), 'adam',\n",
    "                          {'learning_rate': learning_rate,\n",
    "                           'wd': 1e-5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T02:14:52.815270Z",
     "start_time": "2018-02-08T02:12:21.505846Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/100], (rmse) loss:2.4970, (rmse) val_loss:0.7345\n",
      "epoch [2/100], (rmse) loss:1.9622, (rmse) val_loss:0.5358\n",
      "epoch [3/100], (rmse) loss:1.2644, (rmse) val_loss:0.2855\n",
      "epoch [4/100], (rmse) loss:0.6378, (rmse) val_loss:0.2677\n",
      "epoch [5/100], (rmse) loss:0.7770, (rmse) val_loss:0.2963\n",
      "epoch [6/100], (rmse) loss:0.6614, (rmse) val_loss:0.2288\n",
      "epoch [7/100], (rmse) loss:0.5040, (rmse) val_loss:0.2361\n",
      "epoch [8/100], (rmse) loss:0.5329, (rmse) val_loss:0.2397\n",
      "epoch [9/100], (rmse) loss:0.4914, (rmse) val_loss:0.2305\n",
      "epoch [10/100], (rmse) loss:0.4547, (rmse) val_loss:0.2336\n",
      "epoch [11/100], (rmse) loss:0.4528, (rmse) val_loss:0.2346\n",
      "epoch [12/100], (rmse) loss:0.4353, (rmse) val_loss:0.2335\n",
      "epoch [13/100], (rmse) loss:0.4269, (rmse) val_loss:0.2357\n",
      "epoch [14/100], (rmse) loss:0.4214, (rmse) val_loss:0.2374\n",
      "epoch [15/100], (rmse) loss:0.4130, (rmse) val_loss:0.2387\n",
      "epoch [16/100], (rmse) loss:0.4083, (rmse) val_loss:0.2399\n",
      "epoch [17/100], (rmse) loss:0.4050, (rmse) val_loss:0.2402\n",
      "epoch [18/100], (rmse) loss:0.4000, (rmse) val_loss:0.2412\n",
      "epoch [19/100], (rmse) loss:0.3991, (rmse) val_loss:0.2421\n",
      "epoch [20/100], (rmse) loss:0.3968, (rmse) val_loss:0.2455\n",
      "epoch [21/100], (rmse) loss:0.3939, (rmse) val_loss:0.2429\n",
      "epoch [22/100], (rmse) loss:0.3894, (rmse) val_loss:0.2434\n",
      "epoch [23/100], (rmse) loss:0.3859, (rmse) val_loss:0.2459\n",
      "epoch [24/100], (rmse) loss:0.3826, (rmse) val_loss:0.2446\n",
      "epoch [25/100], (rmse) loss:0.3790, (rmse) val_loss:0.2460\n",
      "epoch [26/100], (rmse) loss:0.3769, (rmse) val_loss:0.2474\n",
      "epoch [27/100], (rmse) loss:0.3754, (rmse) val_loss:0.2475\n",
      "epoch [28/100], (rmse) loss:0.3720, (rmse) val_loss:0.2472\n",
      "epoch [29/100], (rmse) loss:0.3689, (rmse) val_loss:0.2488\n",
      "epoch [30/100], (rmse) loss:0.3678, (rmse) val_loss:0.2496\n",
      "epoch [31/100], (rmse) loss:0.3688, (rmse) val_loss:0.2495\n",
      "epoch [32/100], (rmse) loss:0.3678, (rmse) val_loss:0.2516\n",
      "epoch [33/100], (rmse) loss:0.3680, (rmse) val_loss:0.2502\n",
      "epoch [34/100], (rmse) loss:0.3670, (rmse) val_loss:0.2510\n",
      "epoch [35/100], (rmse) loss:0.3655, (rmse) val_loss:0.2513\n",
      "epoch [36/100], (rmse) loss:0.3619, (rmse) val_loss:0.2518\n",
      "epoch [37/100], (rmse) loss:0.3597, (rmse) val_loss:0.2511\n",
      "epoch [38/100], (rmse) loss:0.3572, (rmse) val_loss:0.2515\n",
      "epoch [39/100], (rmse) loss:0.3537, (rmse) val_loss:0.2529\n",
      "epoch [40/100], (rmse) loss:0.3538, (rmse) val_loss:0.2529\n",
      "epoch [41/100], (rmse) loss:0.3524, (rmse) val_loss:0.2530\n",
      "epoch [42/100], (rmse) loss:0.3529, (rmse) val_loss:0.2538\n",
      "epoch [43/100], (rmse) loss:0.3536, (rmse) val_loss:0.2542\n",
      "epoch [44/100], (rmse) loss:0.3546, (rmse) val_loss:0.2543\n",
      "epoch [45/100], (rmse) loss:0.3543, (rmse) val_loss:0.2559\n",
      "epoch [46/100], (rmse) loss:0.3520, (rmse) val_loss:0.2535\n",
      "epoch [47/100], (rmse) loss:0.3493, (rmse) val_loss:0.2536\n",
      "epoch [48/100], (rmse) loss:0.3465, (rmse) val_loss:0.2541\n",
      "epoch [49/100], (rmse) loss:0.3457, (rmse) val_loss:0.2556\n",
      "epoch [50/100], (rmse) loss:0.3455, (rmse) val_loss:0.2549\n",
      "epoch [51/100], (rmse) loss:0.3435, (rmse) val_loss:0.2544\n",
      "epoch [52/100], (rmse) loss:0.3462, (rmse) val_loss:0.2543\n",
      "epoch [53/100], (rmse) loss:0.3435, (rmse) val_loss:0.2553\n",
      "epoch [54/100], (rmse) loss:0.3424, (rmse) val_loss:0.2571\n",
      "epoch [55/100], (rmse) loss:0.3421, (rmse) val_loss:0.2566\n",
      "epoch [56/100], (rmse) loss:0.3431, (rmse) val_loss:0.2565\n",
      "epoch [57/100], (rmse) loss:0.3428, (rmse) val_loss:0.2570\n",
      "epoch [58/100], (rmse) loss:0.3465, (rmse) val_loss:0.2571\n",
      "epoch [59/100], (rmse) loss:0.3412, (rmse) val_loss:0.2569\n",
      "epoch [60/100], (rmse) loss:0.3393, (rmse) val_loss:0.2570\n",
      "epoch [61/100], (rmse) loss:0.3394, (rmse) val_loss:0.2570\n",
      "epoch [62/100], (rmse) loss:0.3385, (rmse) val_loss:0.2572\n",
      "epoch [63/100], (rmse) loss:0.3377, (rmse) val_loss:0.2575\n",
      "epoch [64/100], (rmse) loss:0.3382, (rmse) val_loss:0.2590\n",
      "epoch [65/100], (rmse) loss:0.3378, (rmse) val_loss:0.2581\n",
      "epoch [66/100], (rmse) loss:0.3363, (rmse) val_loss:0.2585\n",
      "epoch [67/100], (rmse) loss:0.3357, (rmse) val_loss:0.2573\n",
      "epoch [68/100], (rmse) loss:0.3323, (rmse) val_loss:0.2582\n",
      "epoch [69/100], (rmse) loss:0.3313, (rmse) val_loss:0.2587\n",
      "epoch [70/100], (rmse) loss:0.3312, (rmse) val_loss:0.2583\n",
      "epoch [71/100], (rmse) loss:0.3298, (rmse) val_loss:0.2592\n",
      "epoch [72/100], (rmse) loss:0.3298, (rmse) val_loss:0.2593\n",
      "epoch [73/100], (rmse) loss:0.3285, (rmse) val_loss:0.2592\n",
      "epoch [74/100], (rmse) loss:0.3288, (rmse) val_loss:0.2600\n",
      "epoch [75/100], (rmse) loss:0.3300, (rmse) val_loss:0.2596\n",
      "epoch [76/100], (rmse) loss:0.3309, (rmse) val_loss:0.2594\n",
      "epoch [77/100], (rmse) loss:0.3312, (rmse) val_loss:0.2587\n",
      "epoch [78/100], (rmse) loss:0.3300, (rmse) val_loss:0.2615\n",
      "epoch [79/100], (rmse) loss:0.3308, (rmse) val_loss:0.2602\n",
      "epoch [80/100], (rmse) loss:0.3312, (rmse) val_loss:0.2609\n",
      "epoch [81/100], (rmse) loss:0.3294, (rmse) val_loss:0.2607\n",
      "epoch [82/100], (rmse) loss:0.3286, (rmse) val_loss:0.2613\n",
      "epoch [83/100], (rmse) loss:0.3259, (rmse) val_loss:0.2603\n",
      "epoch [84/100], (rmse) loss:0.3247, (rmse) val_loss:0.2609\n",
      "epoch [85/100], (rmse) loss:0.3232, (rmse) val_loss:0.2609\n",
      "epoch [86/100], (rmse) loss:0.3252, (rmse) val_loss:0.2622\n",
      "epoch [87/100], (rmse) loss:0.3245, (rmse) val_loss:0.2618\n",
      "epoch [88/100], (rmse) loss:0.3236, (rmse) val_loss:0.2625\n",
      "epoch [89/100], (rmse) loss:0.3254, (rmse) val_loss:0.2623\n",
      "epoch [90/100], (rmse) loss:0.3245, (rmse) val_loss:0.2619\n",
      "epoch [91/100], (rmse) loss:0.3229, (rmse) val_loss:0.2619\n",
      "epoch [92/100], (rmse) loss:0.3227, (rmse) val_loss:0.2621\n",
      "epoch [93/100], (rmse) loss:0.3202, (rmse) val_loss:0.2629\n",
      "epoch [94/100], (rmse) loss:0.3192, (rmse) val_loss:0.2630\n",
      "epoch [95/100], (rmse) loss:0.3184, (rmse) val_loss:0.2638\n",
      "epoch [96/100], (rmse) loss:0.3244, (rmse) val_loss:0.2636\n",
      "epoch [97/100], (rmse) loss:0.3231, (rmse) val_loss:0.2629\n",
      "epoch [98/100], (rmse) loss:0.3186, (rmse) val_loss:0.2622\n",
      "epoch [99/100], (rmse) loss:0.3168, (rmse) val_loss:0.2626\n",
      "epoch [100/100], (rmse) loss:0.3179, (rmse) val_loss:0.2629\n"
     ]
    }
   ],
   "source": [
    "## Model Running (Training)\n",
    "model = run_epoch(model, X_train, X_eval, num_epochs= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T02:14:52.835767Z",
     "start_time": "2018-02-08T02:14:52.816685Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Regression 형태가 아닌 Classification 문제로 변환\n",
    "# movielens의 경우 0.5 ~ 5까지 0.5구간으로 평점이 존재하며 이에 따라 총 10개의 class인 문제로 표현 가능\n",
    "class Deep_MF_ml_class(gluon.HybridBlock):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Deep_MF_ml_class, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            # layers created in name_scope will inherit name space\n",
    "            # from parent layer.\n",
    "            self.user = nn.Embedding(input_dim=n_users, output_dim=25)\n",
    "            self.movie = nn.Embedding(input_dim=n_movies, output_dim=25)\n",
    "            \n",
    "            \n",
    "            self.out = gluon.nn.HybridSequential('output_')\n",
    "            with self.out.name_scope():\n",
    "                self.out.add(nn.Flatten())\n",
    "                self.out.add(nn.Dense(64, activation='relu')) # encoding & latent layer\n",
    "                self.out.add(gluon.nn.BatchNorm(axis=1, center=True, scale=True))\n",
    "                self.out.add(nn.Dense(64, activation='relu')) # encoding & latent layer\n",
    "                self.out.add(gluon.nn.BatchNorm(axis=1, center=True, scale=True))\n",
    "                self.out.add(nn.Dense(10)) # encoding & latent layer\n",
    "            \n",
    "    def hybrid_forward(self, F, user_i, movie_i):\n",
    "        # Here `F` can be either mx.nd or mx.sym, x is the input data,\n",
    "        user_i = self.user(user_i)\n",
    "        movie_i = self.movie(movie_i)\n",
    "        \n",
    "        z = F.concat(user_i, movie_i)\n",
    "        z = self.out(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T02:14:52.849708Z",
     "start_time": "2018-02-08T02:14:52.837153Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model initialization\n",
    "model = Deep_MF_ml_class()\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx=ctx)\n",
    "\n",
    "\n",
    "learning_rate = 1e-3\n",
    "criterion = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "optimizer = gluon.Trainer(model.collect_params(), 'adam',\n",
    "                          {'learning_rate': learning_rate,\n",
    "                           'wd': 1e-5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T02:31:59.093362Z",
     "start_time": "2018-02-08T02:31:59.086133Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    \n",
    "    for user, movie, rating in data_iterator:\n",
    "        user = user.as_in_context(ctx)\n",
    "        movie = movie.as_in_context(ctx)\n",
    "        rating = rating.as_in_context(ctx)\n",
    "    \n",
    "        output = net(user, movie)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=rating)\n",
    "    return acc.get()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T02:47:33.924434Z",
     "start_time": "2018-02-08T02:31:59.372630Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/100], loss:0.9802, val_loss:0.4485, train_acc:0.6778, val_acc:0.4074\n",
      "epoch [2/100], loss:0.9660, val_loss:0.4490, train_acc:0.6864, val_acc:0.4071\n",
      "epoch [3/100], loss:0.9531, val_loss:0.4509, train_acc:0.6915, val_acc:0.4057\n",
      "epoch [4/100], loss:0.9413, val_loss:0.4521, train_acc:0.6986, val_acc:0.4024\n",
      "epoch [5/100], loss:0.9306, val_loss:0.4538, train_acc:0.7020, val_acc:0.4042\n",
      "epoch [6/100], loss:0.9204, val_loss:0.4554, train_acc:0.7094, val_acc:0.4002\n",
      "epoch [7/100], loss:0.9113, val_loss:0.4571, train_acc:0.7134, val_acc:0.4025\n",
      "epoch [8/100], loss:0.9026, val_loss:0.4590, train_acc:0.7166, val_acc:0.3995\n",
      "epoch [9/100], loss:0.8945, val_loss:0.4607, train_acc:0.7199, val_acc:0.4029\n",
      "epoch [10/100], loss:0.8873, val_loss:0.4620, train_acc:0.7254, val_acc:0.4022\n",
      "epoch [11/100], loss:0.8805, val_loss:0.4636, train_acc:0.7295, val_acc:0.3997\n",
      "epoch [12/100], loss:0.8738, val_loss:0.4652, train_acc:0.7302, val_acc:0.4001\n",
      "epoch [13/100], loss:0.8676, val_loss:0.4672, train_acc:0.7355, val_acc:0.4022\n",
      "epoch [14/100], loss:0.8618, val_loss:0.4690, train_acc:0.7393, val_acc:0.4001\n",
      "epoch [15/100], loss:0.8564, val_loss:0.4703, train_acc:0.7409, val_acc:0.3993\n",
      "epoch [16/100], loss:0.8515, val_loss:0.4718, train_acc:0.7438, val_acc:0.3990\n",
      "epoch [17/100], loss:0.8464, val_loss:0.4736, train_acc:0.7480, val_acc:0.3980\n",
      "epoch [18/100], loss:0.8419, val_loss:0.4752, train_acc:0.7493, val_acc:0.3980\n",
      "epoch [19/100], loss:0.8377, val_loss:0.4761, train_acc:0.7518, val_acc:0.3985\n",
      "epoch [20/100], loss:0.8331, val_loss:0.4778, train_acc:0.7523, val_acc:0.3978\n",
      "epoch [21/100], loss:0.8294, val_loss:0.4796, train_acc:0.7575, val_acc:0.3950\n",
      "epoch [22/100], loss:0.8252, val_loss:0.4807, train_acc:0.7600, val_acc:0.3965\n",
      "epoch [23/100], loss:0.8213, val_loss:0.4824, train_acc:0.7596, val_acc:0.3915\n",
      "epoch [24/100], loss:0.8181, val_loss:0.4835, train_acc:0.7631, val_acc:0.3969\n",
      "epoch [25/100], loss:0.8141, val_loss:0.4850, train_acc:0.7645, val_acc:0.3919\n",
      "epoch [26/100], loss:0.8112, val_loss:0.4864, train_acc:0.7671, val_acc:0.3952\n",
      "epoch [27/100], loss:0.8078, val_loss:0.4880, train_acc:0.7696, val_acc:0.3925\n",
      "epoch [28/100], loss:0.8046, val_loss:0.4895, train_acc:0.7700, val_acc:0.3911\n",
      "epoch [29/100], loss:0.8015, val_loss:0.4903, train_acc:0.7725, val_acc:0.3954\n",
      "epoch [30/100], loss:0.7984, val_loss:0.4920, train_acc:0.7750, val_acc:0.3909\n",
      "epoch [31/100], loss:0.7955, val_loss:0.4931, train_acc:0.7762, val_acc:0.3896\n",
      "epoch [32/100], loss:0.7927, val_loss:0.4951, train_acc:0.7772, val_acc:0.3900\n",
      "epoch [33/100], loss:0.7895, val_loss:0.4959, train_acc:0.7792, val_acc:0.3930\n",
      "epoch [34/100], loss:0.7868, val_loss:0.4974, train_acc:0.7784, val_acc:0.3893\n",
      "epoch [35/100], loss:0.7845, val_loss:0.4983, train_acc:0.7824, val_acc:0.3924\n",
      "epoch [36/100], loss:0.7819, val_loss:0.5002, train_acc:0.7826, val_acc:0.3904\n",
      "epoch [37/100], loss:0.7797, val_loss:0.5016, train_acc:0.7844, val_acc:0.3910\n",
      "epoch [38/100], loss:0.7770, val_loss:0.5027, train_acc:0.7854, val_acc:0.3949\n",
      "epoch [39/100], loss:0.7743, val_loss:0.5045, train_acc:0.7868, val_acc:0.3912\n",
      "epoch [40/100], loss:0.7727, val_loss:0.5054, train_acc:0.7885, val_acc:0.3937\n",
      "epoch [41/100], loss:0.7705, val_loss:0.5066, train_acc:0.7891, val_acc:0.3919\n",
      "epoch [42/100], loss:0.7682, val_loss:0.5080, train_acc:0.7914, val_acc:0.3898\n",
      "epoch [43/100], loss:0.7661, val_loss:0.5092, train_acc:0.7909, val_acc:0.3893\n",
      "epoch [44/100], loss:0.7645, val_loss:0.5106, train_acc:0.7939, val_acc:0.3902\n",
      "epoch [45/100], loss:0.7615, val_loss:0.5122, train_acc:0.7943, val_acc:0.3886\n",
      "epoch [46/100], loss:0.7596, val_loss:0.5128, train_acc:0.7940, val_acc:0.3910\n",
      "epoch [47/100], loss:0.7580, val_loss:0.5144, train_acc:0.7961, val_acc:0.3899\n",
      "epoch [48/100], loss:0.7559, val_loss:0.5156, train_acc:0.7974, val_acc:0.3851\n",
      "epoch [49/100], loss:0.7546, val_loss:0.5170, train_acc:0.7989, val_acc:0.3864\n",
      "epoch [50/100], loss:0.7518, val_loss:0.5181, train_acc:0.7986, val_acc:0.3885\n",
      "epoch [51/100], loss:0.7504, val_loss:0.5190, train_acc:0.7985, val_acc:0.3879\n",
      "epoch [52/100], loss:0.7489, val_loss:0.5205, train_acc:0.8001, val_acc:0.3873\n",
      "epoch [53/100], loss:0.7467, val_loss:0.5217, train_acc:0.8014, val_acc:0.3872\n",
      "epoch [54/100], loss:0.7454, val_loss:0.5226, train_acc:0.8015, val_acc:0.3874\n",
      "epoch [55/100], loss:0.7432, val_loss:0.5239, train_acc:0.8040, val_acc:0.3861\n",
      "epoch [56/100], loss:0.7413, val_loss:0.5254, train_acc:0.8052, val_acc:0.3868\n",
      "epoch [57/100], loss:0.7397, val_loss:0.5261, train_acc:0.8053, val_acc:0.3895\n",
      "epoch [58/100], loss:0.7383, val_loss:0.5277, train_acc:0.8040, val_acc:0.3875\n",
      "epoch [59/100], loss:0.7366, val_loss:0.5284, train_acc:0.8071, val_acc:0.3860\n",
      "epoch [60/100], loss:0.7348, val_loss:0.5294, train_acc:0.8080, val_acc:0.3891\n",
      "epoch [61/100], loss:0.7333, val_loss:0.5313, train_acc:0.8096, val_acc:0.3861\n",
      "epoch [62/100], loss:0.7319, val_loss:0.5325, train_acc:0.8100, val_acc:0.3861\n",
      "epoch [63/100], loss:0.7307, val_loss:0.5330, train_acc:0.8117, val_acc:0.3893\n",
      "epoch [64/100], loss:0.7292, val_loss:0.5343, train_acc:0.8105, val_acc:0.3865\n",
      "epoch [65/100], loss:0.7280, val_loss:0.5352, train_acc:0.8121, val_acc:0.3877\n",
      "epoch [66/100], loss:0.7265, val_loss:0.5362, train_acc:0.8130, val_acc:0.3851\n",
      "epoch [67/100], loss:0.7252, val_loss:0.5377, train_acc:0.8128, val_acc:0.3888\n",
      "epoch [68/100], loss:0.7234, val_loss:0.5386, train_acc:0.8138, val_acc:0.3885\n",
      "epoch [69/100], loss:0.7215, val_loss:0.5391, train_acc:0.8155, val_acc:0.3884\n",
      "epoch [70/100], loss:0.7205, val_loss:0.5406, train_acc:0.8139, val_acc:0.3864\n",
      "epoch [71/100], loss:0.7190, val_loss:0.5419, train_acc:0.8170, val_acc:0.3882\n",
      "epoch [72/100], loss:0.7181, val_loss:0.5431, train_acc:0.8164, val_acc:0.3887\n",
      "epoch [73/100], loss:0.7174, val_loss:0.5438, train_acc:0.8154, val_acc:0.3892\n",
      "epoch [74/100], loss:0.7164, val_loss:0.5449, train_acc:0.8160, val_acc:0.3908\n",
      "epoch [75/100], loss:0.7155, val_loss:0.5457, train_acc:0.8181, val_acc:0.3869\n",
      "epoch [76/100], loss:0.7133, val_loss:0.5459, train_acc:0.8181, val_acc:0.3909\n",
      "epoch [77/100], loss:0.7118, val_loss:0.5477, train_acc:0.8191, val_acc:0.3867\n",
      "epoch [78/100], loss:0.7107, val_loss:0.5485, train_acc:0.8205, val_acc:0.3886\n",
      "epoch [79/100], loss:0.7097, val_loss:0.5496, train_acc:0.8198, val_acc:0.3874\n",
      "epoch [80/100], loss:0.7082, val_loss:0.5503, train_acc:0.8222, val_acc:0.3862\n",
      "epoch [81/100], loss:0.7065, val_loss:0.5515, train_acc:0.8205, val_acc:0.3884\n",
      "epoch [82/100], loss:0.7054, val_loss:0.5519, train_acc:0.8219, val_acc:0.3840\n",
      "epoch [83/100], loss:0.7042, val_loss:0.5532, train_acc:0.8242, val_acc:0.3873\n",
      "epoch [84/100], loss:0.7036, val_loss:0.5543, train_acc:0.8241, val_acc:0.3870\n",
      "epoch [85/100], loss:0.7028, val_loss:0.5552, train_acc:0.8250, val_acc:0.3909\n",
      "epoch [86/100], loss:0.7012, val_loss:0.5559, train_acc:0.8259, val_acc:0.3882\n",
      "epoch [87/100], loss:0.6996, val_loss:0.5570, train_acc:0.8270, val_acc:0.3880\n",
      "epoch [88/100], loss:0.6981, val_loss:0.5587, train_acc:0.8267, val_acc:0.3863\n",
      "epoch [89/100], loss:0.6975, val_loss:0.5593, train_acc:0.8280, val_acc:0.3895\n",
      "epoch [90/100], loss:0.6965, val_loss:0.5602, train_acc:0.8284, val_acc:0.3886\n",
      "epoch [91/100], loss:0.6949, val_loss:0.5615, train_acc:0.8284, val_acc:0.3894\n",
      "epoch [92/100], loss:0.6950, val_loss:0.5627, train_acc:0.8291, val_acc:0.3903\n",
      "epoch [93/100], loss:0.6935, val_loss:0.5634, train_acc:0.8302, val_acc:0.3853\n",
      "epoch [94/100], loss:0.6924, val_loss:0.5647, train_acc:0.8288, val_acc:0.3888\n",
      "epoch [95/100], loss:0.6919, val_loss:0.5650, train_acc:0.8303, val_acc:0.3889\n",
      "epoch [96/100], loss:0.6903, val_loss:0.5665, train_acc:0.8306, val_acc:0.3875\n",
      "epoch [97/100], loss:0.6890, val_loss:0.5675, train_acc:0.8325, val_acc:0.3843\n",
      "epoch [98/100], loss:0.6882, val_loss:0.5686, train_acc:0.8306, val_acc:0.3854\n",
      "epoch [99/100], loss:0.6877, val_loss:0.5691, train_acc:0.8309, val_acc:0.3834\n",
      "epoch [100/100], loss:0.6868, val_loss:0.5704, train_acc:0.8308, val_acc:0.3881\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100             \n",
    "              \n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    n_total = 0.0\n",
    "\n",
    "    # for training\n",
    "    for user, movie, rating in X_train:\n",
    "        user = user.as_in_context(ctx)\n",
    "        movie = movie.as_in_context(ctx)\n",
    "        rating = rating.as_in_context(ctx)\n",
    "\n",
    "        with mx.autograd.record():\n",
    "            output = model(user, movie)\n",
    "            loss = criterion(output, rating)\n",
    "        loss.backward()\n",
    "        optimizer.step(user.shape[0])\n",
    "        running_loss += mx.nd.sum(loss).asscalar()\n",
    "        n_total += user.shape[0]\n",
    "\n",
    "    for val_user, val_movie, val_rating in X_eval:\n",
    "        val_user = val_user.as_in_context(ctx)\n",
    "        val_movie = val_movie.as_in_context(ctx)\n",
    "\n",
    "        with mx.autograd.record():\n",
    "            val_output = model(val_user, val_movie)\n",
    "            val_loss_tmp = criterion(val_output, val_rating)\n",
    "        val_loss += mx.nd.sum(val_loss_tmp).asscalar()\n",
    "    \n",
    "    train_accuracy = evaluate_accuracy(X_train, model)\n",
    "    test_accuracy = evaluate_accuracy(X_eval, model)\n",
    "    # ===================log========================\n",
    "    print('epoch [{}/{}], loss:{:.4f}, val_loss:{:.4f}, train_acc:{:.4f}, val_acc:{:.4f}'\n",
    "          .format(epoch + 1, num_epochs, np.sqrt(running_loss / n_total), np.sqrt(val_loss / n_total), train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T02:47:34.174078Z",
     "start_time": "2018-02-08T02:47:33.925932Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfwAAAFsCAYAAAApNAtQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAGQOADAGAA/w/sRl3WcGSfrQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x106322898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_set = mx.nd.array((valid_users.astype('float32')))\n",
    "movie_set = mx.nd.array((valid_movies.astype('float32')))\n",
    "\n",
    "y_pred = model(user_set, movie_set).asnumpy().argmax(axis=1)\n",
    "y_pred = (y_pred + 1.) / 2\n",
    "\n",
    "plt.title(\"Distribution of Predictions\", fontsize=14)\n",
    "plt.ylabel(\"Count\", fontsize=14)\n",
    "plt.xlabel(\"Rating\", fontsize=14)\n",
    "plt.hist(y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T02:47:34.179858Z",
     "start_time": "2018-02-08T02:47:34.175660Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6298351005601741"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.sqrt(((y_pred - valid_ratings) ** 2).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
